{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1BgyV0cypQ1YlT0LAzUBsTpC1qduwj8yg","timestamp":1680374159795}],"gpuType":"A100","authorship_tag":"ABX9TyO7rlwtN6mSiOp44Gg0+dMp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["## -*- coding: utf-8 -*-\n","# @Auther   : Mingsong Li (lms-07)\n","# @Time     : 2022-Nov\n","# @Address  : Time Lab @ SDU\n","# @FileName : process_dl.py\n","# @Project  : CVSSN (HSIC), IEEE TCSVT\n","\n","# for IP, KSC, and UP data sets, main processing file for the involved deep learning models,\n","# i.e., ,\n","# ContextualNet, RSSAN, SSTN, SSAtt, SSAN, SSSAN, A2S2KResNet, and the proposed CVSSN\n","\n"],"metadata":{"id":"BrKQpn_RwoGW"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yq6wLy5lwgOD","executionInfo":{"status":"ok","timestamp":1680560147646,"user_tz":300,"elapsed":44893,"user":{"displayName":"Michael Wesselink","userId":"03807820748818897534"}},"outputId":"8b90cbed-7ab9-45f3-ff95-d61a50212186"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["#Original code attributed above from https://github.com/lms-07/CVSSN\n","\n","#Edits by Mike Wesselink 3.27.23\n","\n","###############################\n","#Mike Wesselink CIS 631\n","\n","#Mount my google drive (Plan B)\n","from google.colab import drive\n","drive.mount('/content/drive/',force_remount=True)"]},{"cell_type":"code","source":["#imports\n","\n","import os\n","import time\n","import torch\n","import random\n","import numpy as np\n","from sklearn import metrics\n","\n","!pip install thop\n","from thop import profile\n","\n","#import spectral\n","!pip install spectral\n","from spectral import *\n","\n","import sys\n","\n","py_file_location = \"/content/drive/MyDrive/Colab Notebooks/CIS 631 Final Project/CVSSN-main\"\n","sys.path.append(os.path.abspath(py_file_location))\n","\n","import utils.evaluation as evaluation\n","import utils.data_load_operate as data_load_operate\n","import visual.cls_map_visual as cls_visual\n","\n","import model.ContextualNet as ContextualNet\n","import model.RSSAN as RSSAN\n","import model.SSTN as SSTN\n","import model.SSAtt as SSAtt\n","import model.A2S2KResNet as A2S2KResNet\n","import model.CVSSN as CVSSN\n","import model.SSAN as SSAN\n","import model.SSSAN as SSSAN\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3qGmG6xcwg1h","executionInfo":{"status":"ok","timestamp":1680560172310,"user_tz":300,"elapsed":24672,"user":{"displayName":"Michael Wesselink","userId":"03807820748818897534"}},"outputId":"c1898bb1-4661-4f2f-a518-4eb0f1258760"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting thop\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from thop) (2.0.0+cu118)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->thop) (1.11.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->thop) (2.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->thop) (3.10.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->thop) (3.1.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->thop) (3.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->thop) (4.5.0)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->thop) (16.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->thop) (3.25.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->thop) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->thop) (1.3.0)\n","Installing collected packages: thop\n","Successfully installed thop-0.1.1.post2209072238\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting spectral\n","  Downloading spectral-0.23.1-py3-none-any.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 KB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from spectral) (1.22.4)\n","Installing collected packages: spectral\n","Successfully installed spectral-0.23.1\n"]}]},{"cell_type":"code","source":["#SET UP MODEL INFO\n","\n","time_current = time.strftime(\"%y-%m-%d-%H.%M\", time.localtime())\n","\n","# random seed setting\n","seed = 20\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)  # Numpy module.\n","random.seed(seed)  # Python random module.\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","###                 0             1       2       3        4        5         6             7\n","model_list = ['ContextualNet', 'RSSAN', 'SSTN', 'SSAN', 'SSSAN', 'SSAtt', 'A2S2KResNet', 'CVSSN']\n","\n","model_flag = 7      #use only the CVSSN model\n","model_spa_set = {1, 2, 3, 5}\n","model_spe_set = {}\n","model_spa_spe_set = {4, 7}\n","model_3D_spa_set = {0, 6}\n","\n","model_3D_spa_flag = 0\n","\n","if model_flag in model_spa_set:\n","    model_type_flag = 1\n","    if model_flag in model_3D_spa_set:\n","        model_3D_spa_flag = 1\n","elif model_flag in model_spe_set:\n","    model_type_flag = 2\n","elif model_flag in model_spa_spe_set:\n","    model_type_flag = 3\n","\n","# 0-3\n","#data_set_name_list = ['IP', 'KSC', 'UP', 'HU_tif']\n","data_set_name_list = ['IP', 'KSC', 'UP', 'SALINAS']\n","data_set_name = data_set_name_list[0]  #>>>test all 4 data sets by selecting position in data_set_name_list above?<<<\n","\n","# seed_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  \n","seed_list=[0,1,2,3,4]\n","# seed_list=[0,1,2] \n","# seed_list=[0,1]\n","#seed_list = [0]  \n","\n","# ratio=0.5\n","# ratio=1.0\n","# ratio=2.5\n","# ratio=5.0\n","# ratio=7.5\n","ratio = 10.0\n","patch_size = 9\n","patch_length = 4\n","\n","\n","###SET UP DATA PATH###\n","\n","#data_set_path = os.path.join(os.getcwd(), 'data')\n","data_set_path = '/content/drive/MyDrive/Colab Notebooks/CIS 631 Final Project/CVSSN-main/data'\n","\n","#results_save_path = \\\n","#    os.path.join(os.getcwd(), 'output/results', model_list[model_flag] + str(\"_\") +\n","#                 data_set_name + str(\"_\") + str(time_current) + str(\"_seed\") + str(seed) + str(\"_ratio\") + str(\n","#        ratio) + str(\"_patch_size\") + str(patch_size))\n","\n","results_save_path = \\\n","    os.path.join('/content/drive/MyDrive/Colab Notebooks/CIS 631 Final Project/CVSSN-main/output/results', model_list[model_flag] + str(\"_\") +\n","                data_set_name + str(\"_\") + str(time_current) + str(\"_seed\") + str(seed) + str(\"_ratio\") + str(\n","        ratio) + str(\"_patch_size\") + str(patch_size))\n","\n","\n","#cls_map_save_path = \\\n","#    os.path.join(os.path.join(os.getcwd(), 'output/cls_maps'), model_list[model_flag] + str(\"_\") +\n","#                 data_set_name + str(\"_\") + str(time_current) + str(\"_seed\") + str(seed)) + str(\"_ratio\") + str(ratio)\n","\n","cls_map_save_path = \\\n","    os.path.join(os.path.join('/content/drive/MyDrive/Colab Notebooks/CIS 631 Final Project/CVSSN-main/output/cls_maps'), model_list[model_flag] + str(\"_\") +\n","                 data_set_name + str(\"_\") + str(time_current) + str(\"_seed\") + str(seed)) + str(\"_ratio\") + str(ratio)\n","\n","if __name__ == '__main__':\n","\n","    torch.cuda.empty_cache()\n","\n","    data, gt = data_load_operate.load_data(data_set_name, data_set_path)\n","\n","    height, width, channels = data.shape\n","\n","    data = data_load_operate.standardization(data)\n","\n","    gt_reshape = gt.reshape(-1)\n","    height, width, channels = data.shape\n","    class_count = max(np.unique(gt))\n","\n","    flag_list = [0, 1]  # ratio or num\n","    ratio_list = [0.1, 0.01]  # [train_ratio,val_ratio]\n","    # ratio_list=[0.075,0.0075] # [train_ratio,val_ratio]\n","    # ratio_list=[0.05,0.005] # [train_ratio,val_ratio]\n","    # ratio_list=[0.0255,0.0025] # [train_ratio,val_ratio]\n","    # ratio_list=[0.01,0.001] # [train_ratio,val_ratio]\n","    # ratio_list=[0.005,0.0005] # [train_ratio,val_ratio]\n","    num_list = [45, 4]  # [train_num,val_num]\n","\n","    batch_size = 32\n","    max_epoch = 100\n","    learning_rate = 0.001\n","    loss = torch.nn.CrossEntropyLoss()\n","\n","    # data pad zero\n","    # data:[h,w,c]->data_padded:[h+2l,w+2l,c]\n","    data_padded = data_load_operate.data_pad_zero(data, patch_length)\n","    height_patched, width_patched, channels = data_padded.shape\n","\n","    OA_ALL = []\n","    AA_ALL = []\n","    KPP_ALL = []\n","    EACH_ACC_ALL = []\n","    Train_Time_ALL = []\n","    Test_Time_ALL = []\n","    CLASS_ACC = np.zeros([len(seed_list), class_count])\n","\n","    # data_total_index = np.arange(data.shape[0] * data.shape[1])  # For total sample cls_map.\n","\n","    for curr_seed in seed_list:\n","\n","        train_data_index, val_data_index, test_data_index, all_data_index = data_load_operate.sampling(ratio_list,\n","                                                                                                       num_list,\n","                                                                                                       gt_reshape,\n","                                                                                                       class_count,\n","                                                                                                       flag_list[0])\n","        index = (train_data_index, val_data_index, test_data_index)\n","        train_iter, test_iter, val_iter = data_load_operate.generate_iter_1 \\\n","            (data_padded, height, width, gt_reshape, index, patch_length, batch_size, model_type_flag,\n","             model_3D_spa_flag)\n","\n","\n","\n","        # load data for the cls map of all the labed samples\n","        # all_iter = data_load_operate.generate_iter_2(data_padded, height, width, gt_reshape, all_data_index,\n","        #                                              patch_length,\n","        #                                              batch_size, model_type_flag, model_3D_spa_flag)\n","        # load data for the cls map of the total samples\n","        # total_iter = data_load_operate.generate_iter_2(data_padded,height, width, gt_reshape, data_total_index, patch_length,\n","        #              25, model_type_flag, model_3D_spa_flag)\n","\n","        if model_flag == 0:\n","            net = ContextualNet.LeeEtAl(channels, class_count)\n","        elif model_flag == 1:\n","            net = RSSAN.RSSAN_net(in_shape=(channels, height_patched, width_patched), num_classes=class_count)\n","        elif model_flag == 2:\n","            net = SSTN.SSTN_AEAE(in_shape=(channels, height_patched, width_patched), num_classes=class_count)\n","        elif model_flag == 3:\n","            net = SSAN.SSAN(channels, patch_size, class_count)\n","        elif model_flag == 4:\n","            net = SSSAN.SSSAN(channels, class_count)\n","        elif model_flag == 5:\n","            net = SSAtt.Hang2020(channels, class_count)\n","        elif model_flag == 6:\n","            net = A2S2KResNet.S3KAIResNet(channels, class_count, 2)\n","        elif model_flag == 7:\n","            net = CVSSN.CVSSN_(channels, patch_size, patch_size, class_count)\n","\n","        # efficiency test, model complexity and computational cost\n","        # test_spe_input=torch.randn(1,channels) # for 1D model\n","        # test_input=torch.randn(1,patch_size,patch_size,channels) # for 2D model\n","        # test_input=torch.randn(1,1,patch_size,patch_size,channels) # for 3D model\n","        #\n","        # flops,para=profile(net,(test_input,test_spe_input))\n","        # flops,para=profile(net,(test_spe_input))\n","        # flops,para=profile(net,(test_input))\n","        #\n","        # print(\"para:{}\\n,flops:{}\".format(para,flops))\n","        # print(\"para(M):{:.3f},\\n flops(M):{:.2f}\".format(para/(1000**2),flops/(1000**2),))\n","\n","        net.to(device)\n","\n","        train_loss_list = [100]\n","        train_acc_list = [0]\n","        val_loss_list = [100]\n","        val_acc_list = [0]\n","        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n","        best_loss = 99999\n","\n","        tic1 = time.perf_counter()\n","\n","        for epoch in range(max_epoch):\n","            train_acc_sum, trained_samples_counter = 0.0, 0\n","            batch_counter, train_loss_sum = 0, 0\n","            time_epoch = time.time()\n","\n","            if model_type_flag == 1:  # data for single spatial net\n","                for X_spa, y in train_iter:\n","                    X_spa, y = X_spa.to(device), y.to(device)\n","                    y_pred = net(X_spa)\n","\n","                    ls = loss(y_pred, y.long())\n","\n","                    optimizer.zero_grad()\n","                    ls.backward()\n","                    optimizer.step()\n","\n","                    train_loss_sum += ls.cpu().item()\n","                    train_acc_sum += (y_pred.argmax(dim=1) == y).sum().cpu().item()\n","                    trained_samples_counter += y.shape[0]\n","                    batch_counter += 1\n","                    epoch_first_iter = 0\n","            elif model_type_flag == 2:  # data for single spectral net\n","                for X_spe, y in train_iter:\n","                    X_spe, y = X_spe.to(device), y.to(device)\n","                    y_pred = net(X_spe)\n","\n","                    ls = loss(y_pred, y.long())\n","\n","                    optimizer.zero_grad()\n","                    ls.backward()\n","                    optimizer.step()\n","\n","                    train_loss_sum += ls.cpu().item()\n","                    train_acc_sum += (y_pred.argmax(dim=1) == y).sum().cpu().item()\n","                    trained_samples_counter += y.shape[0]\n","                    batch_counter += 1\n","                    epoch_first_iter = 0\n","            elif model_type_flag == 3:  # data for spectral-spatial net\n","                for X_spa, X_spe, y in train_iter:\n","                    X_spa, X_spe, y = X_spa.to(device), X_spe.to(device), y.to(device)\n","                    y_pred = net(X_spa, X_spe)\n","                    if model_flag == 10:\n","                        for i in range(len(y_pred)):\n","                            if i == 0:\n","                                ls = loss(y_pred[i], y.long())\n","                            if i > 0:\n","                                ls += loss(y_pred[i], y.long())\n","                    else:\n","\n","                        ls = loss(y_pred, y.long())\n","\n","                    optimizer.zero_grad()\n","                    ls.backward()\n","                    optimizer.step()\n","\n","                    train_loss_sum += ls.cpu().item()\n","                    train_acc_sum += (y_pred.argmax(dim=1) == y).sum().cpu().item()\n","                    trained_samples_counter += y.shape[0]\n","                    batch_counter += 1\n","                    epoch_first_iter = 0\n","\n","            val_acc, val_loss = evaluation.evaluate_OA(val_iter, net, loss, device, model_type_flag)\n","            val_loss_list.append(val_loss)\n","            val_acc_list.append(val_acc)\n","\n","            if val_loss < best_loss:\n","                best_loss = val_loss\n","                torch.save(net.state_dict(), results_save_path + \"_best_model.pt\")\n","                print('save model...')\n","\n","            torch.cuda.empty_cache()\n","\n","            train_loss_list.append(train_loss_sum)\n","            train_acc_list.append(train_acc_sum / trained_samples_counter)\n","\n","            print('epoch: %d, training_sampler_num: %d, batch_count: %.2f, train loss: %.6f, tarin loss sum: %.6f, '\n","                  'train acc: %.3f, train_acc_sum: %.1f, time: %.1f sec' %\n","                  (epoch + 1, trained_samples_counter, batch_counter, train_loss_sum / batch_counter, train_loss_sum,\n","                   train_acc_sum / trained_samples_counter, train_acc_sum, time.time() - time_epoch))\n","\n","        toc1 = time.perf_counter()\n","        print('Training stage finished:\\n epoch %d, loss %.4f, train acc %.3f, training time %.2f s'\n","              % (epoch + 1, train_loss_sum / batch_counter, train_acc_sum / trained_samples_counter, toc1 - tic1))\n","        training_time = toc1 - tic1\n","        Train_Time_ALL.append(training_time)\n","\n","        print(\"\\n\\n====================Starting evaluation for testing set.========================\\n\")\n","\n","        pred_test = []\n","        # torch.cuda.empty_cache()\n","        with torch.no_grad():\n","            # net.load_state_dict(torch.load(model_save_path+\"_best_model.pt\"))\n","            net.eval()\n","            train_acc_sum, samples_num_counter = 0.0, 0\n","            if model_type_flag == 1:  # data for single spatial net\n","                for X_spa, y in test_iter:\n","                    X_spa = X_spa.to(device)\n","                    y = y.to(device)\n","\n","                    tic2 = time.perf_counter()\n","                    y_pred = net(X_spa)\n","                    toc2 = time.perf_counter()\n","\n","                    pred_test.extend(np.array(y_pred.cpu().argmax(axis=1)))\n","            elif model_type_flag == 2:  # data for single spectral net\n","                for X_spe, y in test_iter:\n","                    X_spe = X_spe.to(device)\n","                    y = y.to(device)\n","\n","                    tic2 = time.perf_counter()\n","                    y_pred = net(X_spe)\n","                    toc2 = time.perf_counter()\n","\n","                    pred_test.extend(np.array(y_pred.cpu().argmax(axis=1)))\n","            elif model_type_flag == 3:  # data for spectral-spatial net\n","                for X_spa, X_spe, y in test_iter:\n","                    X_spa = X_spa.to(device)\n","                    X_spe = X_spe.to(device)\n","                    y = y.to(device)\n","\n","                    tic2 = time.perf_counter()\n","                    y_pred = net(X_spa, X_spe)\n","                    toc2 = time.perf_counter()\n","\n","                    pred_test.extend(np.array(y_pred.cpu().argmax(axis=1)))\n","\n","            y_gt = gt_reshape[test_data_index] - 1\n","            OA = metrics.accuracy_score(y_gt, pred_test)\n","            confusion_matrix = metrics.confusion_matrix(pred_test, y_gt)\n","            print(\"confusion_matrix\\n{}\".format(confusion_matrix))\n","            ECA, AA = evaluation.AA_ECA(confusion_matrix)\n","            kappa = metrics.cohen_kappa_score(pred_test, y_gt)\n","            cls_report = evaluation.claification_report(y_gt, pred_test, data_set_name)\n","            print(\"classification_report\\n{}\".format(cls_report))\n","\n","            # Visualization for all the labeled samples and total the samples\n","            # sample_list1 = [all_iter, all_data_index]\n","            # sample_list2 = [total_iter]\n","\n","            # cls_visual.pred_cls_map_dl(sample_list1,net,gt,cls_map_save_path,model_type_flag)\n","            # cls_visual.pred_cls_map_dl(sample_list2, net, gt, cls_map_save_path,model_type_flag)\n","\n","            testing_time = toc2 - tic2\n","            Test_Time_ALL.append(testing_time)\n","\n","\n","            # Output infors\n","            f = open(results_save_path + '_results.txt', 'a+')\n","            str_results = '\\n======================' \\\n","                          + \" learning rate=\" + str(learning_rate) \\\n","                          + \" epochs=\" + str(max_epoch) \\\n","                          + \" train ratio=\" + str(ratio_list[0]) \\\n","                          + \" val ratio=\" + str(ratio_list[1]) \\\n","                          + \" ======================\" \\\n","                          + \"\\nOA=\" + str(OA) \\\n","                          + \"\\nAA=\" + str(AA) \\\n","                          + '\\nkpp=' + str(kappa) \\\n","                          + '\\nacc per class:' + str(ECA) \\\n","                          + \"\\ntrain time:\" + str(training_time) \\\n","                          + \"\\ntest time:\" + str(testing_time) + \"\\n\"\n","\n","            f.write(str_results)\n","            f.write('{}'.format(confusion_matrix))\n","            f.write('\\n\\n')\n","            f.write('{}'.format(cls_report))\n","            f.close()\n","\n","            OA_ALL.append(OA)\n","            AA_ALL.append(AA)\n","            KPP_ALL.append(kappa)\n","            EACH_ACC_ALL.append(ECA)\n","\n","        torch.cuda.empty_cache()\n","        del net, train_iter, test_iter, val_iter\n","        # del net, train_iter, test_iter, val_iter, all_iter\n","        # del net\n","\n","    OA_ALL = np.array(OA_ALL)\n","    AA_ALL = np.array(AA_ALL)\n","    KPP_ALL = np.array(KPP_ALL)\n","    EACH_ACC_ALL = np.array(EACH_ACC_ALL)\n","    Train_Time_ALL = np.array(Train_Time_ALL)\n","    Test_Time_ALL = np.array(Test_Time_ALL)\n","\n","    np.set_printoptions(precision=4)\n","    print(\"\\n====================Mean result of {} times runs =========================\".format(len(seed_list)))\n","    print('List of OA:', list(OA_ALL))\n","    print('List of AA:', list(AA_ALL))\n","    print('List of KPP:', list(KPP_ALL))\n","    print('OA=', round(np.mean(OA_ALL) * 100, 2), '+-', round(np.std(OA_ALL) * 100, 2))\n","    print('AA=', round(np.mean(AA_ALL) * 100, 2), '+-', round(np.std(AA_ALL) * 100, 2))\n","    print('Kpp=', round(np.mean(KPP_ALL) * 100, 2), '+-', round(np.std(KPP_ALL) * 100, 2))\n","    print('Acc per class=', np.round(np.mean(EACH_ACC_ALL, 0) * 100, decimals=2), '+-',\n","          np.round(np.std(EACH_ACC_ALL, 0) * 100, decimals=2))\n","\n","    print(\"Average training time=\", round(np.mean(Train_Time_ALL), 2), '+-', round(np.std(Train_Time_ALL), 3))\n","    print(\"Average testing time=\", round(np.mean(Test_Time_ALL) * 1000, 2), '+-',\n","          round(np.std(Test_Time_ALL) * 1000, 3))\n","\n","    # Output infors\n","    f = open(results_save_path + '_results.txt', 'a+')\n","    str_results = '\\n\\n***************Mean result of ' + str(len(seed_list)) + 'times runs ********************' \\\n","                  + '\\nList of OA:' + str(list(OA_ALL)) \\\n","                  + '\\nList of AA:' + str(list(AA_ALL)) \\\n","                  + '\\nList of KPP:' + str(list(KPP_ALL)) \\\n","                  + '\\nOA=' + str(round(np.mean(OA_ALL) * 100, 2)) + '+-' + str(round(np.std(OA_ALL) * 100, 2)) \\\n","                  + '\\nAA=' + str(round(np.mean(AA_ALL) * 100, 2)) + '+-' + str(round(np.std(AA_ALL) * 100, 2)) \\\n","                  + '\\nKpp=' + str(round(np.mean(KPP_ALL) * 100, 2)) + '+-' + str(round(np.std(KPP_ALL) * 100, 2)) \\\n","                  + '\\nAcc per class=\\n' + str(np.round(np.mean(EACH_ACC_ALL, 0) * 100, 2)) + '+-' + str(\n","        np.round(np.std(EACH_ACC_ALL, 0) * 100, 2)) \\\n","                  + \"\\nAverage training time=\" + str(np.round(np.mean(Train_Time_ALL), decimals=2)) + '+-' + str(\n","        np.round(np.std(Train_Time_ALL), decimals=3)) \\\n","                  + \"\\nAverage testing time=\" + str(np.round(np.mean(Test_Time_ALL) * 1000, decimals=2)) + '+-' + str(\n","        np.round(np.std(Test_Time_ALL) * 100, decimals=3))\n","    f.write(str_results)\n","    f.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ATgpQ1R56bW9","executionInfo":{"status":"ok","timestamp":1680560299371,"user_tz":300,"elapsed":127066,"user":{"displayName":"Michael Wesselink","userId":"03807820748818897534"}},"outputId":"c862ebc9-5596-4752-f2aa-76d8ca368fdd"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["save model...\n","epoch: 1, training_sampler_num: 1020, batch_count: 32.00, train loss: 1.283966, tarin loss sum: 41.086925, train acc: 0.640, train_acc_sum: 653.0, time: 8.3 sec\n","save model...\n","epoch: 2, training_sampler_num: 1020, batch_count: 32.00, train loss: 1.237129, tarin loss sum: 39.588129, train acc: 0.569, train_acc_sum: 580.0, time: 0.2 sec\n","save model...\n","epoch: 3, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.526927, tarin loss sum: 16.861676, train acc: 0.818, train_acc_sum: 834.0, time: 0.2 sec\n","save model...\n","epoch: 4, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.402877, tarin loss sum: 12.892078, train acc: 0.845, train_acc_sum: 862.0, time: 0.2 sec\n","epoch: 5, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.369170, tarin loss sum: 11.813446, train acc: 0.869, train_acc_sum: 886.0, time: 0.2 sec\n","epoch: 6, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.228184, tarin loss sum: 7.301884, train acc: 0.925, train_acc_sum: 944.0, time: 0.3 sec\n","save model...\n","epoch: 7, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.285541, tarin loss sum: 9.137321, train acc: 0.904, train_acc_sum: 922.0, time: 0.3 sec\n","epoch: 8, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.148880, tarin loss sum: 4.764148, train acc: 0.957, train_acc_sum: 976.0, time: 0.2 sec\n","epoch: 9, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.106282, tarin loss sum: 3.401025, train acc: 0.970, train_acc_sum: 989.0, time: 0.2 sec\n","epoch: 10, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.087456, tarin loss sum: 2.798601, train acc: 0.972, train_acc_sum: 991.0, time: 0.2 sec\n","epoch: 11, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.152006, tarin loss sum: 4.864181, train acc: 0.956, train_acc_sum: 975.0, time: 0.2 sec\n","save model...\n","epoch: 12, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.072450, tarin loss sum: 2.318393, train acc: 0.980, train_acc_sum: 1000.0, time: 0.2 sec\n","epoch: 13, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.036585, tarin loss sum: 1.170715, train acc: 0.992, train_acc_sum: 1012.0, time: 0.2 sec\n","epoch: 14, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.033005, tarin loss sum: 1.056145, train acc: 0.994, train_acc_sum: 1014.0, time: 0.2 sec\n","save model...\n","epoch: 15, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.047527, tarin loss sum: 1.520872, train acc: 0.985, train_acc_sum: 1005.0, time: 0.2 sec\n","epoch: 16, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.070880, tarin loss sum: 2.268146, train acc: 0.978, train_acc_sum: 998.0, time: 0.2 sec\n","epoch: 17, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.062832, tarin loss sum: 2.010630, train acc: 0.985, train_acc_sum: 1005.0, time: 0.2 sec\n","save model...\n","epoch: 18, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.023528, tarin loss sum: 0.752912, train acc: 0.994, train_acc_sum: 1014.0, time: 0.2 sec\n","epoch: 19, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.029715, tarin loss sum: 0.950877, train acc: 0.990, train_acc_sum: 1010.0, time: 0.2 sec\n","epoch: 20, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.030983, tarin loss sum: 0.991465, train acc: 0.987, train_acc_sum: 1007.0, time: 0.2 sec\n","epoch: 21, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.030594, tarin loss sum: 0.978999, train acc: 0.991, train_acc_sum: 1011.0, time: 0.2 sec\n","epoch: 22, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.028490, tarin loss sum: 0.911669, train acc: 0.992, train_acc_sum: 1012.0, time: 0.2 sec\n","epoch: 23, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.064681, tarin loss sum: 2.069799, train acc: 0.983, train_acc_sum: 1003.0, time: 0.2 sec\n","epoch: 24, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.067749, tarin loss sum: 2.167973, train acc: 0.977, train_acc_sum: 997.0, time: 0.2 sec\n","epoch: 25, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.090154, tarin loss sum: 2.884913, train acc: 0.973, train_acc_sum: 992.0, time: 0.2 sec\n","epoch: 26, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.021148, tarin loss sum: 0.676743, train acc: 0.992, train_acc_sum: 1012.0, time: 0.2 sec\n","epoch: 27, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.006008, tarin loss sum: 0.192256, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 28, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.003770, tarin loss sum: 0.120635, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","save model...\n","epoch: 29, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.002478, tarin loss sum: 0.079292, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 30, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.001049, tarin loss sum: 0.033577, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","save model...\n","epoch: 31, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000936, tarin loss sum: 0.029941, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 32, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000856, tarin loss sum: 0.027396, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 33, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000792, tarin loss sum: 0.025351, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 34, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000702, tarin loss sum: 0.022456, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 35, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000653, tarin loss sum: 0.020894, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 36, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000623, tarin loss sum: 0.019935, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 37, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000564, tarin loss sum: 0.018057, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 38, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000525, tarin loss sum: 0.016791, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","save model...\n","epoch: 39, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000486, tarin loss sum: 0.015560, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 40, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000456, tarin loss sum: 0.014602, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 41, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000427, tarin loss sum: 0.013666, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 42, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000404, tarin loss sum: 0.012918, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 43, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000393, tarin loss sum: 0.012581, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 44, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000369, tarin loss sum: 0.011810, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 45, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000362, tarin loss sum: 0.011570, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","save model...\n","epoch: 46, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000323, tarin loss sum: 0.010343, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","save model...\n","epoch: 47, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000313, tarin loss sum: 0.010010, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 48, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000297, tarin loss sum: 0.009514, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 49, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000283, tarin loss sum: 0.009052, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 50, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000268, tarin loss sum: 0.008591, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 51, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000261, tarin loss sum: 0.008349, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 52, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000246, tarin loss sum: 0.007884, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 53, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000231, tarin loss sum: 0.007408, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 54, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000224, tarin loss sum: 0.007164, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 55, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000213, tarin loss sum: 0.006806, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 56, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000205, tarin loss sum: 0.006556, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 57, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000199, tarin loss sum: 0.006356, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 58, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000191, tarin loss sum: 0.006127, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 59, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000181, tarin loss sum: 0.005790, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 60, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000173, tarin loss sum: 0.005533, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 61, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000170, tarin loss sum: 0.005445, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 62, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000162, tarin loss sum: 0.005188, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","save model...\n","epoch: 63, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000155, tarin loss sum: 0.004957, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 64, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000148, tarin loss sum: 0.004745, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 65, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000143, tarin loss sum: 0.004572, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 66, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000137, tarin loss sum: 0.004393, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 67, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000134, tarin loss sum: 0.004301, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 68, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000132, tarin loss sum: 0.004221, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 69, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000127, tarin loss sum: 0.004050, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 70, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000123, tarin loss sum: 0.003938, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 71, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000119, tarin loss sum: 0.003800, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 72, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000116, tarin loss sum: 0.003719, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 73, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000112, tarin loss sum: 0.003580, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 74, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000104, tarin loss sum: 0.003344, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 75, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000104, tarin loss sum: 0.003338, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 76, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000100, tarin loss sum: 0.003209, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 77, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000095, tarin loss sum: 0.003046, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 78, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000092, tarin loss sum: 0.002957, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 79, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000089, tarin loss sum: 0.002851, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 80, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000087, tarin loss sum: 0.002770, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 81, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000083, tarin loss sum: 0.002658, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 82, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000082, tarin loss sum: 0.002625, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 83, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000079, tarin loss sum: 0.002516, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 84, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000076, tarin loss sum: 0.002436, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 85, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000074, tarin loss sum: 0.002365, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 86, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000072, tarin loss sum: 0.002308, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 87, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000070, tarin loss sum: 0.002233, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 88, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000067, tarin loss sum: 0.002152, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 89, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000066, tarin loss sum: 0.002123, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 90, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000064, tarin loss sum: 0.002058, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 91, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000062, tarin loss sum: 0.001989, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 92, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000061, tarin loss sum: 0.001955, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 93, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000059, tarin loss sum: 0.001900, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 94, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000057, tarin loss sum: 0.001839, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 95, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000057, tarin loss sum: 0.001815, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 96, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000054, tarin loss sum: 0.001731, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 97, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000054, tarin loss sum: 0.001716, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 98, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000051, tarin loss sum: 0.001635, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 99, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000050, tarin loss sum: 0.001608, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 100, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000049, tarin loss sum: 0.001561, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","Training stage finished:\n"," epoch 100, loss 0.0000, train acc 1.000, training time 28.71 s\n","\n","\n","====================Starting evaluation for testing set.========================\n","\n","confusion_matrix\n","[[  41    0    0    0    5    0    0    0    0    0    0    0    0    0\n","     0    0]\n"," [   0 1248    7    0    0    0    0    0    0    4   10    0    0    0\n","     2    0]\n"," [   0    0  730    2    0    0    0    0    0    0    1    0    0    0\n","     0    0]\n"," [   0    2    0  199    0    0    0    0    0    0    0    0    0    0\n","     0    0]\n"," [   0    0    0    0  403    0    3    0    0    3    0    0    0    0\n","     4    0]\n"," [   0    1    0    0    0  644    0    0    0    0    0    0    0    0\n","     1    0]\n"," [   0    0    0    0    0    0   21    0    0    0    0    0    0    0\n","     0    0]\n"," [   0    0    0    0    2    0    0  427    0    0    0    0    0    0\n","     0    0]\n"," [   0    0    0    0    0    0    0    0   16    0    0    0    0    0\n","     0    0]\n"," [   0    2    0    0    0    0    0    0    0  844    4    2    0    0\n","     2    0]\n"," [   0    6    1    0    2    1    0    0    0   10 2170    0    0    0\n","     0    0]\n"," [   0   11    0   11    0    0    0    0    0    2    0  524    0    0\n","     0    0]\n"," [   0    0    1    0    0    0    0    0    0    0    0    0  183    0\n","     0    0]\n"," [   0    2    0    0   16    3    0    0    0    3    1    0    0 1118\n","    11    0]\n"," [   0    0    0    0    3    2    0    0    0    0    0    0    0    9\n","   325    0]\n"," [   0    0    0    0    0    0    0    0    0    0    0    3    0    0\n","     0   83]]\n","classification_report\n","                              precision    recall  f1-score   support\n","\n","                     Alfalfa       0.89      1.00      0.94        41\n","                 Corn-notill       0.98      0.98      0.98      1272\n","                Corn-mintill       1.00      0.99      0.99       739\n","                        Corn       0.99      0.94      0.96       212\n","               Grass-pasture       0.98      0.94      0.95       431\n","                 Grass-trees       1.00      0.99      0.99       650\n","         Grass-pasture-mowed       1.00      0.88      0.93        24\n","               Hay-windrowed       1.00      1.00      1.00       427\n","                        Oats       1.00      1.00      1.00        16\n","              Soybean-notill       0.99      0.97      0.98       866\n","             Soybean-mintill       0.99      0.99      0.99      2186\n","               Soybean-clean       0.96      0.99      0.97       529\n","                       Wheat       0.99      1.00      1.00       183\n","                       Woods       0.97      0.99      0.98      1127\n","Buildings-Grass-Trees-Drives       0.96      0.94      0.95       345\n","          Stone-Steel-Towers       0.97      1.00      0.98        83\n","\n","                    accuracy                           0.98      9131\n","                   macro avg       0.98      0.98      0.98      9131\n","                weighted avg       0.98      0.98      0.98      9131\n","\n","save model...\n","epoch: 1, training_sampler_num: 1020, batch_count: 32.00, train loss: 1.324551, tarin loss sum: 42.385631, train acc: 0.645, train_acc_sum: 658.0, time: 0.2 sec\n","save model...\n","epoch: 2, training_sampler_num: 1020, batch_count: 32.00, train loss: 1.369821, tarin loss sum: 43.834261, train acc: 0.533, train_acc_sum: 544.0, time: 0.2 sec\n","save model...\n","epoch: 3, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.640591, tarin loss sum: 20.498906, train acc: 0.775, train_acc_sum: 790.0, time: 0.2 sec\n","save model...\n","epoch: 4, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.495524, tarin loss sum: 15.856772, train acc: 0.825, train_acc_sum: 842.0, time: 0.2 sec\n","save model...\n","epoch: 5, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.371723, tarin loss sum: 11.895151, train acc: 0.870, train_acc_sum: 887.0, time: 0.2 sec\n","epoch: 6, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.197800, tarin loss sum: 6.329598, train acc: 0.938, train_acc_sum: 957.0, time: 0.2 sec\n","epoch: 7, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.223538, tarin loss sum: 7.153231, train acc: 0.919, train_acc_sum: 937.0, time: 0.2 sec\n","epoch: 8, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.152998, tarin loss sum: 4.895923, train acc: 0.948, train_acc_sum: 967.0, time: 0.2 sec\n","epoch: 9, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.105337, tarin loss sum: 3.370780, train acc: 0.967, train_acc_sum: 986.0, time: 0.2 sec\n","epoch: 10, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.105511, tarin loss sum: 3.376360, train acc: 0.966, train_acc_sum: 985.0, time: 0.2 sec\n","epoch: 11, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.046608, tarin loss sum: 1.491461, train acc: 0.988, train_acc_sum: 1008.0, time: 0.2 sec\n","epoch: 12, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.085806, tarin loss sum: 2.745783, train acc: 0.973, train_acc_sum: 992.0, time: 0.2 sec\n","epoch: 13, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.115865, tarin loss sum: 3.707684, train acc: 0.957, train_acc_sum: 976.0, time: 0.2 sec\n","save model...\n","epoch: 14, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.050317, tarin loss sum: 1.610145, train acc: 0.987, train_acc_sum: 1007.0, time: 0.2 sec\n","epoch: 15, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.021712, tarin loss sum: 0.694788, train acc: 0.997, train_acc_sum: 1017.0, time: 0.2 sec\n","epoch: 16, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.012371, tarin loss sum: 0.395883, train acc: 0.999, train_acc_sum: 1019.0, time: 0.2 sec\n","save model...\n","epoch: 17, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.048943, tarin loss sum: 1.566181, train acc: 0.983, train_acc_sum: 1003.0, time: 0.2 sec\n","epoch: 18, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.027113, tarin loss sum: 0.867616, train acc: 0.990, train_acc_sum: 1010.0, time: 0.2 sec\n","save model...\n","epoch: 19, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.014651, tarin loss sum: 0.468826, train acc: 0.996, train_acc_sum: 1016.0, time: 0.2 sec\n","epoch: 20, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.026270, tarin loss sum: 0.840642, train acc: 0.992, train_acc_sum: 1012.0, time: 0.2 sec\n","epoch: 21, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.073847, tarin loss sum: 2.363095, train acc: 0.976, train_acc_sum: 996.0, time: 0.2 sec\n","epoch: 22, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.057905, tarin loss sum: 1.852954, train acc: 0.982, train_acc_sum: 1002.0, time: 0.2 sec\n","epoch: 23, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.020882, tarin loss sum: 0.668209, train acc: 0.995, train_acc_sum: 1015.0, time: 0.2 sec\n","epoch: 24, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.018236, tarin loss sum: 0.583537, train acc: 0.996, train_acc_sum: 1016.0, time: 0.2 sec\n","epoch: 25, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.034733, tarin loss sum: 1.111444, train acc: 0.992, train_acc_sum: 1012.0, time: 0.2 sec\n","epoch: 26, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.009985, tarin loss sum: 0.319506, train acc: 0.999, train_acc_sum: 1019.0, time: 0.2 sec\n","epoch: 27, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.006211, tarin loss sum: 0.198747, train acc: 0.999, train_acc_sum: 1019.0, time: 0.2 sec\n","epoch: 28, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.006725, tarin loss sum: 0.215190, train acc: 0.997, train_acc_sum: 1017.0, time: 0.2 sec\n","save model...\n","epoch: 29, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.001590, tarin loss sum: 0.050885, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 30, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.001615, tarin loss sum: 0.051665, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 31, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.001399, tarin loss sum: 0.044781, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 32, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000856, tarin loss sum: 0.027400, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 33, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000782, tarin loss sum: 0.025018, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 34, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000732, tarin loss sum: 0.023428, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 35, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000717, tarin loss sum: 0.022944, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 36, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000645, tarin loss sum: 0.020632, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 37, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000560, tarin loss sum: 0.017930, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 38, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000521, tarin loss sum: 0.016656, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 39, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000480, tarin loss sum: 0.015372, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 40, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000456, tarin loss sum: 0.014602, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 41, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000426, tarin loss sum: 0.013624, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 42, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000413, tarin loss sum: 0.013229, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 43, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000392, tarin loss sum: 0.012538, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 44, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000362, tarin loss sum: 0.011588, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 45, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000351, tarin loss sum: 0.011240, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 46, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000332, tarin loss sum: 0.010614, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 47, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000323, tarin loss sum: 0.010338, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","save model...\n","epoch: 48, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000302, tarin loss sum: 0.009671, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 49, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000293, tarin loss sum: 0.009366, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 50, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000269, tarin loss sum: 0.008599, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 51, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000254, tarin loss sum: 0.008124, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 52, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000248, tarin loss sum: 0.007937, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 53, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000239, tarin loss sum: 0.007642, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 54, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000229, tarin loss sum: 0.007337, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 55, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000217, tarin loss sum: 0.006931, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 56, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000206, tarin loss sum: 0.006590, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 57, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000206, tarin loss sum: 0.006583, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 58, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000197, tarin loss sum: 0.006314, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 59, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000181, tarin loss sum: 0.005806, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 60, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000180, tarin loss sum: 0.005765, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 61, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000170, tarin loss sum: 0.005424, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 62, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000165, tarin loss sum: 0.005268, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 63, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000158, tarin loss sum: 0.005042, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 64, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000152, tarin loss sum: 0.004855, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 65, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000148, tarin loss sum: 0.004728, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 66, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000141, tarin loss sum: 0.004516, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 67, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000139, tarin loss sum: 0.004453, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 68, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000136, tarin loss sum: 0.004367, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 69, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000134, tarin loss sum: 0.004273, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 70, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000124, tarin loss sum: 0.003966, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 71, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000122, tarin loss sum: 0.003901, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 72, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000116, tarin loss sum: 0.003703, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 73, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000110, tarin loss sum: 0.003520, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 74, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000111, tarin loss sum: 0.003557, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 75, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000103, tarin loss sum: 0.003308, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 76, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000104, tarin loss sum: 0.003332, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 77, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000098, tarin loss sum: 0.003147, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 78, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000094, tarin loss sum: 0.003009, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 79, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000093, tarin loss sum: 0.002960, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 80, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000090, tarin loss sum: 0.002865, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 81, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000085, tarin loss sum: 0.002717, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 82, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000085, tarin loss sum: 0.002707, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 83, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000083, tarin loss sum: 0.002654, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 84, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000081, tarin loss sum: 0.002581, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 85, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000076, tarin loss sum: 0.002444, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 86, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000074, tarin loss sum: 0.002366, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 87, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000071, tarin loss sum: 0.002274, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 88, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000070, tarin loss sum: 0.002244, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 89, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000071, tarin loss sum: 0.002261, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 90, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000067, tarin loss sum: 0.002130, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 91, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000064, tarin loss sum: 0.002039, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 92, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000062, tarin loss sum: 0.001996, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 93, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000060, tarin loss sum: 0.001935, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 94, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000059, tarin loss sum: 0.001874, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 95, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000057, tarin loss sum: 0.001815, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 96, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000055, tarin loss sum: 0.001752, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 97, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000054, tarin loss sum: 0.001742, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 98, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000052, tarin loss sum: 0.001666, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 99, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000051, tarin loss sum: 0.001648, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 100, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000050, tarin loss sum: 0.001591, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","Training stage finished:\n"," epoch 100, loss 0.0000, train acc 1.000, training time 21.00 s\n","\n","\n","====================Starting evaluation for testing set.========================\n","\n","confusion_matrix\n","[[  39    0    0    0    0    0    0    0    0    6    0    0    0    0\n","     0    0]\n"," [   0 1219    0    0    0    0    0    0    0    5   28    0    0    0\n","     2    0]\n"," [   0    2  735    0    0    0    0    0    0    0    0    0    0    0\n","     0    0]\n"," [   0    0    0  210    9    2    0    0    0    0    0    1    0    0\n","     0    0]\n"," [   0    0    0    0  400    0    0    0    0    3    1    0    0    0\n","     0    0]\n"," [   0    1    0    0    0  644    0    0    0    0    0    0    0    0\n","     0    0]\n"," [   0    0    0    0    8    0   24    0    0    0    0    0    0    0\n","     0    0]\n"," [   0    0    0    0    0    0    0  427    0    0    0    0    0    0\n","     0    0]\n"," [   0    0    0    0    0    0    0    0   16    0    0    0    0    0\n","     0    0]\n"," [   0   11    1    0    0    0    0    0    0  842    8    0    0    0\n","     0    0]\n"," [   0   36    3    0    3    0    0    0    0    1 2145    0    1    0\n","     0    0]\n"," [   0    0    0    2    0    0    0    0    0    6    0  524    0    0\n","     0    3]\n"," [   0    0    0    0    0    0    0    0    0    0    0    0  182    0\n","     0    0]\n"," [   2    3    0    0   11    3    0    0    0    1    4    0    0 1103\n","     0    0]\n"," [   0    0    0    0    0    1    0    0    0    2    0    0    0   24\n","   343    0]\n"," [   0    0    0    0    0    0    0    0    0    0    0    4    0    0\n","     0   80]]\n","classification_report\n","                              precision    recall  f1-score   support\n","\n","                     Alfalfa       0.87      0.95      0.91        41\n","                 Corn-notill       0.97      0.96      0.97      1272\n","                Corn-mintill       1.00      0.99      1.00       739\n","                        Corn       0.95      0.99      0.97       212\n","               Grass-pasture       0.99      0.93      0.96       431\n","                 Grass-trees       1.00      0.99      0.99       650\n","         Grass-pasture-mowed       0.75      1.00      0.86        24\n","               Hay-windrowed       1.00      1.00      1.00       427\n","                        Oats       1.00      1.00      1.00        16\n","              Soybean-notill       0.98      0.97      0.97       866\n","             Soybean-mintill       0.98      0.98      0.98      2186\n","               Soybean-clean       0.98      0.99      0.98       529\n","                       Wheat       1.00      0.99      1.00       183\n","                       Woods       0.98      0.98      0.98      1127\n","Buildings-Grass-Trees-Drives       0.93      0.99      0.96       345\n","          Stone-Steel-Towers       0.95      0.96      0.96        83\n","\n","                    accuracy                           0.98      9131\n","                   macro avg       0.96      0.98      0.97      9131\n","                weighted avg       0.98      0.98      0.98      9131\n","\n","save model...\n","epoch: 1, training_sampler_num: 1020, batch_count: 32.00, train loss: 1.278391, tarin loss sum: 40.908527, train acc: 0.629, train_acc_sum: 642.0, time: 0.2 sec\n","save model...\n","epoch: 2, training_sampler_num: 1020, batch_count: 32.00, train loss: 1.216069, tarin loss sum: 38.914209, train acc: 0.581, train_acc_sum: 593.0, time: 0.2 sec\n","epoch: 3, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.671937, tarin loss sum: 21.501986, train acc: 0.765, train_acc_sum: 780.0, time: 0.2 sec\n","save model...\n","epoch: 4, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.500557, tarin loss sum: 16.017809, train acc: 0.813, train_acc_sum: 829.0, time: 0.2 sec\n","epoch: 5, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.332236, tarin loss sum: 10.631549, train acc: 0.882, train_acc_sum: 900.0, time: 0.2 sec\n","epoch: 6, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.234227, tarin loss sum: 7.495276, train acc: 0.920, train_acc_sum: 938.0, time: 0.2 sec\n","save model...\n","epoch: 7, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.129116, tarin loss sum: 4.131710, train acc: 0.962, train_acc_sum: 981.0, time: 0.2 sec\n","epoch: 8, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.107153, tarin loss sum: 3.428889, train acc: 0.964, train_acc_sum: 983.0, time: 0.2 sec\n","epoch: 9, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.170323, tarin loss sum: 5.450324, train acc: 0.950, train_acc_sum: 969.0, time: 0.2 sec\n","save model...\n","epoch: 10, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.097460, tarin loss sum: 3.118714, train acc: 0.972, train_acc_sum: 991.0, time: 0.2 sec\n","save model...\n","epoch: 11, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.077072, tarin loss sum: 2.466306, train acc: 0.976, train_acc_sum: 996.0, time: 0.2 sec\n","save model...\n","epoch: 12, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.041732, tarin loss sum: 1.335410, train acc: 0.992, train_acc_sum: 1012.0, time: 0.2 sec\n","epoch: 13, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.029300, tarin loss sum: 0.937605, train acc: 0.995, train_acc_sum: 1015.0, time: 0.2 sec\n","epoch: 14, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.125710, tarin loss sum: 4.022726, train acc: 0.967, train_acc_sum: 986.0, time: 0.2 sec\n","epoch: 15, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.108778, tarin loss sum: 3.480891, train acc: 0.957, train_acc_sum: 976.0, time: 0.2 sec\n","epoch: 16, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.053052, tarin loss sum: 1.697654, train acc: 0.976, train_acc_sum: 996.0, time: 0.2 sec\n","epoch: 17, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.029824, tarin loss sum: 0.954373, train acc: 0.993, train_acc_sum: 1013.0, time: 0.2 sec\n","epoch: 18, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.009473, tarin loss sum: 0.303130, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 19, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.005976, tarin loss sum: 0.191238, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 20, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.004643, tarin loss sum: 0.148566, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 21, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.008337, tarin loss sum: 0.266770, train acc: 0.998, train_acc_sum: 1018.0, time: 0.2 sec\n","epoch: 22, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.010731, tarin loss sum: 0.343378, train acc: 0.998, train_acc_sum: 1018.0, time: 0.2 sec\n","epoch: 23, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.030775, tarin loss sum: 0.984816, train acc: 0.994, train_acc_sum: 1014.0, time: 0.2 sec\n","epoch: 24, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.024072, tarin loss sum: 0.770312, train acc: 0.995, train_acc_sum: 1015.0, time: 0.2 sec\n","epoch: 25, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.021002, tarin loss sum: 0.672076, train acc: 0.996, train_acc_sum: 1016.0, time: 0.2 sec\n","epoch: 26, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.006655, tarin loss sum: 0.212975, train acc: 0.998, train_acc_sum: 1018.0, time: 0.2 sec\n","save model...\n","epoch: 27, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.004431, tarin loss sum: 0.141778, train acc: 0.999, train_acc_sum: 1019.0, time: 0.2 sec\n","epoch: 28, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.010476, tarin loss sum: 0.335228, train acc: 0.997, train_acc_sum: 1017.0, time: 0.2 sec\n","epoch: 29, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.270776, tarin loss sum: 8.664847, train acc: 0.925, train_acc_sum: 944.0, time: 0.2 sec\n","epoch: 30, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.223953, tarin loss sum: 7.166481, train acc: 0.931, train_acc_sum: 950.0, time: 0.2 sec\n","epoch: 31, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.069312, tarin loss sum: 2.217996, train acc: 0.981, train_acc_sum: 1001.0, time: 0.2 sec\n","epoch: 32, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.035479, tarin loss sum: 1.135335, train acc: 0.986, train_acc_sum: 1006.0, time: 0.2 sec\n","epoch: 33, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.016650, tarin loss sum: 0.532814, train acc: 0.997, train_acc_sum: 1017.0, time: 0.2 sec\n","epoch: 34, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.004140, tarin loss sum: 0.132471, train acc: 0.999, train_acc_sum: 1019.0, time: 0.2 sec\n","epoch: 35, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.002481, tarin loss sum: 0.079382, train acc: 1.000, train_acc_sum: 1020.0, time: 0.3 sec\n","epoch: 36, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.001556, tarin loss sum: 0.049791, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 37, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.001370, tarin loss sum: 0.043842, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 38, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.001179, tarin loss sum: 0.037740, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 39, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.001032, tarin loss sum: 0.033021, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 40, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000922, tarin loss sum: 0.029514, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 41, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000852, tarin loss sum: 0.027264, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 42, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000765, tarin loss sum: 0.024494, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 43, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000723, tarin loss sum: 0.023127, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 44, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000662, tarin loss sum: 0.021175, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 45, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000619, tarin loss sum: 0.019801, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 46, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000575, tarin loss sum: 0.018392, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 47, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000538, tarin loss sum: 0.017207, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 48, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000502, tarin loss sum: 0.016072, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 49, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000477, tarin loss sum: 0.015251, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 50, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000468, tarin loss sum: 0.014972, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 51, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000419, tarin loss sum: 0.013409, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 52, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000410, tarin loss sum: 0.013113, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 53, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000388, tarin loss sum: 0.012421, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 54, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000368, tarin loss sum: 0.011777, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 55, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000354, tarin loss sum: 0.011326, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 56, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000332, tarin loss sum: 0.010617, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 57, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000322, tarin loss sum: 0.010295, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 58, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000305, tarin loss sum: 0.009762, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 59, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000288, tarin loss sum: 0.009228, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 60, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000274, tarin loss sum: 0.008782, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 61, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000262, tarin loss sum: 0.008393, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 62, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000252, tarin loss sum: 0.008055, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 63, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000242, tarin loss sum: 0.007736, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 64, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000232, tarin loss sum: 0.007423, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 65, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000222, tarin loss sum: 0.007108, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 66, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000221, tarin loss sum: 0.007060, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 67, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000219, tarin loss sum: 0.007008, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 68, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000199, tarin loss sum: 0.006371, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 69, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000191, tarin loss sum: 0.006100, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 70, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000182, tarin loss sum: 0.005821, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 71, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000175, tarin loss sum: 0.005604, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 72, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000169, tarin loss sum: 0.005407, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 73, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000164, tarin loss sum: 0.005256, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 74, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000159, tarin loss sum: 0.005075, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 75, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000154, tarin loss sum: 0.004940, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 76, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000148, tarin loss sum: 0.004751, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 77, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000145, tarin loss sum: 0.004653, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 78, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000140, tarin loss sum: 0.004466, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 79, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000133, tarin loss sum: 0.004265, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 80, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000130, tarin loss sum: 0.004158, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 81, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000128, tarin loss sum: 0.004103, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 82, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000122, tarin loss sum: 0.003918, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 83, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000119, tarin loss sum: 0.003813, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 84, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000113, tarin loss sum: 0.003622, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 85, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000111, tarin loss sum: 0.003547, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 86, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000107, tarin loss sum: 0.003421, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 87, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000105, tarin loss sum: 0.003369, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 88, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000102, tarin loss sum: 0.003260, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 89, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000098, tarin loss sum: 0.003141, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 90, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000096, tarin loss sum: 0.003056, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 91, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000093, tarin loss sum: 0.002985, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 92, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000090, tarin loss sum: 0.002883, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 93, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000087, tarin loss sum: 0.002794, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 94, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000086, tarin loss sum: 0.002737, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 95, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000082, tarin loss sum: 0.002626, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 96, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000080, tarin loss sum: 0.002566, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 97, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000078, tarin loss sum: 0.002486, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 98, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000075, tarin loss sum: 0.002416, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 99, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000074, tarin loss sum: 0.002375, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 100, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000071, tarin loss sum: 0.002280, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","Training stage finished:\n"," epoch 100, loss 0.0001, train acc 1.000, training time 21.15 s\n","\n","\n","====================Starting evaluation for testing set.========================\n","\n","confusion_matrix\n","[[  38    0    0    0    0    0    0    0    0    4    0    0    0    0\n","     0    0]\n"," [   3 1237    5    0    0    1    0    0    0    3   25    0    0    0\n","    10    1]\n"," [   0    0  730    0    0    0    0    0    0    0    0    6    0    0\n","     0    0]\n"," [   0    0    1  212    0    6    0    0    0    0    0   18    0    0\n","     0    0]\n"," [   0    0    0    0  426    1    3    0    0   15    0    0    0    0\n","     0    0]\n"," [   0    0    0    0    0  637    0    1    0    0    0    0    0    0\n","     0    0]\n"," [   0    0    0    0    0    0   21    0    0    0    0    0    0    0\n","     0    0]\n"," [   0    0    0    0    0    0    0  426    0    0    0    0    0    0\n","     0    0]\n"," [   0    0    0    0    0    0    0    0   16    0    0    0    0    0\n","     0    0]\n"," [   0   15    0    0    0    0    0    0    0  825   11    0    0    0\n","     0    0]\n"," [   0   19    0    0    1    4    0    0    0    9 2148    1    0    0\n","     0    0]\n"," [   0    0    3    0    0    0    0    0    0    5    0  504    0    0\n","     0    8]\n"," [   0    0    0    0    0    0    0    0    0    0    0    0  183    0\n","     0    0]\n"," [   0    1    0    0    3    1    0    0    0    1    1    0    0 1114\n","    21    0]\n"," [   0    0    0    0    1    0    0    0    0    4    0    0    0   13\n","   314    0]\n"," [   0    0    0    0    0    0    0    0    0    0    1    0    0    0\n","     0   74]]\n","classification_report\n","                              precision    recall  f1-score   support\n","\n","                     Alfalfa       0.90      0.93      0.92        41\n","                 Corn-notill       0.96      0.97      0.97      1272\n","                Corn-mintill       0.99      0.99      0.99       739\n","                        Corn       0.89      1.00      0.94       212\n","               Grass-pasture       0.96      0.99      0.97       431\n","                 Grass-trees       1.00      0.98      0.99       650\n","         Grass-pasture-mowed       1.00      0.88      0.93        24\n","               Hay-windrowed       1.00      1.00      1.00       427\n","                        Oats       1.00      1.00      1.00        16\n","              Soybean-notill       0.97      0.95      0.96       866\n","             Soybean-mintill       0.98      0.98      0.98      2186\n","               Soybean-clean       0.97      0.95      0.96       529\n","                       Wheat       1.00      1.00      1.00       183\n","                       Woods       0.98      0.99      0.98      1127\n","Buildings-Grass-Trees-Drives       0.95      0.91      0.93       345\n","          Stone-Steel-Towers       0.99      0.89      0.94        83\n","\n","                    accuracy                           0.98      9131\n","                   macro avg       0.97      0.96      0.97      9131\n","                weighted avg       0.98      0.98      0.98      9131\n","\n","save model...\n","epoch: 1, training_sampler_num: 1020, batch_count: 32.00, train loss: 1.281939, tarin loss sum: 41.022041, train acc: 0.650, train_acc_sum: 663.0, time: 0.2 sec\n","save model...\n","epoch: 2, training_sampler_num: 1020, batch_count: 32.00, train loss: 1.263319, tarin loss sum: 40.426214, train acc: 0.557, train_acc_sum: 568.0, time: 0.2 sec\n","save model...\n","epoch: 3, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.624403, tarin loss sum: 19.980895, train acc: 0.778, train_acc_sum: 794.0, time: 0.2 sec\n","save model...\n","epoch: 4, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.533855, tarin loss sum: 17.083370, train acc: 0.818, train_acc_sum: 834.0, time: 0.2 sec\n","save model...\n","epoch: 5, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.367241, tarin loss sum: 11.751722, train acc: 0.880, train_acc_sum: 898.0, time: 0.2 sec\n","epoch: 6, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.246729, tarin loss sum: 7.895337, train acc: 0.915, train_acc_sum: 933.0, time: 0.2 sec\n","save model...\n","epoch: 7, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.205064, tarin loss sum: 6.562052, train acc: 0.927, train_acc_sum: 946.0, time: 0.2 sec\n","save model...\n","epoch: 8, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.138573, tarin loss sum: 4.434332, train acc: 0.948, train_acc_sum: 967.0, time: 0.2 sec\n","epoch: 9, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.153124, tarin loss sum: 4.899969, train acc: 0.953, train_acc_sum: 972.0, time: 0.2 sec\n","epoch: 10, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.138915, tarin loss sum: 4.445295, train acc: 0.956, train_acc_sum: 975.0, time: 0.2 sec\n","epoch: 11, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.061738, tarin loss sum: 1.975615, train acc: 0.983, train_acc_sum: 1003.0, time: 0.2 sec\n","epoch: 12, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.059124, tarin loss sum: 1.891954, train acc: 0.979, train_acc_sum: 999.0, time: 0.2 sec\n","epoch: 13, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.072458, tarin loss sum: 2.318659, train acc: 0.973, train_acc_sum: 992.0, time: 0.2 sec\n","epoch: 14, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.095414, tarin loss sum: 3.053245, train acc: 0.971, train_acc_sum: 990.0, time: 0.2 sec\n","epoch: 15, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.081876, tarin loss sum: 2.620029, train acc: 0.975, train_acc_sum: 995.0, time: 0.2 sec\n","epoch: 16, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.168722, tarin loss sum: 5.399091, train acc: 0.954, train_acc_sum: 973.0, time: 0.2 sec\n","epoch: 17, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.037791, tarin loss sum: 1.209317, train acc: 0.995, train_acc_sum: 1015.0, time: 0.2 sec\n","epoch: 18, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.035144, tarin loss sum: 1.124596, train acc: 0.988, train_acc_sum: 1008.0, time: 0.2 sec\n","epoch: 19, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.031291, tarin loss sum: 1.001302, train acc: 0.987, train_acc_sum: 1007.0, time: 0.2 sec\n","epoch: 20, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.021121, tarin loss sum: 0.675861, train acc: 0.993, train_acc_sum: 1013.0, time: 0.2 sec\n","epoch: 21, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.015457, tarin loss sum: 0.494628, train acc: 0.997, train_acc_sum: 1017.0, time: 0.2 sec\n","save model...\n","epoch: 22, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.013477, tarin loss sum: 0.431260, train acc: 0.995, train_acc_sum: 1015.0, time: 0.2 sec\n","epoch: 23, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.042063, tarin loss sum: 1.346029, train acc: 0.990, train_acc_sum: 1010.0, time: 0.2 sec\n","epoch: 24, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.234615, tarin loss sum: 7.507692, train acc: 0.965, train_acc_sum: 984.0, time: 0.2 sec\n","epoch: 25, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.176630, tarin loss sum: 5.652174, train acc: 0.933, train_acc_sum: 952.0, time: 0.2 sec\n","epoch: 26, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.199698, tarin loss sum: 6.390338, train acc: 0.935, train_acc_sum: 954.0, time: 0.2 sec\n","epoch: 27, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.059255, tarin loss sum: 1.896162, train acc: 0.981, train_acc_sum: 1001.0, time: 0.2 sec\n","save model...\n","epoch: 28, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.044764, tarin loss sum: 1.432454, train acc: 0.984, train_acc_sum: 1004.0, time: 0.2 sec\n","epoch: 29, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.012830, tarin loss sum: 0.410559, train acc: 0.995, train_acc_sum: 1015.0, time: 0.2 sec\n","epoch: 30, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.017928, tarin loss sum: 0.573697, train acc: 0.992, train_acc_sum: 1012.0, time: 0.2 sec\n","epoch: 31, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.005434, tarin loss sum: 0.173896, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 32, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.003631, tarin loss sum: 0.116198, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 33, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.003489, tarin loss sum: 0.111638, train acc: 0.999, train_acc_sum: 1019.0, time: 0.2 sec\n","epoch: 34, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.002747, tarin loss sum: 0.087918, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 35, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.006987, tarin loss sum: 0.223581, train acc: 0.998, train_acc_sum: 1018.0, time: 0.2 sec\n","epoch: 36, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.004391, tarin loss sum: 0.140515, train acc: 0.999, train_acc_sum: 1019.0, time: 0.2 sec\n","epoch: 37, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.003340, tarin loss sum: 0.106867, train acc: 0.999, train_acc_sum: 1019.0, time: 0.2 sec\n","save model...\n","epoch: 38, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.001288, tarin loss sum: 0.041230, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 39, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.001187, tarin loss sum: 0.037984, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 40, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.001285, tarin loss sum: 0.041106, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 41, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.001229, tarin loss sum: 0.039315, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 42, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.001067, tarin loss sum: 0.034129, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 43, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000884, tarin loss sum: 0.028294, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 44, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000763, tarin loss sum: 0.024431, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 45, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000774, tarin loss sum: 0.024759, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 46, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000703, tarin loss sum: 0.022493, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 47, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000718, tarin loss sum: 0.022975, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 48, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000665, tarin loss sum: 0.021293, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 49, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000665, tarin loss sum: 0.021281, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 50, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000514, tarin loss sum: 0.016461, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 51, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000559, tarin loss sum: 0.017891, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 52, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000490, tarin loss sum: 0.015690, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 53, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000461, tarin loss sum: 0.014761, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 54, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000499, tarin loss sum: 0.015960, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 55, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000511, tarin loss sum: 0.016338, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 56, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000657, tarin loss sum: 0.021014, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 57, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000401, tarin loss sum: 0.012841, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 58, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000348, tarin loss sum: 0.011122, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 59, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000325, tarin loss sum: 0.010389, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 60, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000334, tarin loss sum: 0.010700, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","save model...\n","epoch: 61, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000316, tarin loss sum: 0.010115, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 62, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000288, tarin loss sum: 0.009215, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","save model...\n","epoch: 63, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000261, tarin loss sum: 0.008348, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 64, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000268, tarin loss sum: 0.008568, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 65, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000249, tarin loss sum: 0.007952, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 66, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000232, tarin loss sum: 0.007419, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","save model...\n","epoch: 67, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000223, tarin loss sum: 0.007135, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","save model...\n","epoch: 68, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000213, tarin loss sum: 0.006810, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 69, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000218, tarin loss sum: 0.006986, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 70, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000199, tarin loss sum: 0.006381, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 71, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000197, tarin loss sum: 0.006299, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 72, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000192, tarin loss sum: 0.006134, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 73, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000188, tarin loss sum: 0.006024, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 74, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000175, tarin loss sum: 0.005606, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 75, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000180, tarin loss sum: 0.005773, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 76, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000169, tarin loss sum: 0.005420, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 77, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000157, tarin loss sum: 0.005019, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 78, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000151, tarin loss sum: 0.004846, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 79, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000148, tarin loss sum: 0.004737, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 80, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000143, tarin loss sum: 0.004582, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 81, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000137, tarin loss sum: 0.004397, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 82, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000136, tarin loss sum: 0.004352, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 83, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000130, tarin loss sum: 0.004174, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 84, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000121, tarin loss sum: 0.003882, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 85, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000120, tarin loss sum: 0.003832, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 86, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000116, tarin loss sum: 0.003721, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 87, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000112, tarin loss sum: 0.003597, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 88, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000107, tarin loss sum: 0.003411, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 89, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000105, tarin loss sum: 0.003355, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 90, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000100, tarin loss sum: 0.003211, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 91, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000097, tarin loss sum: 0.003095, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 92, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000098, tarin loss sum: 0.003131, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 93, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000105, tarin loss sum: 0.003355, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 94, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000100, tarin loss sum: 0.003184, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 95, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000090, tarin loss sum: 0.002878, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 96, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000087, tarin loss sum: 0.002769, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 97, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000086, tarin loss sum: 0.002757, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 98, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000081, tarin loss sum: 0.002577, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 99, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000078, tarin loss sum: 0.002505, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 100, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000074, tarin loss sum: 0.002359, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","Training stage finished:\n"," epoch 100, loss 0.0001, train acc 1.000, training time 21.14 s\n","\n","\n","====================Starting evaluation for testing set.========================\n","\n","confusion_matrix\n","[[  38    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0]\n"," [   1 1257    3    0    0    0    0    0    0    2   23    0    0    0\n","     0    0]\n"," [   0    7  726    0    0    0    0    0    0    0    3    0    0    0\n","     0    0]\n"," [   0    0    1  210    0    0    0    0    0    0    0    3    0    0\n","     0    0]\n"," [   0    0    4    0  418    1    1    0    0    0    0    0    1    0\n","     0    0]\n"," [   0    2    0    0    0  648    0    0    0    0    1    0    0    0\n","     0    0]\n"," [   0    0    0    0    0    0   23    0    0    0    0    0    0    0\n","     0    0]\n"," [   1    0    0    0    0    0    0  427    0    0    0    0    0    1\n","     0    0]\n"," [   0    0    0    0    0    0    0    0   16    0    0    1    0    0\n","     0    0]\n"," [   1    6    1    0    0    0    0    0    0  830    2    2    0    0\n","     0    0]\n"," [   0    0    1    0    1    1    0    0    0   18 2157    0    0    1\n","     0    0]\n"," [   0    0    2    2    0    0    0    0    0    5    0  522    0    0\n","     0    2]\n"," [   0    0    0    0    0    0    0    0    0    0    0    0  182    0\n","     0    0]\n"," [   0    0    0    0   12    0    0    0    0    0    0    0    0 1114\n","    13    0]\n"," [   0    0    1    0    0    0    0    0    0   11    0    0    0   11\n","   332    0]\n"," [   0    0    0    0    0    0    0    0    0    0    0    1    0    0\n","     0   81]]\n","classification_report\n","                              precision    recall  f1-score   support\n","\n","                     Alfalfa       1.00      0.93      0.96        41\n","                 Corn-notill       0.98      0.99      0.98      1272\n","                Corn-mintill       0.99      0.98      0.98       739\n","                        Corn       0.98      0.99      0.99       212\n","               Grass-pasture       0.98      0.97      0.98       431\n","                 Grass-trees       1.00      1.00      1.00       650\n","         Grass-pasture-mowed       1.00      0.96      0.98        24\n","               Hay-windrowed       1.00      1.00      1.00       427\n","                        Oats       0.94      1.00      0.97        16\n","              Soybean-notill       0.99      0.96      0.97       866\n","             Soybean-mintill       0.99      0.99      0.99      2186\n","               Soybean-clean       0.98      0.99      0.98       529\n","                       Wheat       1.00      0.99      1.00       183\n","                       Woods       0.98      0.99      0.98      1127\n","Buildings-Grass-Trees-Drives       0.94      0.96      0.95       345\n","          Stone-Steel-Towers       0.99      0.98      0.98        83\n","\n","                    accuracy                           0.98      9131\n","                   macro avg       0.98      0.98      0.98      9131\n","                weighted avg       0.98      0.98      0.98      9131\n","\n","save model...\n","epoch: 1, training_sampler_num: 1020, batch_count: 32.00, train loss: 1.310348, tarin loss sum: 41.931151, train acc: 0.607, train_acc_sum: 619.0, time: 0.2 sec\n","save model...\n","epoch: 2, training_sampler_num: 1020, batch_count: 32.00, train loss: 1.344472, tarin loss sum: 43.023114, train acc: 0.532, train_acc_sum: 543.0, time: 0.2 sec\n","save model...\n","epoch: 3, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.619518, tarin loss sum: 19.824592, train acc: 0.785, train_acc_sum: 801.0, time: 0.2 sec\n","epoch: 4, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.380943, tarin loss sum: 12.190163, train acc: 0.870, train_acc_sum: 887.0, time: 0.2 sec\n","save model...\n","epoch: 5, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.380082, tarin loss sum: 12.162640, train acc: 0.865, train_acc_sum: 882.0, time: 0.2 sec\n","epoch: 6, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.243660, tarin loss sum: 7.797126, train acc: 0.925, train_acc_sum: 943.0, time: 0.2 sec\n","epoch: 7, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.167854, tarin loss sum: 5.371338, train acc: 0.955, train_acc_sum: 974.0, time: 0.2 sec\n","epoch: 8, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.176678, tarin loss sum: 5.653681, train acc: 0.943, train_acc_sum: 962.0, time: 0.2 sec\n","save model...\n","epoch: 9, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.111632, tarin loss sum: 3.572217, train acc: 0.967, train_acc_sum: 986.0, time: 0.2 sec\n","epoch: 10, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.140224, tarin loss sum: 4.487173, train acc: 0.960, train_acc_sum: 979.0, time: 0.2 sec\n","epoch: 11, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.101549, tarin loss sum: 3.249557, train acc: 0.966, train_acc_sum: 985.0, time: 0.2 sec\n","epoch: 12, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.044187, tarin loss sum: 1.413992, train acc: 0.990, train_acc_sum: 1010.0, time: 0.2 sec\n","save model...\n","epoch: 13, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.032879, tarin loss sum: 1.052113, train acc: 0.993, train_acc_sum: 1013.0, time: 0.2 sec\n","epoch: 14, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.045038, tarin loss sum: 1.441218, train acc: 0.981, train_acc_sum: 1001.0, time: 0.2 sec\n","epoch: 15, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.063780, tarin loss sum: 2.040957, train acc: 0.983, train_acc_sum: 1003.0, time: 0.2 sec\n","epoch: 16, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.052437, tarin loss sum: 1.677993, train acc: 0.983, train_acc_sum: 1003.0, time: 0.2 sec\n","epoch: 17, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.080598, tarin loss sum: 2.579140, train acc: 0.982, train_acc_sum: 1002.0, time: 0.2 sec\n","epoch: 18, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.045602, tarin loss sum: 1.459258, train acc: 0.985, train_acc_sum: 1005.0, time: 0.2 sec\n","epoch: 19, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.057166, tarin loss sum: 1.829317, train acc: 0.983, train_acc_sum: 1003.0, time: 0.2 sec\n","epoch: 20, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.020310, tarin loss sum: 0.649911, train acc: 0.997, train_acc_sum: 1017.0, time: 0.2 sec\n","epoch: 21, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.006622, tarin loss sum: 0.211894, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 22, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.011366, tarin loss sum: 0.363701, train acc: 0.999, train_acc_sum: 1019.0, time: 0.2 sec\n","epoch: 23, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.004083, tarin loss sum: 0.130671, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 24, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.014056, tarin loss sum: 0.449795, train acc: 0.995, train_acc_sum: 1015.0, time: 0.2 sec\n","save model...\n","epoch: 25, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.006650, tarin loss sum: 0.212809, train acc: 0.998, train_acc_sum: 1018.0, time: 0.2 sec\n","save model...\n","epoch: 26, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.015237, tarin loss sum: 0.487575, train acc: 0.997, train_acc_sum: 1017.0, time: 0.2 sec\n","epoch: 27, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.010385, tarin loss sum: 0.332327, train acc: 0.997, train_acc_sum: 1017.0, time: 0.2 sec\n","epoch: 28, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.002880, tarin loss sum: 0.092154, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 29, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.001833, tarin loss sum: 0.058649, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 30, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.001352, tarin loss sum: 0.043264, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 31, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000915, tarin loss sum: 0.029280, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 32, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000812, tarin loss sum: 0.025999, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 33, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000731, tarin loss sum: 0.023402, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 34, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000671, tarin loss sum: 0.021456, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 35, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000624, tarin loss sum: 0.019964, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 36, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000575, tarin loss sum: 0.018414, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 37, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000541, tarin loss sum: 0.017326, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 38, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000502, tarin loss sum: 0.016054, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 39, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000458, tarin loss sum: 0.014644, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 40, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000445, tarin loss sum: 0.014251, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 41, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000413, tarin loss sum: 0.013204, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 42, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000394, tarin loss sum: 0.012595, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 43, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000360, tarin loss sum: 0.011521, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 44, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000369, tarin loss sum: 0.011803, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 45, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000355, tarin loss sum: 0.011346, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 46, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000311, tarin loss sum: 0.009965, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 47, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000292, tarin loss sum: 0.009343, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 48, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000279, tarin loss sum: 0.008943, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 49, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000271, tarin loss sum: 0.008673, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 50, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000261, tarin loss sum: 0.008351, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 51, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000247, tarin loss sum: 0.007895, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 52, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000227, tarin loss sum: 0.007274, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 53, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000220, tarin loss sum: 0.007032, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 54, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000216, tarin loss sum: 0.006911, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 55, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000204, tarin loss sum: 0.006525, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 56, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000195, tarin loss sum: 0.006245, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 57, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000187, tarin loss sum: 0.005985, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 58, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000182, tarin loss sum: 0.005819, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 59, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000171, tarin loss sum: 0.005468, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 60, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000169, tarin loss sum: 0.005423, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 61, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000161, tarin loss sum: 0.005151, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 62, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000154, tarin loss sum: 0.004925, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 63, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000146, tarin loss sum: 0.004669, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 64, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000146, tarin loss sum: 0.004673, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 65, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000143, tarin loss sum: 0.004589, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 66, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000132, tarin loss sum: 0.004217, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 67, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000129, tarin loss sum: 0.004141, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 68, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000123, tarin loss sum: 0.003951, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 69, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000121, tarin loss sum: 0.003886, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 70, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000117, tarin loss sum: 0.003758, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 71, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000113, tarin loss sum: 0.003626, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 72, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000110, tarin loss sum: 0.003516, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 73, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000104, tarin loss sum: 0.003341, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 74, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000102, tarin loss sum: 0.003270, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 75, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000101, tarin loss sum: 0.003239, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 76, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000097, tarin loss sum: 0.003103, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 77, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000092, tarin loss sum: 0.002948, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 78, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000091, tarin loss sum: 0.002912, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 79, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000087, tarin loss sum: 0.002797, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 80, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000083, tarin loss sum: 0.002665, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 81, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000081, tarin loss sum: 0.002601, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 82, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000077, tarin loss sum: 0.002461, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 83, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000075, tarin loss sum: 0.002404, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 84, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000073, tarin loss sum: 0.002349, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 85, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000073, tarin loss sum: 0.002343, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 86, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000071, tarin loss sum: 0.002271, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 87, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000069, tarin loss sum: 0.002195, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 88, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000066, tarin loss sum: 0.002109, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 89, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000064, tarin loss sum: 0.002046, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 90, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000062, tarin loss sum: 0.001980, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 91, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000060, tarin loss sum: 0.001925, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 92, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000061, tarin loss sum: 0.001946, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 93, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000059, tarin loss sum: 0.001889, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 94, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000055, tarin loss sum: 0.001754, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 95, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000054, tarin loss sum: 0.001738, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 96, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000052, tarin loss sum: 0.001658, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 97, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000051, tarin loss sum: 0.001646, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 98, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000049, tarin loss sum: 0.001574, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 99, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000048, tarin loss sum: 0.001540, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","epoch: 100, training_sampler_num: 1020, batch_count: 32.00, train loss: 0.000046, tarin loss sum: 0.001481, train acc: 1.000, train_acc_sum: 1020.0, time: 0.2 sec\n","Training stage finished:\n"," epoch 100, loss 0.0000, train acc 1.000, training time 20.91 s\n","\n","\n","====================Starting evaluation for testing set.========================\n","\n","confusion_matrix\n","[[  38    0    0    0    0    0    0    0    0    3    0    0    0    0\n","     0    0]\n"," [   1 1233   10    7    0    0    0    0    0    8   10    0    0    0\n","     0    0]\n"," [   0    0  716    0    0    0    0    0    0    0    0   11    0    0\n","     0    0]\n"," [   0    0    0  201    0    0    0    4    0    0    0    0    0    0\n","     0    0]\n"," [   2    0    0    0  427    0    7    0    0    0    0    0    0    0\n","     0    0]\n"," [   0    2    0    0    2  643    0    0    0    0    0    0    0    0\n","     2    0]\n"," [   0    0    0    0    0    0   17    0    0    0    0    0    0    0\n","     0    0]\n"," [   0    0    0    0    0    0    0  423    0    0    0    0    0    0\n","     0    0]\n"," [   0    0    0    0    0    0    0    0   15    0    0    0    0    0\n","     0    0]\n"," [   0   13    2    0    0    0    0    0    0  846    4    0    0    0\n","     0    0]\n"," [   0   17   11    0    0    1    0    0    0    8 2170    0    0    0\n","     0    0]\n"," [   0    7    0    4    0    0    0    0    0    0    0  515    0    0\n","     3    1]\n"," [   0    0    0    0    0    0    0    0    0    0    0    0  182    0\n","     0    0]\n"," [   0    0    0    0    0    2    0    0    0    0    0    0    0 1124\n","     9    0]\n"," [   0    0    0    0    2    4    0    0    1    1    2    0    1    3\n","   331    0]\n"," [   0    0    0    0    0    0    0    0    0    0    0    3    0    0\n","     0   82]]\n","classification_report\n","                              precision    recall  f1-score   support\n","\n","                     Alfalfa       0.93      0.93      0.93        41\n","                 Corn-notill       0.97      0.97      0.97      1272\n","                Corn-mintill       0.98      0.97      0.98       739\n","                        Corn       0.98      0.95      0.96       212\n","               Grass-pasture       0.98      0.99      0.99       431\n","                 Grass-trees       0.99      0.99      0.99       650\n","         Grass-pasture-mowed       1.00      0.71      0.83        24\n","               Hay-windrowed       1.00      0.99      1.00       427\n","                        Oats       1.00      0.94      0.97        16\n","              Soybean-notill       0.98      0.98      0.98       866\n","             Soybean-mintill       0.98      0.99      0.99      2186\n","               Soybean-clean       0.97      0.97      0.97       529\n","                       Wheat       1.00      0.99      1.00       183\n","                       Woods       0.99      1.00      0.99      1127\n","Buildings-Grass-Trees-Drives       0.96      0.96      0.96       345\n","          Stone-Steel-Towers       0.96      0.99      0.98        83\n","\n","                    accuracy                           0.98      9131\n","                   macro avg       0.98      0.96      0.97      9131\n","                weighted avg       0.98      0.98      0.98      9131\n","\n","\n","====================Mean result of 5 times runs =========================\n","List of OA: [0.9830248603657868, 0.9783156280801665, 0.9752491512430183, 0.9835724455152777, 0.981601138977111]\n","List of AA: [0.9781090228539631, 0.9571741402047993, 0.9712833899518317, 0.982292969125151, 0.9800833068189632]\n","List of KPP: [0.9806428337248542, 0.9752869018439173, 0.9717826133162158, 0.981272956062074, 0.979013523360556]\n","OA= 98.04 +- 0.31\n","AA= 97.38 +- 0.91\n","Kpp= 97.76 +- 0.36\n","Acc per class= [91.79 97.31 99.13 95.85 97.72 99.6  95.   99.81 98.82 97.97 98.57 97.12\n"," 99.89 97.83 94.52 97.13] +- [ 4.54  0.65  0.5   3.54  1.1   0.29 10.    0.23  2.35  0.67  0.41  0.85\n","  0.22  0.7   1.28  1.38]\n","Average training time= 22.58 +- 3.068\n","Average testing time= 4.73 +- 5.308\n"]}]}]}