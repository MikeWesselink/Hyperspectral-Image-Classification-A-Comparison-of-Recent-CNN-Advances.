{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO6GQQBZympdjzx5mKjynzF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["## -*- coding: utf-8 -*-\n","# @Auther   : Mingsong Li (lms-07)\n","# @Time     : 2022-Nov\n","# @Address  : Time Lab @ SDU\n","# @FileName : process_dl.py\n","# @Project  : CVSSN (HSIC), IEEE TCSVT\n","\n","# for IP, KSC, and UP data sets, main processing file for the involved deep learning models,\n","# i.e., ,\n","# ContextualNet, RSSAN, SSTN, SSAtt, SSAN, SSSAN, A2S2KResNet, and the proposed CVSSN\n","\n"],"metadata":{"id":"BrKQpn_RwoGW"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yq6wLy5lwgOD","executionInfo":{"status":"ok","timestamp":1680560620025,"user_tz":300,"elapsed":30729,"user":{"displayName":"Michael Wesselink","userId":"03807820748818897534"}},"outputId":"20835bed-0ed6-48e5-f11a-837bcd7ee995"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["#Original code attributed above from https://github.com/lms-07/CVSSN\n","\n","#Edits by Mike Wesselink 3.27.23\n","\n","###############################\n","#Mike Wesselink CIS 631\n","\n","#Mount my google drive (Plan B)\n","from google.colab import drive\n","drive.mount('/content/drive/',force_remount=True)"]},{"cell_type":"code","source":["#imports\n","\n","import os\n","import time\n","import torch\n","import random\n","import numpy as np\n","from sklearn import metrics\n","\n","!pip install thop\n","from thop import profile\n","\n","#import spectral\n","!pip install spectral\n","from spectral import *\n","\n","import sys\n","\n","py_file_location = \"/content/drive/MyDrive/Colab Notebooks/CIS 631 Final Project/CVSSN-main\"\n","sys.path.append(os.path.abspath(py_file_location))\n","\n","import utils.evaluation as evaluation\n","import utils.data_load_operate as data_load_operate\n","import visual.cls_map_visual as cls_visual\n","\n","import model.CVSSN as CVSSN\n","\n","#import model.ContextualNet as ContextualNet\n","#import model.RSSAN as RSSAN\n","#import model.SSTN as SSTN\n","#import model.SSAtt as SSAtt\n","#import model.A2S2KResNet as A2S2KResNet\n","#import model.SSAN as SSAN\n","#import model.SSSAN as SSSAN\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3qGmG6xcwg1h","executionInfo":{"status":"ok","timestamp":1680560643567,"user_tz":300,"elapsed":23558,"user":{"displayName":"Michael Wesselink","userId":"03807820748818897534"}},"outputId":"b89bbeac-6760-4c85-b9c6-9a452a41b860"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting thop\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from thop) (2.0.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->thop) (3.10.7)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->thop) (1.11.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->thop) (2.0.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->thop) (3.1.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->thop) (3.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->thop) (4.5.0)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->thop) (16.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->thop) (3.25.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->thop) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->thop) (1.3.0)\n","Installing collected packages: thop\n","Successfully installed thop-0.1.1.post2209072238\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting spectral\n","  Downloading spectral-0.23.1-py3-none-any.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from spectral) (1.22.4)\n","Installing collected packages: spectral\n","Successfully installed spectral-0.23.1\n"]}]},{"cell_type":"code","source":["#SET UP MODEL INFO\n","\n","time_current = time.strftime(\"%y-%m-%d-%H.%M\", time.localtime())\n","\n","# random seed setting\n","seed = 20\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)  # Numpy module.\n","random.seed(seed)  # Python random module.\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","###                 0             1       2       3        4        5         6             7\n","model_list = ['ContextualNet', 'RSSAN', 'SSTN', 'SSAN', 'SSSAN', 'SSAtt', 'A2S2KResNet', 'CVSSN']\n","\n","model_flag = 7      #use only the CVSSN model\n","model_spa_set = {1, 2, 3, 5}\n","model_spe_set = {}\n","model_spa_spe_set = {4, 7}\n","model_3D_spa_set = {0, 6}\n","\n","model_3D_spa_flag = 0\n","\n","if model_flag in model_spa_set:\n","    model_type_flag = 1\n","    if model_flag in model_3D_spa_set:\n","        model_3D_spa_flag = 1\n","elif model_flag in model_spe_set:\n","    model_type_flag = 2\n","elif model_flag in model_spa_spe_set:\n","    model_type_flag = 3\n","\n","# 0-3\n","#data_set_name_list = ['IP', 'KSC', 'UP', 'HU_tif']\n","data_set_name_list = ['IP', 'KSC', 'UP', 'Salinas']\n","data_set_name = data_set_name_list[3]  #>>>test all 4 data sets by selecting position in data_set_name_list above?<<<\n","\n","# seed_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  \n","seed_list=[0,1,2,3,4]\n","# seed_list=[0,1,2] \n","# seed_list=[0,1]\n","#seed_list = [0]  \n","\n","# ratio=0.5\n","# ratio=1.0\n","# ratio=2.5\n","# ratio=5.0\n","# ratio=7.5\n","ratio = 10.0\n","patch_size = 9\n","patch_length = 4\n","\n","\n","###SET UP DATA PATH###\n","\n","#data_set_path = os.path.join(os.getcwd(), 'data')\n","data_set_path = '/content/drive/MyDrive/Colab Notebooks/CIS 631 Final Project/CVSSN-main/data'\n","\n","#results_save_path = \\\n","#    os.path.join(os.getcwd(), 'output/results', model_list[model_flag] + str(\"_\") +\n","#                 data_set_name + str(\"_\") + str(time_current) + str(\"_seed\") + str(seed) + str(\"_ratio\") + str(\n","#        ratio) + str(\"_patch_size\") + str(patch_size))\n","\n","results_save_path = \\\n","    os.path.join('/content/drive/MyDrive/Colab Notebooks/CIS 631 Final Project/CVSSN-main/output/results', model_list[model_flag] + str(\"_\") +\n","                data_set_name + str(\"_\") + str(time_current) + str(\"_seed\") + str(seed) + str(\"_ratio\") + str(\n","        ratio) + str(\"_patch_size\") + str(patch_size))\n","\n","\n","#cls_map_save_path = \\\n","#    os.path.join(os.path.join(os.getcwd(), 'output/cls_maps'), model_list[model_flag] + str(\"_\") +\n","#                 data_set_name + str(\"_\") + str(time_current) + str(\"_seed\") + str(seed)) + str(\"_ratio\") + str(ratio)\n","\n","cls_map_save_path = \\\n","    os.path.join(os.path.join('/content/drive/MyDrive/Colab Notebooks/CIS 631 Final Project/CVSSN-main/output/cls_maps'), model_list[model_flag] + str(\"_\") +\n","                 data_set_name + str(\"_\") + str(time_current) + str(\"_seed\") + str(seed)) + str(\"_ratio\") + str(ratio)\n","\n","if __name__ == '__main__':\n","\n","    torch.cuda.empty_cache()\n","\n","    data, gt = data_load_operate.load_data(data_set_name, data_set_path)\n","\n","    height, width, channels = data.shape\n","\n","    data = data_load_operate.standardization(data)\n","\n","    gt_reshape = gt.reshape(-1)\n","    height, width, channels = data.shape\n","    class_count = max(np.unique(gt))\n","\n","    flag_list = [0, 1]  # ratio or num\n","    ratio_list = [0.1, 0.01]  # [train_ratio,val_ratio]\n","    # ratio_list=[0.075,0.0075] # [train_ratio,val_ratio]\n","    # ratio_list=[0.05,0.005] # [train_ratio,val_ratio]\n","    # ratio_list=[0.0255,0.0025] # [train_ratio,val_ratio]\n","    # ratio_list=[0.01,0.001] # [train_ratio,val_ratio]\n","    # ratio_list=[0.005,0.0005] # [train_ratio,val_ratio]\n","    num_list = [45, 4]  # [train_num,val_num]\n","\n","    batch_size = 32\n","    max_epoch = 100\n","    learning_rate = 0.001\n","    loss = torch.nn.CrossEntropyLoss()\n","\n","    # data pad zero\n","    # data:[h,w,c]->data_padded:[h+2l,w+2l,c]\n","    data_padded = data_load_operate.data_pad_zero(data, patch_length)\n","    height_patched, width_patched, channels = data_padded.shape\n","\n","    OA_ALL = []\n","    AA_ALL = []\n","    KPP_ALL = []\n","    EACH_ACC_ALL = []\n","    Train_Time_ALL = []\n","    Test_Time_ALL = []\n","    CLASS_ACC = np.zeros([len(seed_list), class_count])\n","\n","    # data_total_index = np.arange(data.shape[0] * data.shape[1])  # For total sample cls_map.\n","\n","    for curr_seed in seed_list:\n","\n","        train_data_index, val_data_index, test_data_index, all_data_index = data_load_operate.sampling(ratio_list,\n","                                                                                                       num_list,\n","                                                                                                       gt_reshape,\n","                                                                                                       class_count,\n","                                                                                                       flag_list[0])\n","        index = (train_data_index, val_data_index, test_data_index)\n","        train_iter, test_iter, val_iter = data_load_operate.generate_iter_1 \\\n","            (data_padded, height, width, gt_reshape, index, patch_length, batch_size, model_type_flag,\n","             model_3D_spa_flag)\n","\n","\n","\n","        # load data for the cls map of all the labed samples\n","        # all_iter = data_load_operate.generate_iter_2(data_padded, height, width, gt_reshape, all_data_index,\n","        #                                              patch_length,\n","        #                                              batch_size, model_type_flag, model_3D_spa_flag)\n","        # load data for the cls map of the total samples\n","        # total_iter = data_load_operate.generate_iter_2(data_padded,height, width, gt_reshape, data_total_index, patch_length,\n","        #              25, model_type_flag, model_3D_spa_flag)\n","\n","        if model_flag == 0:\n","            net = ContextualNet.LeeEtAl(channels, class_count)\n","        elif model_flag == 1:\n","            net = RSSAN.RSSAN_net(in_shape=(channels, height_patched, width_patched), num_classes=class_count)\n","        elif model_flag == 2:\n","            net = SSTN.SSTN_AEAE(in_shape=(channels, height_patched, width_patched), num_classes=class_count)\n","        elif model_flag == 3:\n","            net = SSAN.SSAN(channels, patch_size, class_count)\n","        elif model_flag == 4:\n","            net = SSSAN.SSSAN(channels, class_count)\n","        elif model_flag == 5:\n","            net = SSAtt.Hang2020(channels, class_count)\n","        elif model_flag == 6:\n","            net = A2S2KResNet.S3KAIResNet(channels, class_count, 2)\n","        elif model_flag == 7:\n","            net = CVSSN.CVSSN_(channels, patch_size, patch_size, class_count)\n","\n","        # efficiency test, model complexity and computational cost\n","        # test_spe_input=torch.randn(1,channels) # for 1D model\n","        # test_input=torch.randn(1,patch_size,patch_size,channels) # for 2D model\n","        # test_input=torch.randn(1,1,patch_size,patch_size,channels) # for 3D model\n","        #\n","        # flops,para=profile(net,(test_input,test_spe_input))\n","        # flops,para=profile(net,(test_spe_input))\n","        # flops,para=profile(net,(test_input))\n","        #\n","        # print(\"para:{}\\n,flops:{}\".format(para,flops))\n","        # print(\"para(M):{:.3f},\\n flops(M):{:.2f}\".format(para/(1000**2),flops/(1000**2),))\n","\n","        net.to(device)\n","\n","        train_loss_list = [100]\n","        train_acc_list = [0]\n","        val_loss_list = [100]\n","        val_acc_list = [0]\n","        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n","        best_loss = 99999\n","\n","        tic1 = time.perf_counter()\n","\n","        for epoch in range(max_epoch):\n","            train_acc_sum, trained_samples_counter = 0.0, 0\n","            batch_counter, train_loss_sum = 0, 0\n","            time_epoch = time.time()\n","\n","            if model_type_flag == 1:  # data for single spatial net\n","                for X_spa, y in train_iter:\n","                    X_spa, y = X_spa.to(device), y.to(device)\n","                    y_pred = net(X_spa)\n","\n","                    ls = loss(y_pred, y.long())\n","\n","                    optimizer.zero_grad()\n","                    ls.backward()\n","                    optimizer.step()\n","\n","                    train_loss_sum += ls.cpu().item()\n","                    train_acc_sum += (y_pred.argmax(dim=1) == y).sum().cpu().item()\n","                    trained_samples_counter += y.shape[0]\n","                    batch_counter += 1\n","                    epoch_first_iter = 0\n","            elif model_type_flag == 2:  # data for single spectral net\n","                for X_spe, y in train_iter:\n","                    X_spe, y = X_spe.to(device), y.to(device)\n","                    y_pred = net(X_spe)\n","\n","                    ls = loss(y_pred, y.long())\n","\n","                    optimizer.zero_grad()\n","                    ls.backward()\n","                    optimizer.step()\n","\n","                    train_loss_sum += ls.cpu().item()\n","                    train_acc_sum += (y_pred.argmax(dim=1) == y).sum().cpu().item()\n","                    trained_samples_counter += y.shape[0]\n","                    batch_counter += 1\n","                    epoch_first_iter = 0\n","            elif model_type_flag == 3:  # data for spectral-spatial net\n","                for X_spa, X_spe, y in train_iter:\n","                    X_spa, X_spe, y = X_spa.to(device), X_spe.to(device), y.to(device)\n","                    y_pred = net(X_spa, X_spe)\n","                    if model_flag == 10:\n","                        for i in range(len(y_pred)):\n","                            if i == 0:\n","                                ls = loss(y_pred[i], y.long())\n","                            if i > 0:\n","                                ls += loss(y_pred[i], y.long())\n","                    else:\n","\n","                        ls = loss(y_pred, y.long())\n","\n","                    optimizer.zero_grad()\n","                    ls.backward()\n","                    optimizer.step()\n","\n","                    train_loss_sum += ls.cpu().item()\n","                    train_acc_sum += (y_pred.argmax(dim=1) == y).sum().cpu().item()\n","                    trained_samples_counter += y.shape[0]\n","                    batch_counter += 1\n","                    epoch_first_iter = 0\n","\n","            val_acc, val_loss = evaluation.evaluate_OA(val_iter, net, loss, device, model_type_flag)\n","            val_loss_list.append(val_loss)\n","            val_acc_list.append(val_acc)\n","\n","            if val_loss < best_loss:\n","                best_loss = val_loss\n","                torch.save(net.state_dict(), results_save_path + \"_best_model.pt\")\n","                print('save model...')\n","\n","            torch.cuda.empty_cache()\n","\n","            train_loss_list.append(train_loss_sum)\n","            train_acc_list.append(train_acc_sum / trained_samples_counter)\n","\n","            print('epoch: %d, training_sampler_num: %d, batch_count: %.2f, train loss: %.6f, tarin loss sum: %.6f, '\n","                  'train acc: %.3f, train_acc_sum: %.1f, time: %.1f sec' %\n","                  (epoch + 1, trained_samples_counter, batch_counter, train_loss_sum / batch_counter, train_loss_sum,\n","                   train_acc_sum / trained_samples_counter, train_acc_sum, time.time() - time_epoch))\n","\n","        toc1 = time.perf_counter()\n","        print('Training stage finished:\\n epoch %d, loss %.4f, train acc %.3f, training time %.2f s'\n","              % (epoch + 1, train_loss_sum / batch_counter, train_acc_sum / trained_samples_counter, toc1 - tic1))\n","        training_time = toc1 - tic1\n","        Train_Time_ALL.append(training_time)\n","\n","        print(\"\\n\\n====================Starting evaluation for testing set.========================\\n\")\n","\n","        pred_test = []\n","        # torch.cuda.empty_cache()\n","        with torch.no_grad():\n","            # net.load_state_dict(torch.load(model_save_path+\"_best_model.pt\"))\n","            net.eval()\n","            train_acc_sum, samples_num_counter = 0.0, 0\n","            if model_type_flag == 1:  # data for single spatial net\n","                for X_spa, y in test_iter:\n","                    X_spa = X_spa.to(device)\n","                    y = y.to(device)\n","\n","                    tic2 = time.perf_counter()\n","                    y_pred = net(X_spa)\n","                    toc2 = time.perf_counter()\n","\n","                    pred_test.extend(np.array(y_pred.cpu().argmax(axis=1)))\n","            elif model_type_flag == 2:  # data for single spectral net\n","                for X_spe, y in test_iter:\n","                    X_spe = X_spe.to(device)\n","                    y = y.to(device)\n","\n","                    tic2 = time.perf_counter()\n","                    y_pred = net(X_spe)\n","                    toc2 = time.perf_counter()\n","\n","                    pred_test.extend(np.array(y_pred.cpu().argmax(axis=1)))\n","            elif model_type_flag == 3:  # data for spectral-spatial net\n","                for X_spa, X_spe, y in test_iter:\n","                    X_spa = X_spa.to(device)\n","                    X_spe = X_spe.to(device)\n","                    y = y.to(device)\n","\n","                    tic2 = time.perf_counter()\n","                    y_pred = net(X_spa, X_spe)\n","                    toc2 = time.perf_counter()\n","\n","                    pred_test.extend(np.array(y_pred.cpu().argmax(axis=1)))\n","\n","            y_gt = gt_reshape[test_data_index] - 1\n","            OA = metrics.accuracy_score(y_gt, pred_test)\n","            confusion_matrix = metrics.confusion_matrix(pred_test, y_gt)\n","            print(\"confusion_matrix\\n{}\".format(confusion_matrix))\n","            ECA, AA = evaluation.AA_ECA(confusion_matrix)\n","            kappa = metrics.cohen_kappa_score(pred_test, y_gt)\n","            cls_report = evaluation.claification_report(y_gt, pred_test, data_set_name)\n","            print(\"classification_report\\n{}\".format(cls_report))\n","\n","            # Visualization for all the labeled samples and total the samples\n","            # sample_list1 = [all_iter, all_data_index]\n","            # sample_list2 = [total_iter]\n","\n","            # cls_visual.pred_cls_map_dl(sample_list1,net,gt,cls_map_save_path,model_type_flag)\n","            # cls_visual.pred_cls_map_dl(sample_list2, net, gt, cls_map_save_path,model_type_flag)\n","\n","            testing_time = toc2 - tic2\n","            Test_Time_ALL.append(testing_time)\n","\n","\n","            # Output infors\n","            f = open(results_save_path + '_results.txt', 'a+')\n","            str_results = '\\n======================' \\\n","                          + \" learning rate=\" + str(learning_rate) \\\n","                          + \" epochs=\" + str(max_epoch) \\\n","                          + \" train ratio=\" + str(ratio_list[0]) \\\n","                          + \" val ratio=\" + str(ratio_list[1]) \\\n","                          + \" ======================\" \\\n","                          + \"\\nOA=\" + str(OA) \\\n","                          + \"\\nAA=\" + str(AA) \\\n","                          + '\\nkpp=' + str(kappa) \\\n","                          + '\\nacc per class:' + str(ECA) \\\n","                          + \"\\ntrain time:\" + str(training_time) \\\n","                          + \"\\ntest time:\" + str(testing_time) + \"\\n\"\n","\n","            f.write(str_results)\n","            f.write('{}'.format(confusion_matrix))\n","            f.write('\\n\\n')\n","            f.write('{}'.format(cls_report))\n","            f.close()\n","\n","            OA_ALL.append(OA)\n","            AA_ALL.append(AA)\n","            KPP_ALL.append(kappa)\n","            EACH_ACC_ALL.append(ECA)\n","\n","        torch.cuda.empty_cache()\n","        del net, train_iter, test_iter, val_iter\n","        # del net, train_iter, test_iter, val_iter, all_iter\n","        # del net\n","\n","    OA_ALL = np.array(OA_ALL)\n","    AA_ALL = np.array(AA_ALL)\n","    KPP_ALL = np.array(KPP_ALL)\n","    EACH_ACC_ALL = np.array(EACH_ACC_ALL)\n","    Train_Time_ALL = np.array(Train_Time_ALL)\n","    Test_Time_ALL = np.array(Test_Time_ALL)\n","\n","    np.set_printoptions(precision=4)\n","    print(\"\\n====================Mean result of {} times runs =========================\".format(len(seed_list)))\n","    print('List of OA:', list(OA_ALL))\n","    print('List of AA:', list(AA_ALL))\n","    print('List of KPP:', list(KPP_ALL))\n","    print('OA=', round(np.mean(OA_ALL) * 100, 2), '+-', round(np.std(OA_ALL) * 100, 2))\n","    print('AA=', round(np.mean(AA_ALL) * 100, 2), '+-', round(np.std(AA_ALL) * 100, 2))\n","    print('Kpp=', round(np.mean(KPP_ALL) * 100, 2), '+-', round(np.std(KPP_ALL) * 100, 2))\n","    print('Acc per class=', np.round(np.mean(EACH_ACC_ALL, 0) * 100, decimals=2), '+-',\n","          np.round(np.std(EACH_ACC_ALL, 0) * 100, decimals=2))\n","\n","    print(\"Average training time=\", round(np.mean(Train_Time_ALL), 2), '+-', round(np.std(Train_Time_ALL), 3))\n","    print(\"Average testing time=\", round(np.mean(Test_Time_ALL) * 1000, 2), '+-',\n","          round(np.std(Test_Time_ALL) * 1000, 3))\n","\n","    # Output infors\n","    f = open(results_save_path + '_results.txt', 'a+')\n","    str_results = '\\n\\n***************Mean result of ' + str(len(seed_list)) + 'times runs ********************' \\\n","                  + '\\nList of OA:' + str(list(OA_ALL)) \\\n","                  + '\\nList of AA:' + str(list(AA_ALL)) \\\n","                  + '\\nList of KPP:' + str(list(KPP_ALL)) \\\n","                  + '\\nOA=' + str(round(np.mean(OA_ALL) * 100, 2)) + '+-' + str(round(np.std(OA_ALL) * 100, 2)) \\\n","                  + '\\nAA=' + str(round(np.mean(AA_ALL) * 100, 2)) + '+-' + str(round(np.std(AA_ALL) * 100, 2)) \\\n","                  + '\\nKpp=' + str(round(np.mean(KPP_ALL) * 100, 2)) + '+-' + str(round(np.std(KPP_ALL) * 100, 2)) \\\n","                  + '\\nAcc per class=\\n' + str(np.round(np.mean(EACH_ACC_ALL, 0) * 100, 2)) + '+-' + str(\n","        np.round(np.std(EACH_ACC_ALL, 0) * 100, 2)) \\\n","                  + \"\\nAverage training time=\" + str(np.round(np.mean(Train_Time_ALL), decimals=2)) + '+-' + str(\n","        np.round(np.std(Train_Time_ALL), decimals=3)) \\\n","                  + \"\\nAverage testing time=\" + str(np.round(np.mean(Test_Time_ALL) * 1000, decimals=2)) + '+-' + str(\n","        np.round(np.std(Test_Time_ALL) * 100, decimals=3))\n","    f.write(str_results)\n","    f.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ATgpQ1R56bW9","executionInfo":{"status":"error","timestamp":1680560815693,"user_tz":300,"elapsed":124557,"user":{"displayName":"Michael Wesselink","userId":"03807820748818897534"}},"outputId":"55f99818-a716-4aa4-9f32-b446fc393f24"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["save model...\n","epoch: 1, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.561334, tarin loss sum: 94.865453, train acc: 0.820, train_acc_sum: 4431.0, time: 8.9 sec\n","save model...\n","epoch: 2, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.329409, tarin loss sum: 55.670116, train acc: 0.898, train_acc_sum: 4850.0, time: 1.0 sec\n","epoch: 3, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.101514, tarin loss sum: 17.155856, train acc: 0.963, train_acc_sum: 5204.0, time: 1.0 sec\n","save model...\n","epoch: 4, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.080329, tarin loss sum: 13.575666, train acc: 0.971, train_acc_sum: 5249.0, time: 1.0 sec\n","epoch: 5, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.056399, tarin loss sum: 9.531450, train acc: 0.977, train_acc_sum: 5281.0, time: 1.0 sec\n","save model...\n","epoch: 6, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.048167, tarin loss sum: 8.140222, train acc: 0.980, train_acc_sum: 5295.0, time: 1.0 sec\n","save model...\n","epoch: 7, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.033941, tarin loss sum: 5.736063, train acc: 0.989, train_acc_sum: 5342.0, time: 1.0 sec\n","save model...\n","epoch: 8, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.030047, tarin loss sum: 5.077864, train acc: 0.989, train_acc_sum: 5343.0, time: 1.0 sec\n","epoch: 9, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.230501, tarin loss sum: 38.954731, train acc: 0.948, train_acc_sum: 5121.0, time: 1.0 sec\n","epoch: 10, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.040070, tarin loss sum: 6.771911, train acc: 0.983, train_acc_sum: 5310.0, time: 1.0 sec\n","save model...\n","epoch: 11, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.018342, tarin loss sum: 3.099802, train acc: 0.995, train_acc_sum: 5378.0, time: 1.0 sec\n","epoch: 12, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.019048, tarin loss sum: 3.219057, train acc: 0.993, train_acc_sum: 5367.0, time: 1.0 sec\n","save model...\n","epoch: 13, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.038014, tarin loss sum: 6.424389, train acc: 0.986, train_acc_sum: 5327.0, time: 1.0 sec\n","epoch: 14, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.011085, tarin loss sum: 1.873319, train acc: 0.996, train_acc_sum: 5382.0, time: 1.0 sec\n","save model...\n","epoch: 15, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.005264, tarin loss sum: 0.889589, train acc: 0.999, train_acc_sum: 5396.0, time: 1.0 sec\n","epoch: 16, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000753, tarin loss sum: 0.127333, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 17, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000411, tarin loss sum: 0.069516, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 18, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000309, tarin loss sum: 0.052260, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 19, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000226, tarin loss sum: 0.038250, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","save model...\n","epoch: 20, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000147, tarin loss sum: 0.024906, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 21, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000135, tarin loss sum: 0.022841, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 22, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000110, tarin loss sum: 0.018601, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 23, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000094, tarin loss sum: 0.015865, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","save model...\n","epoch: 24, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000081, tarin loss sum: 0.013613, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 25, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000067, tarin loss sum: 0.011293, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 26, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000063, tarin loss sum: 0.010708, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 27, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000051, tarin loss sum: 0.008650, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","save model...\n","epoch: 28, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000045, tarin loss sum: 0.007677, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 29, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000043, tarin loss sum: 0.007314, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","save model...\n","epoch: 30, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000037, tarin loss sum: 0.006175, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 31, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000030, tarin loss sum: 0.005149, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 32, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000027, tarin loss sum: 0.004589, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 33, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000024, tarin loss sum: 0.004086, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 34, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000021, tarin loss sum: 0.003561, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 35, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000020, tarin loss sum: 0.003349, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 36, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000017, tarin loss sum: 0.002903, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 37, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000015, tarin loss sum: 0.002543, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","save model...\n","epoch: 38, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000013, tarin loss sum: 0.002238, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 39, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000012, tarin loss sum: 0.002074, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 40, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000011, tarin loss sum: 0.001874, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 41, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000009, tarin loss sum: 0.001587, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","save model...\n","epoch: 42, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000009, tarin loss sum: 0.001465, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 43, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000007, tarin loss sum: 0.001252, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 44, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000008, tarin loss sum: 0.001297, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 45, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000006, tarin loss sum: 0.001023, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 46, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000006, tarin loss sum: 0.000935, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 47, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000005, tarin loss sum: 0.000834, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","save model...\n","epoch: 48, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000004, tarin loss sum: 0.000729, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 49, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000004, tarin loss sum: 0.000698, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 50, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000004, tarin loss sum: 0.000609, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 51, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000003, tarin loss sum: 0.000552, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 52, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000003, tarin loss sum: 0.000477, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 53, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000003, tarin loss sum: 0.000471, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 54, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000003, tarin loss sum: 0.000432, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 55, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000002, tarin loss sum: 0.000357, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 56, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000002, tarin loss sum: 0.000319, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 57, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000002, tarin loss sum: 0.000290, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 58, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000002, tarin loss sum: 0.000269, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 59, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000001, tarin loss sum: 0.000234, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 60, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000001, tarin loss sum: 0.000217, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","save model...\n","epoch: 61, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000001, tarin loss sum: 0.000197, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 62, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000001, tarin loss sum: 0.000164, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 63, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000001, tarin loss sum: 0.000173, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 64, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000001, tarin loss sum: 0.000142, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 65, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000001, tarin loss sum: 0.000118, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 66, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000001, tarin loss sum: 0.000111, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 67, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000001, tarin loss sum: 0.000099, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 68, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000001, tarin loss sum: 0.000087, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 69, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000000, tarin loss sum: 0.000082, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 70, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000000, tarin loss sum: 0.000069, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 71, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000000, tarin loss sum: 0.000063, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 72, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000000, tarin loss sum: 0.000059, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 73, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000000, tarin loss sum: 0.000052, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 74, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000000, tarin loss sum: 0.000048, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 75, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000000, tarin loss sum: 0.000044, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 76, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000000, tarin loss sum: 0.000037, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 77, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000000, tarin loss sum: 0.000035, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 78, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000000, tarin loss sum: 0.000033, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 79, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.275801, tarin loss sum: 46.610299, train acc: 0.967, train_acc_sum: 5225.0, time: 1.0 sec\n","epoch: 80, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.252753, tarin loss sum: 42.715205, train acc: 0.919, train_acc_sum: 4968.0, time: 1.0 sec\n","epoch: 81, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.077326, tarin loss sum: 13.068049, train acc: 0.970, train_acc_sum: 5240.0, time: 1.0 sec\n","epoch: 82, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.036266, tarin loss sum: 6.128886, train acc: 0.986, train_acc_sum: 5327.0, time: 1.0 sec\n","epoch: 83, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.018144, tarin loss sum: 3.066360, train acc: 0.994, train_acc_sum: 5370.0, time: 1.0 sec\n","epoch: 84, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.017571, tarin loss sum: 2.969501, train acc: 0.993, train_acc_sum: 5367.0, time: 1.0 sec\n","epoch: 85, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.020141, tarin loss sum: 3.403846, train acc: 0.993, train_acc_sum: 5365.0, time: 1.0 sec\n","epoch: 86, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.014501, tarin loss sum: 2.450649, train acc: 0.995, train_acc_sum: 5378.0, time: 1.0 sec\n","epoch: 87, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.020970, tarin loss sum: 3.543975, train acc: 0.994, train_acc_sum: 5370.0, time: 1.0 sec\n","epoch: 88, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.001784, tarin loss sum: 0.301441, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 89, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000553, tarin loss sum: 0.093518, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 90, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000279, tarin loss sum: 0.047212, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 91, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000199, tarin loss sum: 0.033686, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 92, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000145, tarin loss sum: 0.024462, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 93, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000110, tarin loss sum: 0.018511, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 94, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000089, tarin loss sum: 0.015002, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 95, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000076, tarin loss sum: 0.012819, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 96, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000063, tarin loss sum: 0.010673, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 97, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000054, tarin loss sum: 0.009136, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 98, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000051, tarin loss sum: 0.008539, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 99, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000041, tarin loss sum: 0.006890, train acc: 1.000, train_acc_sum: 5403.0, time: 1.0 sec\n","epoch: 100, training_sampler_num: 5403, batch_count: 169.00, train loss: 0.000038, tarin loss sum: 0.006388, train acc: 1.000, train_acc_sum: 5403.0, time: 1.1 sec\n","Training stage finished:\n"," epoch 100, loss 0.0000, train acc 1.000, training time 109.03 s\n","\n","\n","====================Starting evaluation for testing set.========================\n","\n","confusion_matrix\n","[[1789    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0]\n"," [   0 3317    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0]\n"," [   0    0 1756    0    0    0    0    0    0    3    0    0    0    0\n","     0    0]\n"," [   0    0    0 1237    0    0    0    0    0    0    0    0    0    0\n","     0    0]\n"," [   0    0    4    5 2385    0    0    0    3    0    0    0    0    0\n","     0    0]\n"," [   0    0    0    0    0 3525    0    0    0    0    0    0    0    0\n","     0    0]\n"," [   0    0    0    0    0    0 3185    0    0    0    0    0    0    0\n","     0    0]\n"," [   0    0    0    0    0    0    0 9941    0    0    1    0    0    0\n","    80   17]\n"," [   0    0    0    0    0    0    0    0 5518    0    0    0    0    0\n","     0    0]\n"," [   0    0    0    0    0    0    0    1    0 2916    0    0    0    0\n","     0    0]\n"," [   0    0    0    0    0    0    0    0    0    0  951    0    0    0\n","     0    0]\n"," [   0    0    0    0    0    0    0    0    0    0    0 1716    0    0\n","     0    0]\n"," [   0    0    0    0    0    0    0    0    0    0    0    0  816    0\n","     0    0]\n"," [   0    0    0    0    0    0    0    0    0    0    0    0    0  953\n","     0    0]\n"," [   0    0    0    0    0    0    0   90    0    0    0    0    0    0\n","  6390    0]\n"," [   0    0    0    0    0    0    2    0    0    0    0    0    0    0\n","     0 1592]]\n"]},{"output_type":"error","ename":"UnboundLocalError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-36c275570920>\u001b[0m in \u001b[0;36m<cell line: 84>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mECA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAA_ECA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mkappa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcohen_kappa_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mcls_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclaification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_set_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"classification_report\\n{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_report\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/CIS 631 Final Project/CVSSN-main/utils/evaluation.py\u001b[0m in \u001b[0;36mclaification_report\u001b[0;34m(label, pred, name)\u001b[0m\n\u001b[1;32m     88\u001b[0m                         'Running_track']\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mclassification_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'target_names' referenced before assignment"]}]}]}