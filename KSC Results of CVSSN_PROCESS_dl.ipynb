{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1BgyV0cypQ1YlT0LAzUBsTpC1qduwj8yg","timestamp":1680362730115}],"authorship_tag":"ABX9TyNa52LOhgh30wxkmi9aisLX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["## -*- coding: utf-8 -*-\n","# @Auther   : Mingsong Li (lms-07)\n","# @Time     : 2022-Nov\n","# @Address  : Time Lab @ SDU\n","# @FileName : process_dl.py\n","# @Project  : CVSSN (HSIC), IEEE TCSVT\n","\n","# for IP, KSC, and UP data sets, main processing file for the involved deep learning models,\n","# i.e., ,\n","# ContextualNet, RSSAN, SSTN, SSAtt, SSAN, SSSAN, A2S2KResNet, and the proposed CVSSN\n","\n"],"metadata":{"id":"BrKQpn_RwoGW"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yq6wLy5lwgOD","executionInfo":{"status":"ok","timestamp":1680358557448,"user_tz":300,"elapsed":16774,"user":{"displayName":"Michael Wesselink","userId":"03807820748818897534"}},"outputId":"ecb6cae2-ceda-491b-f1b7-5d58b7081e9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["#Original code attributed above from https://github.com/lms-07/CVSSN\n","\n","#Edits by Mike Wesselink 3.27.23\n","\n","###############################\n","#Mike Wesselink CIS 631\n","\n","#Mount my google drive (Plan B)\n","from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["#imports\n","\n","import os\n","import time\n","import torch\n","import random\n","import numpy as np\n","from sklearn import metrics\n","\n","!pip install thop\n","from thop import profile\n","\n","#import spectral\n","!pip install spectral\n","from spectral import *\n","\n","import sys\n","\n","py_file_location = \"/content/drive/MyDrive/Colab Notebooks/CIS 631 Final Project/CVSSN-main\"\n","sys.path.append(os.path.abspath(py_file_location))\n","\n","import utils.evaluation as evaluation\n","import utils.data_load_operate as data_load_operate\n","import visual.cls_map_visual as cls_visual\n","\n","import model.ContextualNet as ContextualNet\n","import model.RSSAN as RSSAN\n","import model.SSTN as SSTN\n","import model.SSAtt as SSAtt\n","import model.A2S2KResNet as A2S2KResNet\n","import model.CVSSN as CVSSN\n","import model.SSAN as SSAN\n","import model.SSSAN as SSSAN\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3qGmG6xcwg1h","executionInfo":{"status":"ok","timestamp":1680358573846,"user_tz":300,"elapsed":14381,"user":{"displayName":"Michael Wesselink","userId":"03807820748818897534"}},"outputId":"3ce57cd2-3b6d-4981-82d0-ed1b6c880929"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting thop\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from thop) (1.13.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->thop) (4.5.0)\n","Installing collected packages: thop\n","Successfully installed thop-0.1.1.post2209072238\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting spectral\n","  Downloading spectral-0.23.1-py3-none-any.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from spectral) (1.22.4)\n","Installing collected packages: spectral\n","Successfully installed spectral-0.23.1\n"]}]},{"cell_type":"code","source":["#SET UP MODEL INFO\n","\n","time_current = time.strftime(\"%y-%m-%d-%H.%M\", time.localtime())\n","\n","# random seed setting\n","seed = 20\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)  # Numpy module.\n","random.seed(seed)  # Python random module.\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","###                 0             1       2       3        4        5         6             7\n","model_list = ['ContextualNet', 'RSSAN', 'SSTN', 'SSAN', 'SSSAN', 'SSAtt', 'A2S2KResNet', 'CVSSN']\n","\n","model_flag = 7\n","model_spa_set = {1, 2, 3, 5}\n","model_spe_set = {}\n","model_spa_spe_set = {4, 7}\n","model_3D_spa_set = {0, 6}\n","\n","model_3D_spa_flag = 0\n","\n","if model_flag in model_spa_set:\n","    model_type_flag = 1\n","    if model_flag in model_3D_spa_set:\n","        model_3D_spa_flag = 1\n","elif model_flag in model_spe_set:\n","    model_type_flag = 2\n","elif model_flag in model_spa_spe_set:\n","    model_type_flag = 3\n","\n","# 0-2\n","#data_set_name_list = ['IP', 'KSC', 'UP', 'HU_tif']\n","data_set_name_list = ['IP', 'KSC', 'UP', 'SALINAS']\n","data_set_name = data_set_name_list[1]  #test all 4 data sets by selecting position in data_set_name_list above?\n","\n","# seed_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  #\n","seed_list=[0,1,2,3,4]\n","# seed_list=[0,1,2] \n","# seed_list=[0,1]\n","#seed_list = [0]  \n","\n","# ratio=0.5\n","# ratio=1.0\n","# ratio=2.5\n","# ratio=5.0\n","# ratio=7.5\n","ratio = 10.0\n","patch_size = 9\n","patch_length = 4\n","\n","\n","###SET UP DATA PATH###\n","\n","#data_set_path = os.path.join(os.getcwd(), 'data')\n","data_set_path = '/content/drive/MyDrive/Colab Notebooks/CIS 631 Final Project/CVSSN-main/data'\n","\n","#results_save_path = \\\n","#    os.path.join(os.getcwd(), 'output/results', model_list[model_flag] + str(\"_\") +\n","#                 data_set_name + str(\"_\") + str(time_current) + str(\"_seed\") + str(seed) + str(\"_ratio\") + str(\n","#        ratio) + str(\"_patch_size\") + str(patch_size))\n","\n","results_save_path = \\\n","    os.path.join('/content/drive/MyDrive/Colab Notebooks/CIS 631 Final Project/CVSSN-main/output/results', model_list[model_flag] + str(\"_\") +\n","                data_set_name + str(\"_\") + str(time_current) + str(\"_seed\") + str(seed) + str(\"_ratio\") + str(\n","        ratio) + str(\"_patch_size\") + str(patch_size))\n","\n","\n","#cls_map_save_path = \\\n","#    os.path.join(os.path.join(os.getcwd(), 'output/cls_maps'), model_list[model_flag] + str(\"_\") +\n","#                 data_set_name + str(\"_\") + str(time_current) + str(\"_seed\") + str(seed)) + str(\"_ratio\") + str(ratio)\n","\n","cls_map_save_path = \\\n","    os.path.join(os.path.join('/content/drive/MyDrive/Colab Notebooks/CIS 631 Final Project/CVSSN-main/output/cls_maps'), model_list[model_flag] + str(\"_\") +\n","                 data_set_name + str(\"_\") + str(time_current) + str(\"_seed\") + str(seed)) + str(\"_ratio\") + str(ratio)\n","\n","if __name__ == '__main__':\n","\n","    torch.cuda.empty_cache()\n","\n","    data, gt = data_load_operate.load_data(data_set_name, data_set_path)\n","\n","    height, width, channels = data.shape\n","\n","    data = data_load_operate.standardization(data)\n","\n","    gt_reshape = gt.reshape(-1)\n","    height, width, channels = data.shape\n","    class_count = max(np.unique(gt))\n","\n","    flag_list = [0, 1]  # ratio or num\n","    ratio_list = [0.1, 0.01]  # [train_ratio,val_ratio]\n","    # ratio_list=[0.075,0.0075] # [train_ratio,val_ratio]\n","    # ratio_list=[0.05,0.005] # [train_ratio,val_ratio]\n","    # ratio_list=[0.0255,0.0025] # [train_ratio,val_ratio]\n","    # ratio_list=[0.01,0.001] # [train_ratio,val_ratio]\n","    # ratio_list=[0.005,0.0005] # [train_ratio,val_ratio]\n","    num_list = [45, 4]  # [train_num,val_num]\n","\n","    batch_size = 32\n","    max_epoch = 100\n","    learning_rate = 0.001\n","    loss = torch.nn.CrossEntropyLoss()\n","\n","    # data pad zero\n","    # data:[h,w,c]->data_padded:[h+2l,w+2l,c]\n","    data_padded = data_load_operate.data_pad_zero(data, patch_length)\n","    height_patched, width_patched, channels = data_padded.shape\n","\n","    OA_ALL = []\n","    AA_ALL = []\n","    KPP_ALL = []\n","    EACH_ACC_ALL = []\n","    Train_Time_ALL = []\n","    Test_Time_ALL = []\n","    CLASS_ACC = np.zeros([len(seed_list), class_count])\n","\n","    # data_total_index = np.arange(data.shape[0] * data.shape[1])  # For total sample cls_map.\n","\n","    for curr_seed in seed_list:\n","\n","        train_data_index, val_data_index, test_data_index, all_data_index = data_load_operate.sampling(ratio_list,\n","                                                                                                       num_list,\n","                                                                                                       gt_reshape,\n","                                                                                                       class_count,\n","                                                                                                       flag_list[0])\n","        index = (train_data_index, val_data_index, test_data_index)\n","        train_iter, test_iter, val_iter = data_load_operate.generate_iter_1 \\\n","            (data_padded, height, width, gt_reshape, index, patch_length, batch_size, model_type_flag,\n","             model_3D_spa_flag)\n","\n","\n","\n","        # load data for the cls map of all the labed samples\n","        # all_iter = data_load_operate.generate_iter_2(data_padded, height, width, gt_reshape, all_data_index,\n","        #                                              patch_length,\n","        #                                              batch_size, model_type_flag, model_3D_spa_flag)\n","        # load data for the cls map of the total samples\n","        # total_iter = data_load_operate.generate_iter_2(data_padded,height, width, gt_reshape, data_total_index, patch_length,\n","        #              25, model_type_flag, model_3D_spa_flag)\n","\n","        if model_flag == 0:\n","            net = ContextualNet.LeeEtAl(channels, class_count)\n","        elif model_flag == 1:\n","            net = RSSAN.RSSAN_net(in_shape=(channels, height_patched, width_patched), num_classes=class_count)\n","        elif model_flag == 2:\n","            net = SSTN.SSTN_AEAE(in_shape=(channels, height_patched, width_patched), num_classes=class_count)\n","        elif model_flag == 3:\n","            net = SSAN.SSAN(channels, patch_size, class_count)\n","        elif model_flag == 4:\n","            net = SSSAN.SSSAN(channels, class_count)\n","        elif model_flag == 5:\n","            net = SSAtt.Hang2020(channels, class_count)\n","        elif model_flag == 6:\n","            net = A2S2KResNet.S3KAIResNet(channels, class_count, 2)\n","        elif model_flag == 7:\n","            net = CVSSN.CVSSN_(channels, patch_size, patch_size, class_count)\n","\n","        # efficiency test, model complexity and computational cost\n","        # test_spe_input=torch.randn(1,channels) # for 1D model\n","        # test_input=torch.randn(1,patch_size,patch_size,channels) # for 2D model\n","        # test_input=torch.randn(1,1,patch_size,patch_size,channels) # for 3D model\n","        #\n","        # flops,para=profile(net,(test_input,test_spe_input))\n","        # flops,para=profile(net,(test_spe_input))\n","        # flops,para=profile(net,(test_input))\n","        #\n","        # print(\"para:{}\\n,flops:{}\".format(para,flops))\n","        # print(\"para(M):{:.3f},\\n flops(M):{:.2f}\".format(para/(1000**2),flops/(1000**2),))\n","\n","        net.to(device)\n","\n","        train_loss_list = [100]\n","        train_acc_list = [0]\n","        val_loss_list = [100]\n","        val_acc_list = [0]\n","        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n","        best_loss = 99999\n","\n","        tic1 = time.perf_counter()\n","\n","        for epoch in range(max_epoch):\n","            train_acc_sum, trained_samples_counter = 0.0, 0\n","            batch_counter, train_loss_sum = 0, 0\n","            time_epoch = time.time()\n","\n","            if model_type_flag == 1:  # data for single spatial net\n","                for X_spa, y in train_iter:\n","                    X_spa, y = X_spa.to(device), y.to(device)\n","                    y_pred = net(X_spa)\n","\n","                    ls = loss(y_pred, y.long())\n","\n","                    optimizer.zero_grad()\n","                    ls.backward()\n","                    optimizer.step()\n","\n","                    train_loss_sum += ls.cpu().item()\n","                    train_acc_sum += (y_pred.argmax(dim=1) == y).sum().cpu().item()\n","                    trained_samples_counter += y.shape[0]\n","                    batch_counter += 1\n","                    epoch_first_iter = 0\n","            elif model_type_flag == 2:  # data for single spectral net\n","                for X_spe, y in train_iter:\n","                    X_spe, y = X_spe.to(device), y.to(device)\n","                    y_pred = net(X_spe)\n","\n","                    ls = loss(y_pred, y.long())\n","\n","                    optimizer.zero_grad()\n","                    ls.backward()\n","                    optimizer.step()\n","\n","                    train_loss_sum += ls.cpu().item()\n","                    train_acc_sum += (y_pred.argmax(dim=1) == y).sum().cpu().item()\n","                    trained_samples_counter += y.shape[0]\n","                    batch_counter += 1\n","                    epoch_first_iter = 0\n","            elif model_type_flag == 3:  # data for spectral-spatial net\n","                for X_spa, X_spe, y in train_iter:\n","                    X_spa, X_spe, y = X_spa.to(device), X_spe.to(device), y.to(device)\n","                    y_pred = net(X_spa, X_spe)\n","                    if model_flag == 10:\n","                        for i in range(len(y_pred)):\n","                            if i == 0:\n","                                ls = loss(y_pred[i], y.long())\n","                            if i > 0:\n","                                ls += loss(y_pred[i], y.long())\n","                    else:\n","\n","                        ls = loss(y_pred, y.long())\n","\n","                    optimizer.zero_grad()\n","                    ls.backward()\n","                    optimizer.step()\n","\n","                    train_loss_sum += ls.cpu().item()\n","                    train_acc_sum += (y_pred.argmax(dim=1) == y).sum().cpu().item()\n","                    trained_samples_counter += y.shape[0]\n","                    batch_counter += 1\n","                    epoch_first_iter = 0\n","\n","            val_acc, val_loss = evaluation.evaluate_OA(val_iter, net, loss, device, model_type_flag)\n","            val_loss_list.append(val_loss)\n","            val_acc_list.append(val_acc)\n","\n","            if val_loss < best_loss:\n","                best_loss = val_loss\n","                torch.save(net.state_dict(), results_save_path + \"_best_model.pt\")\n","                print('save model...')\n","\n","            torch.cuda.empty_cache()\n","\n","            train_loss_list.append(train_loss_sum)\n","            train_acc_list.append(train_acc_sum / trained_samples_counter)\n","\n","            print('epoch: %d, training_sampler_num: %d, batch_count: %.2f, train loss: %.6f, tarin loss sum: %.6f, '\n","                  'train acc: %.3f, train_acc_sum: %.1f, time: %.1f sec' %\n","                  (epoch + 1, trained_samples_counter, batch_counter, train_loss_sum / batch_counter, train_loss_sum,\n","                   train_acc_sum / trained_samples_counter, train_acc_sum, time.time() - time_epoch))\n","\n","        toc1 = time.perf_counter()\n","        print('Training stage finished:\\n epoch %d, loss %.4f, train acc %.3f, training time %.2f s'\n","              % (epoch + 1, train_loss_sum / batch_counter, train_acc_sum / trained_samples_counter, toc1 - tic1))\n","        training_time = toc1 - tic1\n","        Train_Time_ALL.append(training_time)\n","\n","        print(\"\\n\\n====================Starting evaluation for testing set.========================\\n\")\n","\n","        pred_test = []\n","        # torch.cuda.empty_cache()\n","        with torch.no_grad():\n","            # net.load_state_dict(torch.load(model_save_path+\"_best_model.pt\"))\n","            net.eval()\n","            train_acc_sum, samples_num_counter = 0.0, 0\n","            if model_type_flag == 1:  # data for single spatial net\n","                for X_spa, y in test_iter:\n","                    X_spa = X_spa.to(device)\n","                    y = y.to(device)\n","\n","                    tic2 = time.perf_counter()\n","                    y_pred = net(X_spa)\n","                    toc2 = time.perf_counter()\n","\n","                    pred_test.extend(np.array(y_pred.cpu().argmax(axis=1)))\n","            elif model_type_flag == 2:  # data for single spectral net\n","                for X_spe, y in test_iter:\n","                    X_spe = X_spe.to(device)\n","                    y = y.to(device)\n","\n","                    tic2 = time.perf_counter()\n","                    y_pred = net(X_spe)\n","                    toc2 = time.perf_counter()\n","\n","                    pred_test.extend(np.array(y_pred.cpu().argmax(axis=1)))\n","            elif model_type_flag == 3:  # data for spectral-spatial net\n","                for X_spa, X_spe, y in test_iter:\n","                    X_spa = X_spa.to(device)\n","                    X_spe = X_spe.to(device)\n","                    y = y.to(device)\n","\n","                    tic2 = time.perf_counter()\n","                    y_pred = net(X_spa, X_spe)\n","                    toc2 = time.perf_counter()\n","\n","                    pred_test.extend(np.array(y_pred.cpu().argmax(axis=1)))\n","\n","            y_gt = gt_reshape[test_data_index] - 1\n","            OA = metrics.accuracy_score(y_gt, pred_test)\n","            confusion_matrix = metrics.confusion_matrix(pred_test, y_gt)\n","            print(\"confusion_matrix\\n{}\".format(confusion_matrix))\n","            ECA, AA = evaluation.AA_ECA(confusion_matrix)\n","            kappa = metrics.cohen_kappa_score(pred_test, y_gt)\n","            cls_report = evaluation.claification_report(y_gt, pred_test, data_set_name)\n","            print(\"classification_report\\n{}\".format(cls_report))\n","\n","            # Visualization for all the labeled samples and total the samples\n","            # sample_list1 = [all_iter, all_data_index]\n","            # sample_list2 = [total_iter]\n","\n","            # cls_visual.pred_cls_map_dl(sample_list1,net,gt,cls_map_save_path,model_type_flag)\n","            # cls_visual.pred_cls_map_dl(sample_list2, net, gt, cls_map_save_path,model_type_flag)\n","\n","            testing_time = toc2 - tic2\n","            Test_Time_ALL.append(testing_time)\n","\n","\n","            # Output infors\n","            f = open(results_save_path + '_results.txt', 'a+')\n","            str_results = '\\n======================' \\\n","                          + \" learning rate=\" + str(learning_rate) \\\n","                          + \" epochs=\" + str(max_epoch) \\\n","                          + \" train ratio=\" + str(ratio_list[0]) \\\n","                          + \" val ratio=\" + str(ratio_list[1]) \\\n","                          + \" ======================\" \\\n","                          + \"\\nOA=\" + str(OA) \\\n","                          + \"\\nAA=\" + str(AA) \\\n","                          + '\\nkpp=' + str(kappa) \\\n","                          + '\\nacc per class:' + str(ECA) \\\n","                          + \"\\ntrain time:\" + str(training_time) \\\n","                          + \"\\ntest time:\" + str(testing_time) + \"\\n\"\n","\n","            f.write(str_results)\n","            f.write('{}'.format(confusion_matrix))\n","            f.write('\\n\\n')\n","            f.write('{}'.format(cls_report))\n","            f.close()\n","\n","            OA_ALL.append(OA)\n","            AA_ALL.append(AA)\n","            KPP_ALL.append(kappa)\n","            EACH_ACC_ALL.append(ECA)\n","\n","        torch.cuda.empty_cache()\n","        del net, train_iter, test_iter, val_iter\n","        # del net, train_iter, test_iter, val_iter, all_iter\n","        # del net\n","\n","    OA_ALL = np.array(OA_ALL)\n","    AA_ALL = np.array(AA_ALL)\n","    KPP_ALL = np.array(KPP_ALL)\n","    EACH_ACC_ALL = np.array(EACH_ACC_ALL)\n","    Train_Time_ALL = np.array(Train_Time_ALL)\n","    Test_Time_ALL = np.array(Test_Time_ALL)\n","\n","    np.set_printoptions(precision=4)\n","    print(\"\\n====================Mean result of {} times runs =========================\".format(len(seed_list)))\n","    print('List of OA:', list(OA_ALL))\n","    print('List of AA:', list(AA_ALL))\n","    print('List of KPP:', list(KPP_ALL))\n","    print('OA=', round(np.mean(OA_ALL) * 100, 2), '+-', round(np.std(OA_ALL) * 100, 2))\n","    print('AA=', round(np.mean(AA_ALL) * 100, 2), '+-', round(np.std(AA_ALL) * 100, 2))\n","    print('Kpp=', round(np.mean(KPP_ALL) * 100, 2), '+-', round(np.std(KPP_ALL) * 100, 2))\n","    print('Acc per class=', np.round(np.mean(EACH_ACC_ALL, 0) * 100, decimals=2), '+-',\n","          np.round(np.std(EACH_ACC_ALL, 0) * 100, decimals=2))\n","\n","    print(\"Average training time=\", round(np.mean(Train_Time_ALL), 2), '+-', round(np.std(Train_Time_ALL), 3))\n","    print(\"Average testing time=\", round(np.mean(Test_Time_ALL) * 1000, 2), '+-',\n","          round(np.std(Test_Time_ALL) * 1000, 3))\n","\n","    # Output infors\n","    f = open(results_save_path + '_results.txt', 'a+')\n","    str_results = '\\n\\n***************Mean result of ' + str(len(seed_list)) + 'times runs ********************' \\\n","                  + '\\nList of OA:' + str(list(OA_ALL)) \\\n","                  + '\\nList of AA:' + str(list(AA_ALL)) \\\n","                  + '\\nList of KPP:' + str(list(KPP_ALL)) \\\n","                  + '\\nOA=' + str(round(np.mean(OA_ALL) * 100, 2)) + '+-' + str(round(np.std(OA_ALL) * 100, 2)) \\\n","                  + '\\nAA=' + str(round(np.mean(AA_ALL) * 100, 2)) + '+-' + str(round(np.std(AA_ALL) * 100, 2)) \\\n","                  + '\\nKpp=' + str(round(np.mean(KPP_ALL) * 100, 2)) + '+-' + str(round(np.std(KPP_ALL) * 100, 2)) \\\n","                  + '\\nAcc per class=\\n' + str(np.round(np.mean(EACH_ACC_ALL, 0) * 100, 2)) + '+-' + str(\n","        np.round(np.std(EACH_ACC_ALL, 0) * 100, 2)) \\\n","                  + \"\\nAverage training time=\" + str(np.round(np.mean(Train_Time_ALL), decimals=2)) + '+-' + str(\n","        np.round(np.std(Train_Time_ALL), decimals=3)) \\\n","                  + \"\\nAverage testing time=\" + str(np.round(np.mean(Test_Time_ALL) * 1000, decimals=2)) + '+-' + str(\n","        np.round(np.std(Test_Time_ALL) * 100, decimals=3))\n","    f.write(str_results)\n","    f.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ATgpQ1R56bW9","executionInfo":{"status":"ok","timestamp":1680362684544,"user_tz":300,"elapsed":89785,"user":{"displayName":"Michael Wesselink","userId":"03807820748818897534"}},"outputId":"5cc32878-c64e-40c5-b1ca-5088f0a36c1b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["save model...\n","epoch: 1, training_sampler_num: 516, batch_count: 17.00, train loss: 1.189008, tarin loss sum: 20.213138, train acc: 0.667, train_acc_sum: 344.0, time: 0.2 sec\n","save model...\n","epoch: 2, training_sampler_num: 516, batch_count: 17.00, train loss: 2.383225, tarin loss sum: 40.514822, train acc: 0.231, train_acc_sum: 119.0, time: 0.2 sec\n","save model...\n","epoch: 3, training_sampler_num: 516, batch_count: 17.00, train loss: 1.547855, tarin loss sum: 26.313537, train acc: 0.475, train_acc_sum: 245.0, time: 0.2 sec\n","save model...\n","epoch: 4, training_sampler_num: 516, batch_count: 17.00, train loss: 0.757702, tarin loss sum: 12.880942, train acc: 0.731, train_acc_sum: 377.0, time: 0.2 sec\n","save model...\n","epoch: 5, training_sampler_num: 516, batch_count: 17.00, train loss: 0.529092, tarin loss sum: 8.994559, train acc: 0.812, train_acc_sum: 419.0, time: 0.2 sec\n","epoch: 6, training_sampler_num: 516, batch_count: 17.00, train loss: 0.529610, tarin loss sum: 9.003364, train acc: 0.828, train_acc_sum: 427.0, time: 0.2 sec\n","save model...\n","epoch: 7, training_sampler_num: 516, batch_count: 17.00, train loss: 0.465906, tarin loss sum: 7.920408, train acc: 0.824, train_acc_sum: 425.0, time: 0.2 sec\n","save model...\n","epoch: 8, training_sampler_num: 516, batch_count: 17.00, train loss: 0.345313, tarin loss sum: 5.870329, train acc: 0.876, train_acc_sum: 452.0, time: 0.2 sec\n","save model...\n","epoch: 9, training_sampler_num: 516, batch_count: 17.00, train loss: 0.285802, tarin loss sum: 4.858641, train acc: 0.899, train_acc_sum: 464.0, time: 0.2 sec\n","epoch: 10, training_sampler_num: 516, batch_count: 17.00, train loss: 0.261132, tarin loss sum: 4.439250, train acc: 0.893, train_acc_sum: 461.0, time: 0.2 sec\n","save model...\n","epoch: 11, training_sampler_num: 516, batch_count: 17.00, train loss: 0.219620, tarin loss sum: 3.733542, train acc: 0.928, train_acc_sum: 479.0, time: 0.2 sec\n","save model...\n","epoch: 12, training_sampler_num: 516, batch_count: 17.00, train loss: 0.162601, tarin loss sum: 2.764221, train acc: 0.950, train_acc_sum: 490.0, time: 0.2 sec\n","epoch: 13, training_sampler_num: 516, batch_count: 17.00, train loss: 0.178202, tarin loss sum: 3.029436, train acc: 0.936, train_acc_sum: 483.0, time: 0.2 sec\n","epoch: 14, training_sampler_num: 516, batch_count: 17.00, train loss: 0.177254, tarin loss sum: 3.013312, train acc: 0.952, train_acc_sum: 491.0, time: 0.2 sec\n","epoch: 15, training_sampler_num: 516, batch_count: 17.00, train loss: 0.206237, tarin loss sum: 3.506024, train acc: 0.915, train_acc_sum: 472.0, time: 0.2 sec\n","epoch: 16, training_sampler_num: 516, batch_count: 17.00, train loss: 0.152988, tarin loss sum: 2.600788, train acc: 0.950, train_acc_sum: 490.0, time: 0.2 sec\n","epoch: 17, training_sampler_num: 516, batch_count: 17.00, train loss: 0.112283, tarin loss sum: 1.908812, train acc: 0.957, train_acc_sum: 494.0, time: 0.2 sec\n","epoch: 18, training_sampler_num: 516, batch_count: 17.00, train loss: 0.104027, tarin loss sum: 1.768453, train acc: 0.953, train_acc_sum: 492.0, time: 0.2 sec\n","epoch: 19, training_sampler_num: 516, batch_count: 17.00, train loss: 0.088059, tarin loss sum: 1.497010, train acc: 0.965, train_acc_sum: 498.0, time: 0.2 sec\n","epoch: 20, training_sampler_num: 516, batch_count: 17.00, train loss: 0.047265, tarin loss sum: 0.803501, train acc: 0.986, train_acc_sum: 509.0, time: 0.2 sec\n","epoch: 21, training_sampler_num: 516, batch_count: 17.00, train loss: 0.035948, tarin loss sum: 0.611113, train acc: 0.992, train_acc_sum: 512.0, time: 0.2 sec\n","save model...\n","epoch: 22, training_sampler_num: 516, batch_count: 17.00, train loss: 0.030073, tarin loss sum: 0.511245, train acc: 0.992, train_acc_sum: 512.0, time: 0.2 sec\n","epoch: 23, training_sampler_num: 516, batch_count: 17.00, train loss: 0.022711, tarin loss sum: 0.386084, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 24, training_sampler_num: 516, batch_count: 17.00, train loss: 0.022755, tarin loss sum: 0.386832, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 25, training_sampler_num: 516, batch_count: 17.00, train loss: 0.021104, tarin loss sum: 0.358762, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","save model...\n","epoch: 26, training_sampler_num: 516, batch_count: 17.00, train loss: 0.026129, tarin loss sum: 0.444195, train acc: 0.992, train_acc_sum: 512.0, time: 0.2 sec\n","epoch: 27, training_sampler_num: 516, batch_count: 17.00, train loss: 0.028366, tarin loss sum: 0.482226, train acc: 0.988, train_acc_sum: 510.0, time: 0.2 sec\n","epoch: 28, training_sampler_num: 516, batch_count: 17.00, train loss: 0.021433, tarin loss sum: 0.364355, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","save model...\n","epoch: 29, training_sampler_num: 516, batch_count: 17.00, train loss: 0.013693, tarin loss sum: 0.232780, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 30, training_sampler_num: 516, batch_count: 17.00, train loss: 0.034310, tarin loss sum: 0.583262, train acc: 0.986, train_acc_sum: 509.0, time: 0.2 sec\n","epoch: 31, training_sampler_num: 516, batch_count: 17.00, train loss: 0.024990, tarin loss sum: 0.424828, train acc: 0.990, train_acc_sum: 511.0, time: 0.2 sec\n","epoch: 32, training_sampler_num: 516, batch_count: 17.00, train loss: 0.015102, tarin loss sum: 0.256729, train acc: 0.996, train_acc_sum: 514.0, time: 0.2 sec\n","epoch: 33, training_sampler_num: 516, batch_count: 17.00, train loss: 0.010319, tarin loss sum: 0.175430, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 34, training_sampler_num: 516, batch_count: 17.00, train loss: 0.005549, tarin loss sum: 0.094331, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 35, training_sampler_num: 516, batch_count: 17.00, train loss: 0.005192, tarin loss sum: 0.088256, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 36, training_sampler_num: 516, batch_count: 17.00, train loss: 0.005126, tarin loss sum: 0.087150, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 37, training_sampler_num: 516, batch_count: 17.00, train loss: 0.009950, tarin loss sum: 0.169148, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 38, training_sampler_num: 516, batch_count: 17.00, train loss: 0.006941, tarin loss sum: 0.117989, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 39, training_sampler_num: 516, batch_count: 17.00, train loss: 0.003791, tarin loss sum: 0.064447, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","save model...\n","epoch: 40, training_sampler_num: 516, batch_count: 17.00, train loss: 0.003565, tarin loss sum: 0.060599, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 41, training_sampler_num: 516, batch_count: 17.00, train loss: 0.005019, tarin loss sum: 0.085324, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 42, training_sampler_num: 516, batch_count: 17.00, train loss: 0.018791, tarin loss sum: 0.319439, train acc: 0.992, train_acc_sum: 512.0, time: 0.2 sec\n","epoch: 43, training_sampler_num: 516, batch_count: 17.00, train loss: 0.014036, tarin loss sum: 0.238604, train acc: 0.996, train_acc_sum: 514.0, time: 0.2 sec\n","save model...\n","epoch: 44, training_sampler_num: 516, batch_count: 17.00, train loss: 0.021760, tarin loss sum: 0.369923, train acc: 0.994, train_acc_sum: 513.0, time: 0.2 sec\n","save model...\n","epoch: 45, training_sampler_num: 516, batch_count: 17.00, train loss: 0.016488, tarin loss sum: 0.280301, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 46, training_sampler_num: 516, batch_count: 17.00, train loss: 0.005768, tarin loss sum: 0.098048, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 47, training_sampler_num: 516, batch_count: 17.00, train loss: 0.006406, tarin loss sum: 0.108898, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 48, training_sampler_num: 516, batch_count: 17.00, train loss: 0.005286, tarin loss sum: 0.089867, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 49, training_sampler_num: 516, batch_count: 17.00, train loss: 0.003904, tarin loss sum: 0.066362, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 50, training_sampler_num: 516, batch_count: 17.00, train loss: 0.002057, tarin loss sum: 0.034963, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 51, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001382, tarin loss sum: 0.023493, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 52, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001192, tarin loss sum: 0.020269, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 53, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001052, tarin loss sum: 0.017880, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 54, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000966, tarin loss sum: 0.016414, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 55, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000928, tarin loss sum: 0.015780, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 56, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000884, tarin loss sum: 0.015025, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 57, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000856, tarin loss sum: 0.014546, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 58, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000786, tarin loss sum: 0.013368, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 59, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000722, tarin loss sum: 0.012274, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 60, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000710, tarin loss sum: 0.012077, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 61, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000644, tarin loss sum: 0.010944, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 62, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000656, tarin loss sum: 0.011155, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 63, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000603, tarin loss sum: 0.010256, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 64, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000574, tarin loss sum: 0.009751, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 65, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000561, tarin loss sum: 0.009538, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 66, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000538, tarin loss sum: 0.009144, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 67, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000542, tarin loss sum: 0.009208, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 68, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000504, tarin loss sum: 0.008561, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 69, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000497, tarin loss sum: 0.008457, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 70, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000451, tarin loss sum: 0.007659, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 71, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000451, tarin loss sum: 0.007670, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 72, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000431, tarin loss sum: 0.007320, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 73, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000402, tarin loss sum: 0.006833, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 74, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000395, tarin loss sum: 0.006711, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 75, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000386, tarin loss sum: 0.006567, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 76, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000368, tarin loss sum: 0.006251, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 77, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000358, tarin loss sum: 0.006087, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 78, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000359, tarin loss sum: 0.006106, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 79, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000334, tarin loss sum: 0.005684, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 80, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000343, tarin loss sum: 0.005832, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 81, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000351, tarin loss sum: 0.005975, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 82, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000305, tarin loss sum: 0.005179, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 83, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000312, tarin loss sum: 0.005301, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 84, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000330, tarin loss sum: 0.005604, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 85, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000294, tarin loss sum: 0.005006, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","save model...\n","epoch: 86, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000278, tarin loss sum: 0.004725, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 87, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000267, tarin loss sum: 0.004541, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 88, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000259, tarin loss sum: 0.004397, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 89, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000245, tarin loss sum: 0.004170, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 90, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000262, tarin loss sum: 0.004461, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 91, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000405, tarin loss sum: 0.006879, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 92, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000432, tarin loss sum: 0.007338, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 93, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000249, tarin loss sum: 0.004237, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 94, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000224, tarin loss sum: 0.003814, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 95, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000213, tarin loss sum: 0.003614, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 96, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000204, tarin loss sum: 0.003476, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 97, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000197, tarin loss sum: 0.003357, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 98, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000206, tarin loss sum: 0.003496, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 99, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000211, tarin loss sum: 0.003588, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","save model...\n","epoch: 100, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000189, tarin loss sum: 0.003213, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","Training stage finished:\n"," epoch 100, loss 0.0002, train acc 1.000, training time 16.57 s\n","\n","\n","====================Starting evaluation for testing set.========================\n","\n","confusion_matrix\n","[[676   0   0   0   0   0   0   0   0   0   0   0   0]\n"," [  0 214   0   0   0   4   0   0   0   0   0   0   0]\n"," [  0   0 221   5   0   0   0   0   0   0   0   0   0]\n"," [  0   1   6 209  11   0   0   0   0   0   0   0   0]\n"," [  0   0   1   5 131   0   0   0   0   0   0   0   0]\n"," [  2   2   0   6   2 201   0   0   0   0   0   0   0]\n"," [  0   0   1   0   0   0  94   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0 384   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0 463  16   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0 344   0   1   0]\n"," [  0   0   0   0   0   0   0   0   0   0 374   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0 447   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0 826]]\n","classification_report\n","                          precision    recall  f1-score   support\n","\n","                   Scrub       1.00      1.00      1.00       678\n","            Willow swamp       0.98      0.99      0.98       217\n","    Cabbage palm hammock       0.98      0.97      0.97       229\n","Cabbage palm/oak hammock       0.92      0.93      0.92       225\n","              Slash pine       0.96      0.91      0.93       144\n","   Oak/broadleaf hammock       0.94      0.98      0.96       205\n","          Hardwood swamp       0.99      1.00      0.99        94\n","         Graminoid marsh       1.00      1.00      1.00       384\n","          Spartine marsh       0.97      1.00      0.98       463\n","           Cattail marsh       1.00      0.96      0.98       360\n","              Salt marsh       1.00      1.00      1.00       374\n","               Mud flats       1.00      1.00      1.00       448\n","                   Water       1.00      1.00      1.00       826\n","\n","                accuracy                           0.99      4647\n","               macro avg       0.98      0.98      0.98      4647\n","            weighted avg       0.99      0.99      0.99      4647\n","\n","save model...\n","epoch: 1, training_sampler_num: 516, batch_count: 17.00, train loss: 1.248329, tarin loss sum: 21.221585, train acc: 0.593, train_acc_sum: 306.0, time: 0.2 sec\n","save model...\n","epoch: 2, training_sampler_num: 516, batch_count: 17.00, train loss: 2.344274, tarin loss sum: 39.852661, train acc: 0.236, train_acc_sum: 122.0, time: 0.2 sec\n","save model...\n","epoch: 3, training_sampler_num: 516, batch_count: 17.00, train loss: 1.716061, tarin loss sum: 29.173041, train acc: 0.434, train_acc_sum: 224.0, time: 0.2 sec\n","save model...\n","epoch: 4, training_sampler_num: 516, batch_count: 17.00, train loss: 0.911860, tarin loss sum: 15.501625, train acc: 0.694, train_acc_sum: 358.0, time: 0.2 sec\n","save model...\n","epoch: 5, training_sampler_num: 516, batch_count: 17.00, train loss: 0.625123, tarin loss sum: 10.627098, train acc: 0.738, train_acc_sum: 381.0, time: 0.2 sec\n","save model...\n","epoch: 6, training_sampler_num: 516, batch_count: 17.00, train loss: 0.453494, tarin loss sum: 7.709392, train acc: 0.841, train_acc_sum: 434.0, time: 0.2 sec\n","epoch: 7, training_sampler_num: 516, batch_count: 17.00, train loss: 0.386912, tarin loss sum: 6.577508, train acc: 0.862, train_acc_sum: 445.0, time: 0.2 sec\n","save model...\n","epoch: 8, training_sampler_num: 516, batch_count: 17.00, train loss: 0.319826, tarin loss sum: 5.437035, train acc: 0.874, train_acc_sum: 451.0, time: 0.2 sec\n","save model...\n","epoch: 9, training_sampler_num: 516, batch_count: 17.00, train loss: 0.267139, tarin loss sum: 4.541371, train acc: 0.903, train_acc_sum: 466.0, time: 0.2 sec\n","save model...\n","epoch: 10, training_sampler_num: 516, batch_count: 17.00, train loss: 0.228481, tarin loss sum: 3.884173, train acc: 0.921, train_acc_sum: 475.0, time: 0.2 sec\n","save model...\n","epoch: 11, training_sampler_num: 516, batch_count: 17.00, train loss: 0.212843, tarin loss sum: 3.618324, train acc: 0.938, train_acc_sum: 484.0, time: 0.2 sec\n","epoch: 12, training_sampler_num: 516, batch_count: 17.00, train loss: 0.180895, tarin loss sum: 3.075215, train acc: 0.940, train_acc_sum: 485.0, time: 0.2 sec\n","epoch: 13, training_sampler_num: 516, batch_count: 17.00, train loss: 0.146292, tarin loss sum: 2.486962, train acc: 0.955, train_acc_sum: 493.0, time: 0.2 sec\n","epoch: 14, training_sampler_num: 516, batch_count: 17.00, train loss: 0.098485, tarin loss sum: 1.674242, train acc: 0.967, train_acc_sum: 499.0, time: 0.2 sec\n","epoch: 15, training_sampler_num: 516, batch_count: 17.00, train loss: 0.097876, tarin loss sum: 1.663889, train acc: 0.959, train_acc_sum: 495.0, time: 0.2 sec\n","save model...\n","epoch: 16, training_sampler_num: 516, batch_count: 17.00, train loss: 0.091496, tarin loss sum: 1.555428, train acc: 0.965, train_acc_sum: 498.0, time: 0.2 sec\n","epoch: 17, training_sampler_num: 516, batch_count: 17.00, train loss: 0.087083, tarin loss sum: 1.480407, train acc: 0.981, train_acc_sum: 506.0, time: 0.2 sec\n","epoch: 18, training_sampler_num: 516, batch_count: 17.00, train loss: 0.112691, tarin loss sum: 1.915746, train acc: 0.957, train_acc_sum: 494.0, time: 0.2 sec\n","save model...\n","epoch: 19, training_sampler_num: 516, batch_count: 17.00, train loss: 0.070242, tarin loss sum: 1.194121, train acc: 0.979, train_acc_sum: 505.0, time: 0.2 sec\n","save model...\n","epoch: 20, training_sampler_num: 516, batch_count: 17.00, train loss: 0.061958, tarin loss sum: 1.053293, train acc: 0.983, train_acc_sum: 507.0, time: 0.2 sec\n","epoch: 21, training_sampler_num: 516, batch_count: 17.00, train loss: 0.060338, tarin loss sum: 1.025745, train acc: 0.981, train_acc_sum: 506.0, time: 0.2 sec\n","save model...\n","epoch: 22, training_sampler_num: 516, batch_count: 17.00, train loss: 0.032749, tarin loss sum: 0.556732, train acc: 0.988, train_acc_sum: 510.0, time: 0.2 sec\n","epoch: 23, training_sampler_num: 516, batch_count: 17.00, train loss: 0.028575, tarin loss sum: 0.485775, train acc: 0.992, train_acc_sum: 512.0, time: 0.2 sec\n","epoch: 24, training_sampler_num: 516, batch_count: 17.00, train loss: 0.022950, tarin loss sum: 0.390144, train acc: 0.994, train_acc_sum: 513.0, time: 0.2 sec\n","epoch: 25, training_sampler_num: 516, batch_count: 17.00, train loss: 0.084196, tarin loss sum: 1.431336, train acc: 0.992, train_acc_sum: 512.0, time: 0.2 sec\n","epoch: 26, training_sampler_num: 516, batch_count: 17.00, train loss: 0.396381, tarin loss sum: 6.738480, train acc: 0.917, train_acc_sum: 473.0, time: 0.2 sec\n","epoch: 27, training_sampler_num: 516, batch_count: 17.00, train loss: 0.149459, tarin loss sum: 2.540810, train acc: 0.936, train_acc_sum: 483.0, time: 0.2 sec\n","epoch: 28, training_sampler_num: 516, batch_count: 17.00, train loss: 0.067493, tarin loss sum: 1.147385, train acc: 0.979, train_acc_sum: 505.0, time: 0.2 sec\n","epoch: 29, training_sampler_num: 516, batch_count: 17.00, train loss: 0.041127, tarin loss sum: 0.699167, train acc: 0.990, train_acc_sum: 511.0, time: 0.2 sec\n","save model...\n","epoch: 30, training_sampler_num: 516, batch_count: 17.00, train loss: 0.024729, tarin loss sum: 0.420387, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","save model...\n","epoch: 31, training_sampler_num: 516, batch_count: 17.00, train loss: 0.021161, tarin loss sum: 0.359743, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 32, training_sampler_num: 516, batch_count: 17.00, train loss: 0.019044, tarin loss sum: 0.323751, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 33, training_sampler_num: 516, batch_count: 17.00, train loss: 0.013120, tarin loss sum: 0.223040, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","save model...\n","epoch: 34, training_sampler_num: 516, batch_count: 17.00, train loss: 0.011709, tarin loss sum: 0.199045, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 35, training_sampler_num: 516, batch_count: 17.00, train loss: 0.009115, tarin loss sum: 0.154961, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 36, training_sampler_num: 516, batch_count: 17.00, train loss: 0.007328, tarin loss sum: 0.124575, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 37, training_sampler_num: 516, batch_count: 17.00, train loss: 0.011599, tarin loss sum: 0.197175, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 38, training_sampler_num: 516, batch_count: 17.00, train loss: 0.011932, tarin loss sum: 0.202850, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 39, training_sampler_num: 516, batch_count: 17.00, train loss: 0.017892, tarin loss sum: 0.304158, train acc: 0.992, train_acc_sum: 512.0, time: 0.2 sec\n","epoch: 40, training_sampler_num: 516, batch_count: 17.00, train loss: 0.006042, tarin loss sum: 0.102718, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 41, training_sampler_num: 516, batch_count: 17.00, train loss: 0.004837, tarin loss sum: 0.082222, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 42, training_sampler_num: 516, batch_count: 17.00, train loss: 0.003577, tarin loss sum: 0.060806, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","save model...\n","epoch: 43, training_sampler_num: 516, batch_count: 17.00, train loss: 0.003062, tarin loss sum: 0.052052, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 44, training_sampler_num: 516, batch_count: 17.00, train loss: 0.002668, tarin loss sum: 0.045356, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 45, training_sampler_num: 516, batch_count: 17.00, train loss: 0.002417, tarin loss sum: 0.041083, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 46, training_sampler_num: 516, batch_count: 17.00, train loss: 0.002186, tarin loss sum: 0.037169, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 47, training_sampler_num: 516, batch_count: 17.00, train loss: 0.002222, tarin loss sum: 0.037780, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 48, training_sampler_num: 516, batch_count: 17.00, train loss: 0.002006, tarin loss sum: 0.034103, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 49, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001811, tarin loss sum: 0.030784, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 50, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001738, tarin loss sum: 0.029554, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 51, training_sampler_num: 516, batch_count: 17.00, train loss: 0.002285, tarin loss sum: 0.038852, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 52, training_sampler_num: 516, batch_count: 17.00, train loss: 0.002200, tarin loss sum: 0.037408, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","save model...\n","epoch: 53, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001348, tarin loss sum: 0.022910, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 54, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001252, tarin loss sum: 0.021283, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 55, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001205, tarin loss sum: 0.020484, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 56, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001123, tarin loss sum: 0.019098, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 57, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001184, tarin loss sum: 0.020128, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 58, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001032, tarin loss sum: 0.017546, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 59, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000938, tarin loss sum: 0.015942, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 60, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000902, tarin loss sum: 0.015336, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 61, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000931, tarin loss sum: 0.015824, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 62, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000855, tarin loss sum: 0.014534, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 63, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000752, tarin loss sum: 0.012792, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 64, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000712, tarin loss sum: 0.012101, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 65, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000707, tarin loss sum: 0.012018, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 66, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000688, tarin loss sum: 0.011704, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 67, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000699, tarin loss sum: 0.011876, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 68, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000623, tarin loss sum: 0.010584, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 69, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000610, tarin loss sum: 0.010362, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 70, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000596, tarin loss sum: 0.010130, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 71, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000535, tarin loss sum: 0.009094, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 72, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000533, tarin loss sum: 0.009055, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 73, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000780, tarin loss sum: 0.013267, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 74, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000711, tarin loss sum: 0.012094, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 75, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000452, tarin loss sum: 0.007691, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 76, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000441, tarin loss sum: 0.007492, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 77, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000431, tarin loss sum: 0.007322, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 78, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000404, tarin loss sum: 0.006868, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 79, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000390, tarin loss sum: 0.006636, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 80, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000411, tarin loss sum: 0.006984, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 81, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000364, tarin loss sum: 0.006183, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 82, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000357, tarin loss sum: 0.006072, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 83, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000343, tarin loss sum: 0.005836, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 84, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000330, tarin loss sum: 0.005607, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 85, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000315, tarin loss sum: 0.005347, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 86, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000310, tarin loss sum: 0.005271, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 87, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000340, tarin loss sum: 0.005774, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 88, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000294, tarin loss sum: 0.004995, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 89, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000358, tarin loss sum: 0.006092, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 90, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000300, tarin loss sum: 0.005106, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 91, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000273, tarin loss sum: 0.004646, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 92, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000264, tarin loss sum: 0.004495, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 93, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000257, tarin loss sum: 0.004366, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 94, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000264, tarin loss sum: 0.004494, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 95, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000238, tarin loss sum: 0.004040, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 96, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000231, tarin loss sum: 0.003924, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 97, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000227, tarin loss sum: 0.003863, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 98, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000214, tarin loss sum: 0.003634, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 99, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000212, tarin loss sum: 0.003598, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 100, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000209, tarin loss sum: 0.003551, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","Training stage finished:\n"," epoch 100, loss 0.0002, train acc 1.000, training time 16.59 s\n","\n","\n","====================Starting evaluation for testing set.========================\n","\n","confusion_matrix\n","[[678   0   0   0   0   0   0   0   0   0   0   0   0]\n"," [  0 213   0   0   0   4   2   0   0   0   0   0   0]\n"," [  0   0 229  17   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0 202  16   1   0   0   0   0  10   0   0]\n"," [  0   0   0   6 126   0   0   0   0   0   0   0   0]\n"," [  0   4   0   0   2 200   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0  92   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0 384   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0 463   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0 360   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0 364   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0 448   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0 826]]\n","classification_report\n","                          precision    recall  f1-score   support\n","\n","                   Scrub       1.00      1.00      1.00       678\n","            Willow swamp       0.97      0.98      0.98       217\n","    Cabbage palm hammock       0.93      1.00      0.96       229\n","Cabbage palm/oak hammock       0.88      0.90      0.89       225\n","              Slash pine       0.95      0.88      0.91       144\n","   Oak/broadleaf hammock       0.97      0.98      0.97       205\n","          Hardwood swamp       1.00      0.98      0.99        94\n","         Graminoid marsh       1.00      1.00      1.00       384\n","          Spartine marsh       1.00      1.00      1.00       463\n","           Cattail marsh       1.00      1.00      1.00       360\n","              Salt marsh       1.00      0.97      0.99       374\n","               Mud flats       1.00      1.00      1.00       448\n","                   Water       1.00      1.00      1.00       826\n","\n","                accuracy                           0.99      4647\n","               macro avg       0.98      0.98      0.98      4647\n","            weighted avg       0.99      0.99      0.99      4647\n","\n","save model...\n","epoch: 1, training_sampler_num: 516, batch_count: 17.00, train loss: 1.207967, tarin loss sum: 20.535446, train acc: 0.647, train_acc_sum: 334.0, time: 0.2 sec\n","save model...\n","epoch: 2, training_sampler_num: 516, batch_count: 17.00, train loss: 2.480956, tarin loss sum: 42.176253, train acc: 0.141, train_acc_sum: 73.0, time: 0.2 sec\n","save model...\n","epoch: 3, training_sampler_num: 516, batch_count: 17.00, train loss: 2.226789, tarin loss sum: 37.855414, train acc: 0.316, train_acc_sum: 163.0, time: 0.2 sec\n","save model...\n","epoch: 4, training_sampler_num: 516, batch_count: 17.00, train loss: 1.778401, tarin loss sum: 30.232815, train acc: 0.360, train_acc_sum: 186.0, time: 0.2 sec\n","save model...\n","epoch: 5, training_sampler_num: 516, batch_count: 17.00, train loss: 1.262780, tarin loss sum: 21.467259, train acc: 0.548, train_acc_sum: 283.0, time: 0.2 sec\n","save model...\n","epoch: 6, training_sampler_num: 516, batch_count: 17.00, train loss: 0.827509, tarin loss sum: 14.067651, train acc: 0.686, train_acc_sum: 354.0, time: 0.2 sec\n","save model...\n","epoch: 7, training_sampler_num: 516, batch_count: 17.00, train loss: 0.600230, tarin loss sum: 10.203908, train acc: 0.779, train_acc_sum: 402.0, time: 0.2 sec\n","save model...\n","epoch: 8, training_sampler_num: 516, batch_count: 17.00, train loss: 0.492535, tarin loss sum: 8.373090, train acc: 0.789, train_acc_sum: 407.0, time: 0.2 sec\n","save model...\n","epoch: 9, training_sampler_num: 516, batch_count: 17.00, train loss: 0.464194, tarin loss sum: 7.891304, train acc: 0.828, train_acc_sum: 427.0, time: 0.2 sec\n","save model...\n","epoch: 10, training_sampler_num: 516, batch_count: 17.00, train loss: 0.278605, tarin loss sum: 4.736277, train acc: 0.893, train_acc_sum: 461.0, time: 0.2 sec\n","save model...\n","epoch: 11, training_sampler_num: 516, batch_count: 17.00, train loss: 0.177326, tarin loss sum: 3.014538, train acc: 0.952, train_acc_sum: 491.0, time: 0.2 sec\n","save model...\n","epoch: 12, training_sampler_num: 516, batch_count: 17.00, train loss: 0.134963, tarin loss sum: 2.294364, train acc: 0.961, train_acc_sum: 496.0, time: 0.2 sec\n","save model...\n","epoch: 13, training_sampler_num: 516, batch_count: 17.00, train loss: 0.113276, tarin loss sum: 1.925698, train acc: 0.975, train_acc_sum: 503.0, time: 0.2 sec\n","save model...\n","epoch: 14, training_sampler_num: 516, batch_count: 17.00, train loss: 0.101541, tarin loss sum: 1.726196, train acc: 0.963, train_acc_sum: 497.0, time: 0.2 sec\n","epoch: 15, training_sampler_num: 516, batch_count: 17.00, train loss: 0.100897, tarin loss sum: 1.715246, train acc: 0.965, train_acc_sum: 498.0, time: 0.2 sec\n","epoch: 16, training_sampler_num: 516, batch_count: 17.00, train loss: 0.102034, tarin loss sum: 1.734586, train acc: 0.965, train_acc_sum: 498.0, time: 0.2 sec\n","epoch: 17, training_sampler_num: 516, batch_count: 17.00, train loss: 0.175996, tarin loss sum: 2.991935, train acc: 0.940, train_acc_sum: 485.0, time: 0.2 sec\n","epoch: 18, training_sampler_num: 516, batch_count: 17.00, train loss: 0.143262, tarin loss sum: 2.435458, train acc: 0.942, train_acc_sum: 486.0, time: 0.2 sec\n","epoch: 19, training_sampler_num: 516, batch_count: 17.00, train loss: 0.081473, tarin loss sum: 1.385049, train acc: 0.979, train_acc_sum: 505.0, time: 0.2 sec\n","epoch: 20, training_sampler_num: 516, batch_count: 17.00, train loss: 0.064742, tarin loss sum: 1.100619, train acc: 0.981, train_acc_sum: 506.0, time: 0.2 sec\n","save model...\n","epoch: 21, training_sampler_num: 516, batch_count: 17.00, train loss: 0.058133, tarin loss sum: 0.988253, train acc: 0.988, train_acc_sum: 510.0, time: 0.2 sec\n","save model...\n","epoch: 22, training_sampler_num: 516, batch_count: 17.00, train loss: 0.059203, tarin loss sum: 1.006452, train acc: 0.983, train_acc_sum: 507.0, time: 0.2 sec\n","epoch: 23, training_sampler_num: 516, batch_count: 17.00, train loss: 0.045273, tarin loss sum: 0.769638, train acc: 0.986, train_acc_sum: 509.0, time: 0.2 sec\n","epoch: 24, training_sampler_num: 516, batch_count: 17.00, train loss: 0.048544, tarin loss sum: 0.825251, train acc: 0.984, train_acc_sum: 508.0, time: 0.2 sec\n","epoch: 25, training_sampler_num: 516, batch_count: 17.00, train loss: 0.040200, tarin loss sum: 0.683397, train acc: 0.983, train_acc_sum: 507.0, time: 0.2 sec\n","epoch: 26, training_sampler_num: 516, batch_count: 17.00, train loss: 0.072147, tarin loss sum: 1.226501, train acc: 0.975, train_acc_sum: 503.0, time: 0.2 sec\n","epoch: 27, training_sampler_num: 516, batch_count: 17.00, train loss: 0.079037, tarin loss sum: 1.343632, train acc: 0.973, train_acc_sum: 502.0, time: 0.2 sec\n","epoch: 28, training_sampler_num: 516, batch_count: 17.00, train loss: 0.109545, tarin loss sum: 1.862270, train acc: 0.969, train_acc_sum: 500.0, time: 0.2 sec\n","epoch: 29, training_sampler_num: 516, batch_count: 17.00, train loss: 0.042156, tarin loss sum: 0.716658, train acc: 0.992, train_acc_sum: 512.0, time: 0.2 sec\n","epoch: 30, training_sampler_num: 516, batch_count: 17.00, train loss: 0.031736, tarin loss sum: 0.539507, train acc: 0.996, train_acc_sum: 514.0, time: 0.2 sec\n","epoch: 31, training_sampler_num: 516, batch_count: 17.00, train loss: 0.022209, tarin loss sum: 0.377554, train acc: 0.994, train_acc_sum: 513.0, time: 0.2 sec\n","epoch: 32, training_sampler_num: 516, batch_count: 17.00, train loss: 0.026017, tarin loss sum: 0.442284, train acc: 0.992, train_acc_sum: 512.0, time: 0.2 sec\n","epoch: 33, training_sampler_num: 516, batch_count: 17.00, train loss: 0.024883, tarin loss sum: 0.423012, train acc: 0.990, train_acc_sum: 511.0, time: 0.2 sec\n","save model...\n","epoch: 34, training_sampler_num: 516, batch_count: 17.00, train loss: 0.028912, tarin loss sum: 0.491499, train acc: 0.996, train_acc_sum: 514.0, time: 0.2 sec\n","epoch: 35, training_sampler_num: 516, batch_count: 17.00, train loss: 0.019281, tarin loss sum: 0.327769, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 36, training_sampler_num: 516, batch_count: 17.00, train loss: 0.022001, tarin loss sum: 0.374016, train acc: 0.988, train_acc_sum: 510.0, time: 0.2 sec\n","save model...\n","epoch: 37, training_sampler_num: 516, batch_count: 17.00, train loss: 0.014033, tarin loss sum: 0.238562, train acc: 0.996, train_acc_sum: 514.0, time: 0.2 sec\n","epoch: 38, training_sampler_num: 516, batch_count: 17.00, train loss: 0.019310, tarin loss sum: 0.328268, train acc: 0.996, train_acc_sum: 514.0, time: 0.2 sec\n","epoch: 39, training_sampler_num: 516, batch_count: 17.00, train loss: 0.012510, tarin loss sum: 0.212671, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 40, training_sampler_num: 516, batch_count: 17.00, train loss: 0.008954, tarin loss sum: 0.152223, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 41, training_sampler_num: 516, batch_count: 17.00, train loss: 0.007914, tarin loss sum: 0.134542, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 42, training_sampler_num: 516, batch_count: 17.00, train loss: 0.011500, tarin loss sum: 0.195504, train acc: 0.996, train_acc_sum: 514.0, time: 0.2 sec\n","epoch: 43, training_sampler_num: 516, batch_count: 17.00, train loss: 0.008582, tarin loss sum: 0.145902, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 44, training_sampler_num: 516, batch_count: 17.00, train loss: 0.005039, tarin loss sum: 0.085667, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 45, training_sampler_num: 516, batch_count: 17.00, train loss: 0.005009, tarin loss sum: 0.085145, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 46, training_sampler_num: 516, batch_count: 17.00, train loss: 0.023110, tarin loss sum: 0.392875, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 47, training_sampler_num: 516, batch_count: 17.00, train loss: 0.229005, tarin loss sum: 3.893086, train acc: 0.942, train_acc_sum: 486.0, time: 0.2 sec\n","epoch: 48, training_sampler_num: 516, batch_count: 17.00, train loss: 0.081913, tarin loss sum: 1.392525, train acc: 0.981, train_acc_sum: 506.0, time: 0.2 sec\n","epoch: 49, training_sampler_num: 516, batch_count: 17.00, train loss: 0.109297, tarin loss sum: 1.858046, train acc: 0.963, train_acc_sum: 497.0, time: 0.2 sec\n","epoch: 50, training_sampler_num: 516, batch_count: 17.00, train loss: 0.056053, tarin loss sum: 0.952908, train acc: 0.981, train_acc_sum: 506.0, time: 0.2 sec\n","epoch: 51, training_sampler_num: 516, batch_count: 17.00, train loss: 0.020887, tarin loss sum: 0.355086, train acc: 0.994, train_acc_sum: 513.0, time: 0.2 sec\n","epoch: 52, training_sampler_num: 516, batch_count: 17.00, train loss: 0.010929, tarin loss sum: 0.185789, train acc: 0.996, train_acc_sum: 514.0, time: 0.2 sec\n","epoch: 53, training_sampler_num: 516, batch_count: 17.00, train loss: 0.007717, tarin loss sum: 0.131182, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 54, training_sampler_num: 516, batch_count: 17.00, train loss: 0.005813, tarin loss sum: 0.098826, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 55, training_sampler_num: 516, batch_count: 17.00, train loss: 0.004760, tarin loss sum: 0.080927, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 56, training_sampler_num: 516, batch_count: 17.00, train loss: 0.005763, tarin loss sum: 0.097963, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 57, training_sampler_num: 516, batch_count: 17.00, train loss: 0.004867, tarin loss sum: 0.082735, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 58, training_sampler_num: 516, batch_count: 17.00, train loss: 0.003413, tarin loss sum: 0.058023, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 59, training_sampler_num: 516, batch_count: 17.00, train loss: 0.002760, tarin loss sum: 0.046917, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 60, training_sampler_num: 516, batch_count: 17.00, train loss: 0.002352, tarin loss sum: 0.039981, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 61, training_sampler_num: 516, batch_count: 17.00, train loss: 0.002150, tarin loss sum: 0.036558, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 62, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001900, tarin loss sum: 0.032302, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 63, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001977, tarin loss sum: 0.033608, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 64, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001606, tarin loss sum: 0.027304, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 65, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001569, tarin loss sum: 0.026673, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 66, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001449, tarin loss sum: 0.024633, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 67, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001351, tarin loss sum: 0.022972, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 68, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001221, tarin loss sum: 0.020754, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 69, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001288, tarin loss sum: 0.021897, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 70, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001437, tarin loss sum: 0.024426, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 71, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001489, tarin loss sum: 0.025310, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 72, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001268, tarin loss sum: 0.021552, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 73, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001131, tarin loss sum: 0.019224, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 74, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000993, tarin loss sum: 0.016875, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 75, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000797, tarin loss sum: 0.013543, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","save model...\n","epoch: 76, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000743, tarin loss sum: 0.012636, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 77, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000687, tarin loss sum: 0.011677, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 78, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000666, tarin loss sum: 0.011314, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 79, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000646, tarin loss sum: 0.010980, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 80, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000596, tarin loss sum: 0.010134, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 81, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000589, tarin loss sum: 0.010018, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 82, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000557, tarin loss sum: 0.009469, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 83, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000523, tarin loss sum: 0.008898, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 84, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000541, tarin loss sum: 0.009203, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 85, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000541, tarin loss sum: 0.009190, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 86, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000487, tarin loss sum: 0.008277, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 87, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000452, tarin loss sum: 0.007687, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 88, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000474, tarin loss sum: 0.008054, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 89, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000534, tarin loss sum: 0.009075, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 90, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000632, tarin loss sum: 0.010742, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 91, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000430, tarin loss sum: 0.007308, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 92, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000567, tarin loss sum: 0.009634, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 93, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000422, tarin loss sum: 0.007172, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 94, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000554, tarin loss sum: 0.009418, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 95, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000415, tarin loss sum: 0.007057, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 96, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000364, tarin loss sum: 0.006186, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 97, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000373, tarin loss sum: 0.006333, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 98, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000312, tarin loss sum: 0.005308, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 99, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000302, tarin loss sum: 0.005134, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 100, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000286, tarin loss sum: 0.004856, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","Training stage finished:\n"," epoch 100, loss 0.0003, train acc 1.000, training time 16.45 s\n","\n","\n","====================Starting evaluation for testing set.========================\n","\n","confusion_matrix\n","[[678   0   0   0   0   1   0   1   0   0   0   0   0]\n"," [  0 216   0   0   0   2   0   0   0   0   0   0   0]\n"," [  0   0 228   8   0   0   0   0   0   0   0   0   0]\n"," [  0   0   1 214  19   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0 121   0   0   0   0   0   0   0   0]\n"," [  0   0   0   3   4 202   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0  94   0   0   0   0   0   0]\n"," [  0   1   0   0   0   0   0 383   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0 463   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0 360   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0 374   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0 448   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0 826]]\n","classification_report\n","                          precision    recall  f1-score   support\n","\n","                   Scrub       1.00      1.00      1.00       678\n","            Willow swamp       0.99      1.00      0.99       217\n","    Cabbage palm hammock       0.97      1.00      0.98       229\n","Cabbage palm/oak hammock       0.91      0.95      0.93       225\n","              Slash pine       1.00      0.84      0.91       144\n","   Oak/broadleaf hammock       0.97      0.99      0.98       205\n","          Hardwood swamp       1.00      1.00      1.00        94\n","         Graminoid marsh       1.00      1.00      1.00       384\n","          Spartine marsh       1.00      1.00      1.00       463\n","           Cattail marsh       1.00      1.00      1.00       360\n","              Salt marsh       1.00      1.00      1.00       374\n","               Mud flats       1.00      1.00      1.00       448\n","                   Water       1.00      1.00      1.00       826\n","\n","                accuracy                           0.99      4647\n","               macro avg       0.99      0.98      0.98      4647\n","            weighted avg       0.99      0.99      0.99      4647\n","\n","save model...\n","epoch: 1, training_sampler_num: 516, batch_count: 17.00, train loss: 1.292311, tarin loss sum: 21.969285, train acc: 0.632, train_acc_sum: 326.0, time: 0.2 sec\n","save model...\n","epoch: 2, training_sampler_num: 516, batch_count: 17.00, train loss: 2.431567, tarin loss sum: 41.336639, train acc: 0.132, train_acc_sum: 68.0, time: 0.2 sec\n","save model...\n","epoch: 3, training_sampler_num: 516, batch_count: 17.00, train loss: 1.883191, tarin loss sum: 32.014240, train acc: 0.391, train_acc_sum: 202.0, time: 0.2 sec\n","save model...\n","epoch: 4, training_sampler_num: 516, batch_count: 17.00, train loss: 1.068251, tarin loss sum: 18.160270, train acc: 0.605, train_acc_sum: 312.0, time: 0.2 sec\n","save model...\n","epoch: 5, training_sampler_num: 516, batch_count: 17.00, train loss: 0.702521, tarin loss sum: 11.942851, train acc: 0.744, train_acc_sum: 384.0, time: 0.2 sec\n","epoch: 6, training_sampler_num: 516, batch_count: 17.00, train loss: 0.495961, tarin loss sum: 8.431335, train acc: 0.822, train_acc_sum: 424.0, time: 0.2 sec\n","save model...\n","epoch: 7, training_sampler_num: 516, batch_count: 17.00, train loss: 0.349073, tarin loss sum: 5.934245, train acc: 0.884, train_acc_sum: 456.0, time: 0.2 sec\n","save model...\n","epoch: 8, training_sampler_num: 516, batch_count: 17.00, train loss: 0.284557, tarin loss sum: 4.837469, train acc: 0.880, train_acc_sum: 454.0, time: 0.2 sec\n","epoch: 9, training_sampler_num: 516, batch_count: 17.00, train loss: 0.206737, tarin loss sum: 3.514528, train acc: 0.934, train_acc_sum: 482.0, time: 0.2 sec\n","epoch: 10, training_sampler_num: 516, batch_count: 17.00, train loss: 0.191925, tarin loss sum: 3.262731, train acc: 0.928, train_acc_sum: 479.0, time: 0.2 sec\n","epoch: 11, training_sampler_num: 516, batch_count: 17.00, train loss: 0.193470, tarin loss sum: 3.288984, train acc: 0.926, train_acc_sum: 478.0, time: 0.2 sec\n","save model...\n","epoch: 12, training_sampler_num: 516, batch_count: 17.00, train loss: 0.161998, tarin loss sum: 2.753967, train acc: 0.950, train_acc_sum: 490.0, time: 0.2 sec\n","epoch: 13, training_sampler_num: 516, batch_count: 17.00, train loss: 0.131654, tarin loss sum: 2.238110, train acc: 0.952, train_acc_sum: 491.0, time: 0.2 sec\n","epoch: 14, training_sampler_num: 516, batch_count: 17.00, train loss: 0.137051, tarin loss sum: 2.329874, train acc: 0.953, train_acc_sum: 492.0, time: 0.2 sec\n","epoch: 15, training_sampler_num: 516, batch_count: 17.00, train loss: 0.158777, tarin loss sum: 2.699212, train acc: 0.942, train_acc_sum: 486.0, time: 0.2 sec\n","epoch: 16, training_sampler_num: 516, batch_count: 17.00, train loss: 0.121102, tarin loss sum: 2.058736, train acc: 0.961, train_acc_sum: 496.0, time: 0.2 sec\n","epoch: 17, training_sampler_num: 516, batch_count: 17.00, train loss: 0.098854, tarin loss sum: 1.680519, train acc: 0.983, train_acc_sum: 507.0, time: 0.2 sec\n","epoch: 18, training_sampler_num: 516, batch_count: 17.00, train loss: 0.157172, tarin loss sum: 2.671925, train acc: 0.942, train_acc_sum: 486.0, time: 0.2 sec\n","epoch: 19, training_sampler_num: 516, batch_count: 17.00, train loss: 0.107258, tarin loss sum: 1.823388, train acc: 0.957, train_acc_sum: 494.0, time: 0.2 sec\n","epoch: 20, training_sampler_num: 516, batch_count: 17.00, train loss: 0.084913, tarin loss sum: 1.443520, train acc: 0.979, train_acc_sum: 505.0, time: 0.2 sec\n","epoch: 21, training_sampler_num: 516, batch_count: 17.00, train loss: 0.094508, tarin loss sum: 1.606637, train acc: 0.959, train_acc_sum: 495.0, time: 0.2 sec\n","save model...\n","epoch: 22, training_sampler_num: 516, batch_count: 17.00, train loss: 0.059546, tarin loss sum: 1.012276, train acc: 0.981, train_acc_sum: 506.0, time: 0.2 sec\n","epoch: 23, training_sampler_num: 516, batch_count: 17.00, train loss: 0.058047, tarin loss sum: 0.986798, train acc: 0.981, train_acc_sum: 506.0, time: 0.2 sec\n","epoch: 24, training_sampler_num: 516, batch_count: 17.00, train loss: 0.050240, tarin loss sum: 0.854076, train acc: 0.981, train_acc_sum: 506.0, time: 0.2 sec\n","epoch: 25, training_sampler_num: 516, batch_count: 17.00, train loss: 0.060125, tarin loss sum: 1.022117, train acc: 0.981, train_acc_sum: 506.0, time: 0.2 sec\n","epoch: 26, training_sampler_num: 516, batch_count: 17.00, train loss: 0.052571, tarin loss sum: 0.893705, train acc: 0.986, train_acc_sum: 509.0, time: 0.2 sec\n","epoch: 27, training_sampler_num: 516, batch_count: 17.00, train loss: 0.126164, tarin loss sum: 2.144788, train acc: 0.963, train_acc_sum: 497.0, time: 0.2 sec\n","epoch: 28, training_sampler_num: 516, batch_count: 17.00, train loss: 0.070683, tarin loss sum: 1.201616, train acc: 0.983, train_acc_sum: 507.0, time: 0.2 sec\n","epoch: 29, training_sampler_num: 516, batch_count: 17.00, train loss: 0.041977, tarin loss sum: 0.713611, train acc: 0.984, train_acc_sum: 508.0, time: 0.2 sec\n","epoch: 30, training_sampler_num: 516, batch_count: 17.00, train loss: 0.028882, tarin loss sum: 0.490988, train acc: 0.990, train_acc_sum: 511.0, time: 0.2 sec\n","epoch: 31, training_sampler_num: 516, batch_count: 17.00, train loss: 0.034502, tarin loss sum: 0.586533, train acc: 0.994, train_acc_sum: 513.0, time: 0.2 sec\n","save model...\n","epoch: 32, training_sampler_num: 516, batch_count: 17.00, train loss: 0.064104, tarin loss sum: 1.089771, train acc: 0.971, train_acc_sum: 501.0, time: 0.2 sec\n","epoch: 33, training_sampler_num: 516, batch_count: 17.00, train loss: 0.028964, tarin loss sum: 0.492384, train acc: 0.992, train_acc_sum: 512.0, time: 0.2 sec\n","epoch: 34, training_sampler_num: 516, batch_count: 17.00, train loss: 0.021147, tarin loss sum: 0.359501, train acc: 0.994, train_acc_sum: 513.0, time: 0.2 sec\n","epoch: 35, training_sampler_num: 516, batch_count: 17.00, train loss: 0.018110, tarin loss sum: 0.307865, train acc: 0.996, train_acc_sum: 514.0, time: 0.2 sec\n","epoch: 36, training_sampler_num: 516, batch_count: 17.00, train loss: 0.015380, tarin loss sum: 0.261462, train acc: 0.994, train_acc_sum: 513.0, time: 0.2 sec\n","epoch: 37, training_sampler_num: 516, batch_count: 17.00, train loss: 0.026439, tarin loss sum: 0.449458, train acc: 0.988, train_acc_sum: 510.0, time: 0.2 sec\n","save model...\n","epoch: 38, training_sampler_num: 516, batch_count: 17.00, train loss: 0.039939, tarin loss sum: 0.678971, train acc: 0.990, train_acc_sum: 511.0, time: 0.2 sec\n","epoch: 39, training_sampler_num: 516, batch_count: 17.00, train loss: 0.032763, tarin loss sum: 0.556969, train acc: 0.990, train_acc_sum: 511.0, time: 0.2 sec\n","epoch: 40, training_sampler_num: 516, batch_count: 17.00, train loss: 0.018553, tarin loss sum: 0.315402, train acc: 0.994, train_acc_sum: 513.0, time: 0.2 sec\n","epoch: 41, training_sampler_num: 516, batch_count: 17.00, train loss: 0.018320, tarin loss sum: 0.311433, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 42, training_sampler_num: 516, batch_count: 17.00, train loss: 0.017765, tarin loss sum: 0.302002, train acc: 0.994, train_acc_sum: 513.0, time: 0.2 sec\n","save model...\n","epoch: 43, training_sampler_num: 516, batch_count: 17.00, train loss: 0.021225, tarin loss sum: 0.360821, train acc: 0.990, train_acc_sum: 511.0, time: 0.2 sec\n","epoch: 44, training_sampler_num: 516, batch_count: 17.00, train loss: 0.019598, tarin loss sum: 0.333165, train acc: 0.990, train_acc_sum: 511.0, time: 0.2 sec\n","epoch: 45, training_sampler_num: 516, batch_count: 17.00, train loss: 0.030751, tarin loss sum: 0.522768, train acc: 0.988, train_acc_sum: 510.0, time: 0.2 sec\n","epoch: 46, training_sampler_num: 516, batch_count: 17.00, train loss: 0.052995, tarin loss sum: 0.900922, train acc: 0.981, train_acc_sum: 506.0, time: 0.2 sec\n","epoch: 47, training_sampler_num: 516, batch_count: 17.00, train loss: 0.024400, tarin loss sum: 0.414808, train acc: 0.988, train_acc_sum: 510.0, time: 0.2 sec\n","epoch: 48, training_sampler_num: 516, batch_count: 17.00, train loss: 0.021585, tarin loss sum: 0.366937, train acc: 0.992, train_acc_sum: 512.0, time: 0.2 sec\n","epoch: 49, training_sampler_num: 516, batch_count: 17.00, train loss: 0.020874, tarin loss sum: 0.354858, train acc: 0.996, train_acc_sum: 514.0, time: 0.2 sec\n","epoch: 50, training_sampler_num: 516, batch_count: 17.00, train loss: 0.026771, tarin loss sum: 0.455109, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 51, training_sampler_num: 516, batch_count: 17.00, train loss: 0.004248, tarin loss sum: 0.072217, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 52, training_sampler_num: 516, batch_count: 17.00, train loss: 0.003609, tarin loss sum: 0.061348, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 53, training_sampler_num: 516, batch_count: 17.00, train loss: 0.003182, tarin loss sum: 0.054097, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 54, training_sampler_num: 516, batch_count: 17.00, train loss: 0.002475, tarin loss sum: 0.042069, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 55, training_sampler_num: 516, batch_count: 17.00, train loss: 0.002339, tarin loss sum: 0.039769, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 56, training_sampler_num: 516, batch_count: 17.00, train loss: 0.002060, tarin loss sum: 0.035017, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 57, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001960, tarin loss sum: 0.033323, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 58, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001820, tarin loss sum: 0.030940, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 59, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001570, tarin loss sum: 0.026684, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 60, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001554, tarin loss sum: 0.026416, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 61, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001373, tarin loss sum: 0.023340, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 62, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001328, tarin loss sum: 0.022576, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 63, training_sampler_num: 516, batch_count: 17.00, train loss: 0.003286, tarin loss sum: 0.055860, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 64, training_sampler_num: 516, batch_count: 17.00, train loss: 0.006708, tarin loss sum: 0.114034, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","save model...\n","epoch: 65, training_sampler_num: 516, batch_count: 17.00, train loss: 0.004096, tarin loss sum: 0.069630, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 66, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001647, tarin loss sum: 0.027996, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 67, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001067, tarin loss sum: 0.018144, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 68, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000933, tarin loss sum: 0.015859, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 69, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000823, tarin loss sum: 0.013998, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 70, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000771, tarin loss sum: 0.013113, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 71, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000733, tarin loss sum: 0.012457, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 72, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000706, tarin loss sum: 0.012003, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 73, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000666, tarin loss sum: 0.011330, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 74, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000625, tarin loss sum: 0.010625, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","save model...\n","epoch: 75, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000562, tarin loss sum: 0.009552, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 76, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000701, tarin loss sum: 0.011924, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 77, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000550, tarin loss sum: 0.009354, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 78, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000495, tarin loss sum: 0.008413, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 79, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000534, tarin loss sum: 0.009082, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 80, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000459, tarin loss sum: 0.007805, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 81, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000454, tarin loss sum: 0.007715, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 82, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000419, tarin loss sum: 0.007119, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 83, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000410, tarin loss sum: 0.006976, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 84, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000393, tarin loss sum: 0.006685, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 85, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000394, tarin loss sum: 0.006700, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 86, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000384, tarin loss sum: 0.006529, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 87, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000368, tarin loss sum: 0.006256, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 88, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000340, tarin loss sum: 0.005779, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 89, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000393, tarin loss sum: 0.006688, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 90, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000342, tarin loss sum: 0.005812, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 91, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000317, tarin loss sum: 0.005397, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 92, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000301, tarin loss sum: 0.005118, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","save model...\n","epoch: 93, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000311, tarin loss sum: 0.005284, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 94, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000488, tarin loss sum: 0.008304, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 95, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000410, tarin loss sum: 0.006977, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 96, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000254, tarin loss sum: 0.004311, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 97, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000260, tarin loss sum: 0.004413, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 98, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000257, tarin loss sum: 0.004362, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 99, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000245, tarin loss sum: 0.004162, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 100, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000235, tarin loss sum: 0.003997, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","Training stage finished:\n"," epoch 100, loss 0.0002, train acc 1.000, training time 16.44 s\n","\n","\n","====================Starting evaluation for testing set.========================\n","\n","confusion_matrix\n","[[676   0   0   0   0   0   0   0   0   0   0   0   0]\n"," [  0 210   0   0   0   4   0   0   0   0   0   0   0]\n"," [  0   0 229   8   0   0   0   0   0   0   0   0   0]\n"," [  0   1   0 216  11   0   0   3   0   0   0   0   0]\n"," [  0   0   0   1 133   0   0   0   0   0   0   0   0]\n"," [  2   6   0   0   0 201   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0  94   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0 381   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0 463   0   5   0   0]\n"," [  0   0   0   0   0   0   0   0   0 360   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0 369   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0 448   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0 826]]\n","classification_report\n","                          precision    recall  f1-score   support\n","\n","                   Scrub       1.00      1.00      1.00       678\n","            Willow swamp       0.98      0.97      0.97       217\n","    Cabbage palm hammock       0.97      1.00      0.98       229\n","Cabbage palm/oak hammock       0.94      0.96      0.95       225\n","              Slash pine       0.99      0.92      0.96       144\n","   Oak/broadleaf hammock       0.96      0.98      0.97       205\n","          Hardwood swamp       1.00      1.00      1.00        94\n","         Graminoid marsh       1.00      0.99      1.00       384\n","          Spartine marsh       0.99      1.00      0.99       463\n","           Cattail marsh       1.00      1.00      1.00       360\n","              Salt marsh       1.00      0.99      0.99       374\n","               Mud flats       1.00      1.00      1.00       448\n","                   Water       1.00      1.00      1.00       826\n","\n","                accuracy                           0.99      4647\n","               macro avg       0.99      0.99      0.99      4647\n","            weighted avg       0.99      0.99      0.99      4647\n","\n","save model...\n","epoch: 1, training_sampler_num: 516, batch_count: 17.00, train loss: 1.149805, tarin loss sum: 19.546688, train acc: 0.719, train_acc_sum: 371.0, time: 0.2 sec\n","save model...\n","epoch: 2, training_sampler_num: 516, batch_count: 17.00, train loss: 2.484134, tarin loss sum: 42.230280, train acc: 0.114, train_acc_sum: 59.0, time: 0.2 sec\n","save model...\n","epoch: 3, training_sampler_num: 516, batch_count: 17.00, train loss: 1.837754, tarin loss sum: 31.241818, train acc: 0.457, train_acc_sum: 236.0, time: 0.2 sec\n","save model...\n","epoch: 4, training_sampler_num: 516, batch_count: 17.00, train loss: 0.901828, tarin loss sum: 15.331078, train acc: 0.667, train_acc_sum: 344.0, time: 0.2 sec\n","save model...\n","epoch: 5, training_sampler_num: 516, batch_count: 17.00, train loss: 0.675540, tarin loss sum: 11.484173, train acc: 0.764, train_acc_sum: 394.0, time: 0.2 sec\n","save model...\n","epoch: 6, training_sampler_num: 516, batch_count: 17.00, train loss: 0.598518, tarin loss sum: 10.174808, train acc: 0.789, train_acc_sum: 407.0, time: 0.2 sec\n","save model...\n","epoch: 7, training_sampler_num: 516, batch_count: 17.00, train loss: 0.481664, tarin loss sum: 8.188294, train acc: 0.798, train_acc_sum: 412.0, time: 0.2 sec\n","save model...\n","epoch: 8, training_sampler_num: 516, batch_count: 17.00, train loss: 0.377993, tarin loss sum: 6.425877, train acc: 0.870, train_acc_sum: 449.0, time: 0.2 sec\n","save model...\n","epoch: 9, training_sampler_num: 516, batch_count: 17.00, train loss: 0.329360, tarin loss sum: 5.599122, train acc: 0.911, train_acc_sum: 470.0, time: 0.2 sec\n","epoch: 10, training_sampler_num: 516, batch_count: 17.00, train loss: 0.305220, tarin loss sum: 5.188733, train acc: 0.899, train_acc_sum: 464.0, time: 0.2 sec\n","save model...\n","epoch: 11, training_sampler_num: 516, batch_count: 17.00, train loss: 0.211445, tarin loss sum: 3.594568, train acc: 0.905, train_acc_sum: 467.0, time: 0.2 sec\n","epoch: 12, training_sampler_num: 516, batch_count: 17.00, train loss: 0.160532, tarin loss sum: 2.729045, train acc: 0.952, train_acc_sum: 491.0, time: 0.2 sec\n","epoch: 13, training_sampler_num: 516, batch_count: 17.00, train loss: 0.122488, tarin loss sum: 2.082300, train acc: 0.952, train_acc_sum: 491.0, time: 0.2 sec\n","epoch: 14, training_sampler_num: 516, batch_count: 17.00, train loss: 0.107524, tarin loss sum: 1.827903, train acc: 0.965, train_acc_sum: 498.0, time: 0.2 sec\n","epoch: 15, training_sampler_num: 516, batch_count: 17.00, train loss: 0.104699, tarin loss sum: 1.779880, train acc: 0.967, train_acc_sum: 499.0, time: 0.2 sec\n","epoch: 16, training_sampler_num: 516, batch_count: 17.00, train loss: 0.129871, tarin loss sum: 2.207815, train acc: 0.965, train_acc_sum: 498.0, time: 0.2 sec\n","epoch: 17, training_sampler_num: 516, batch_count: 17.00, train loss: 0.134808, tarin loss sum: 2.291732, train acc: 0.957, train_acc_sum: 494.0, time: 0.2 sec\n","epoch: 18, training_sampler_num: 516, batch_count: 17.00, train loss: 0.101797, tarin loss sum: 1.730550, train acc: 0.950, train_acc_sum: 490.0, time: 0.2 sec\n","epoch: 19, training_sampler_num: 516, batch_count: 17.00, train loss: 0.086733, tarin loss sum: 1.474465, train acc: 0.965, train_acc_sum: 498.0, time: 0.2 sec\n","epoch: 20, training_sampler_num: 516, batch_count: 17.00, train loss: 0.067627, tarin loss sum: 1.149654, train acc: 0.977, train_acc_sum: 504.0, time: 0.2 sec\n","save model...\n","epoch: 21, training_sampler_num: 516, batch_count: 17.00, train loss: 0.045196, tarin loss sum: 0.768330, train acc: 0.990, train_acc_sum: 511.0, time: 0.2 sec\n","save model...\n","epoch: 22, training_sampler_num: 516, batch_count: 17.00, train loss: 0.050838, tarin loss sum: 0.864243, train acc: 0.981, train_acc_sum: 506.0, time: 0.2 sec\n","epoch: 23, training_sampler_num: 516, batch_count: 17.00, train loss: 0.036706, tarin loss sum: 0.624009, train acc: 0.988, train_acc_sum: 510.0, time: 0.2 sec\n","epoch: 24, training_sampler_num: 516, batch_count: 17.00, train loss: 0.035171, tarin loss sum: 0.597913, train acc: 0.988, train_acc_sum: 510.0, time: 0.2 sec\n","save model...\n","epoch: 25, training_sampler_num: 516, batch_count: 17.00, train loss: 0.018442, tarin loss sum: 0.313520, train acc: 0.996, train_acc_sum: 514.0, time: 0.2 sec\n","epoch: 26, training_sampler_num: 516, batch_count: 17.00, train loss: 0.015396, tarin loss sum: 0.261732, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 27, training_sampler_num: 516, batch_count: 17.00, train loss: 0.012620, tarin loss sum: 0.214535, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 28, training_sampler_num: 516, batch_count: 17.00, train loss: 0.016450, tarin loss sum: 0.279647, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 29, training_sampler_num: 516, batch_count: 17.00, train loss: 0.029221, tarin loss sum: 0.496762, train acc: 0.988, train_acc_sum: 510.0, time: 0.2 sec\n","epoch: 30, training_sampler_num: 516, batch_count: 17.00, train loss: 0.030007, tarin loss sum: 0.510118, train acc: 0.990, train_acc_sum: 511.0, time: 0.2 sec\n","epoch: 31, training_sampler_num: 516, batch_count: 17.00, train loss: 0.030541, tarin loss sum: 0.519194, train acc: 0.992, train_acc_sum: 512.0, time: 0.2 sec\n","epoch: 32, training_sampler_num: 516, batch_count: 17.00, train loss: 0.078530, tarin loss sum: 1.335003, train acc: 0.983, train_acc_sum: 507.0, time: 0.2 sec\n","epoch: 33, training_sampler_num: 516, batch_count: 17.00, train loss: 0.069704, tarin loss sum: 1.184969, train acc: 0.971, train_acc_sum: 501.0, time: 0.2 sec\n","epoch: 34, training_sampler_num: 516, batch_count: 17.00, train loss: 0.269967, tarin loss sum: 4.589437, train acc: 0.934, train_acc_sum: 482.0, time: 0.2 sec\n","epoch: 35, training_sampler_num: 516, batch_count: 17.00, train loss: 0.043245, tarin loss sum: 0.735159, train acc: 0.990, train_acc_sum: 511.0, time: 0.2 sec\n","epoch: 36, training_sampler_num: 516, batch_count: 17.00, train loss: 0.035285, tarin loss sum: 0.599837, train acc: 0.990, train_acc_sum: 511.0, time: 0.2 sec\n","epoch: 37, training_sampler_num: 516, batch_count: 17.00, train loss: 0.260874, tarin loss sum: 4.434855, train acc: 0.963, train_acc_sum: 497.0, time: 0.2 sec\n","epoch: 38, training_sampler_num: 516, batch_count: 17.00, train loss: 0.057606, tarin loss sum: 0.979297, train acc: 0.983, train_acc_sum: 507.0, time: 0.2 sec\n","epoch: 39, training_sampler_num: 516, batch_count: 17.00, train loss: 0.026184, tarin loss sum: 0.445134, train acc: 0.992, train_acc_sum: 512.0, time: 0.2 sec\n","epoch: 40, training_sampler_num: 516, batch_count: 17.00, train loss: 0.016984, tarin loss sum: 0.288734, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 41, training_sampler_num: 516, batch_count: 17.00, train loss: 0.008074, tarin loss sum: 0.137256, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 42, training_sampler_num: 516, batch_count: 17.00, train loss: 0.007091, tarin loss sum: 0.120555, train acc: 0.998, train_acc_sum: 515.0, time: 0.2 sec\n","epoch: 43, training_sampler_num: 516, batch_count: 17.00, train loss: 0.005931, tarin loss sum: 0.100827, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 44, training_sampler_num: 516, batch_count: 17.00, train loss: 0.004777, tarin loss sum: 0.081201, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 45, training_sampler_num: 516, batch_count: 17.00, train loss: 0.004820, tarin loss sum: 0.081948, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 46, training_sampler_num: 516, batch_count: 17.00, train loss: 0.003187, tarin loss sum: 0.054187, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 47, training_sampler_num: 516, batch_count: 17.00, train loss: 0.002631, tarin loss sum: 0.044735, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 48, training_sampler_num: 516, batch_count: 17.00, train loss: 0.002616, tarin loss sum: 0.044475, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 49, training_sampler_num: 516, batch_count: 17.00, train loss: 0.002287, tarin loss sum: 0.038883, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","save model...\n","epoch: 50, training_sampler_num: 516, batch_count: 17.00, train loss: 0.002148, tarin loss sum: 0.036519, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 51, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001970, tarin loss sum: 0.033489, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 52, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001900, tarin loss sum: 0.032296, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 53, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001542, tarin loss sum: 0.026222, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","save model...\n","epoch: 54, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001545, tarin loss sum: 0.026267, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 55, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001549, tarin loss sum: 0.026326, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 56, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001460, tarin loss sum: 0.024827, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 57, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001409, tarin loss sum: 0.023948, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 58, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001189, tarin loss sum: 0.020216, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","save model...\n","epoch: 59, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001168, tarin loss sum: 0.019863, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 60, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001075, tarin loss sum: 0.018282, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 61, training_sampler_num: 516, batch_count: 17.00, train loss: 0.001031, tarin loss sum: 0.017529, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 62, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000971, tarin loss sum: 0.016512, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 63, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000929, tarin loss sum: 0.015798, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 64, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000882, tarin loss sum: 0.014993, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 65, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000842, tarin loss sum: 0.014307, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 66, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000874, tarin loss sum: 0.014853, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 67, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000790, tarin loss sum: 0.013422, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 68, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000688, tarin loss sum: 0.011700, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 69, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000659, tarin loss sum: 0.011196, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 70, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000630, tarin loss sum: 0.010703, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 71, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000595, tarin loss sum: 0.010121, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 72, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000590, tarin loss sum: 0.010029, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 73, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000606, tarin loss sum: 0.010297, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 74, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000570, tarin loss sum: 0.009694, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 75, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000616, tarin loss sum: 0.010478, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 76, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000552, tarin loss sum: 0.009390, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 77, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000568, tarin loss sum: 0.009652, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 78, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000537, tarin loss sum: 0.009129, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 79, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000471, tarin loss sum: 0.008009, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 80, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000452, tarin loss sum: 0.007685, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 81, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000453, tarin loss sum: 0.007699, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 82, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000391, tarin loss sum: 0.006644, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 83, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000414, tarin loss sum: 0.007030, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 84, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000494, tarin loss sum: 0.008398, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 85, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000408, tarin loss sum: 0.006939, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 86, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000355, tarin loss sum: 0.006036, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 87, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000346, tarin loss sum: 0.005878, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 88, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000316, tarin loss sum: 0.005368, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 89, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000318, tarin loss sum: 0.005411, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 90, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000329, tarin loss sum: 0.005585, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 91, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000309, tarin loss sum: 0.005250, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 92, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000282, tarin loss sum: 0.004793, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 93, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000278, tarin loss sum: 0.004719, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 94, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000272, tarin loss sum: 0.004617, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 95, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000262, tarin loss sum: 0.004452, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","save model...\n","epoch: 96, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000296, tarin loss sum: 0.005038, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 97, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000257, tarin loss sum: 0.004374, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 98, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000243, tarin loss sum: 0.004126, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 99, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000282, tarin loss sum: 0.004802, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","epoch: 100, training_sampler_num: 516, batch_count: 17.00, train loss: 0.000267, tarin loss sum: 0.004539, train acc: 1.000, train_acc_sum: 516.0, time: 0.2 sec\n","Training stage finished:\n"," epoch 100, loss 0.0003, train acc 1.000, training time 16.47 s\n","\n","\n","====================Starting evaluation for testing set.========================\n","\n","confusion_matrix\n","[[673   1   0   0   0   0   0   0   0   0   0   0   0]\n"," [  0 208   0   0   0   2   0   0   0   0   0   0   0]\n"," [  0   0 229   5   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0 202   9   0   0   0   0   0   0   0   0]\n"," [  0   0   0   6 128   0   0   0   0   0   0   0   0]\n"," [  5   4   0  12   7 203   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0  94   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0 384   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0 463   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0 360   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0 371   0   0]\n"," [  0   4   0   0   0   0   0   0   0   0   3 448   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0 826]]\n","classification_report\n","                          precision    recall  f1-score   support\n","\n","                   Scrub       1.00      0.99      1.00       678\n","            Willow swamp       0.99      0.96      0.97       217\n","    Cabbage palm hammock       0.98      1.00      0.99       229\n","Cabbage palm/oak hammock       0.96      0.90      0.93       225\n","              Slash pine       0.96      0.89      0.92       144\n","   Oak/broadleaf hammock       0.88      0.99      0.93       205\n","          Hardwood swamp       1.00      1.00      1.00        94\n","         Graminoid marsh       1.00      1.00      1.00       384\n","          Spartine marsh       1.00      1.00      1.00       463\n","           Cattail marsh       1.00      1.00      1.00       360\n","              Salt marsh       1.00      0.99      1.00       374\n","               Mud flats       0.98      1.00      0.99       448\n","                   Water       1.00      1.00      1.00       826\n","\n","                accuracy                           0.99      4647\n","               macro avg       0.98      0.98      0.98      4647\n","            weighted avg       0.99      0.99      0.99      4647\n","\n","\n","====================Mean result of 5 times runs =========================\n","List of OA: [0.986442866365397, 0.9866580589627717, 0.991392296105014, 0.9911771035076393, 0.9875188293522703]\n","List of AA: [0.9794823761753565, 0.9777701814993335, 0.9871091639549847, 0.9866303163696645, 0.9802767773190543]\n","List of KPP: [0.9849076568892579, 0.9851480861292855, 0.9904166711563817, 0.9901785366330423, 0.9861063514705537]\n","OA= 98.86 +- 0.22\n","AA= 98.23 +- 0.39\n","Kpp= 98.74 +- 0.24\n","Acc per class= [ 99.91  98.34  96.39  92.19  97.17  94.43  99.79  99.95  99.12  99.94\n"," 100.    99.69 100.  ] +- [0.12 0.68 1.74 2.48 2.02 3.4  0.42 0.1  1.3  0.12 0.   0.62 0.  ]\n","Average training time= 16.5 +- 0.063\n","Average testing time= 2.79 +- 0.236\n"]}]}]}